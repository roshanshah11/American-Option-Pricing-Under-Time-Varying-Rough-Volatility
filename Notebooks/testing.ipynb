{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccc9cab",
   "metadata": {
    "id": "eccc9cab"
   },
   "source": [
    "# Pricing American options in rough Bergomi with linear and deep signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ckOdm_OAGE",
   "metadata": {
    "id": "c9ckOdm_OAGE"
   },
   "source": [
    "In this notebook we show how to use the code from https://github.com/lucapelizzari/Optimal_Stopping_with_signatures/tree/main, to compute lower and upper bounds for American options in the rough Bergomi model using different signature methods, see for example Section 4.2 of https://arxiv.org/abs/2312.03444 for the linear approach, whereas the deep neural network approaches will be discussed in a forthcoming paper.\n",
    "\n",
    "The repository consists of:\n",
    "\n",
    "*   Simulation packages for fractional Brownian motion, rough Bergomi and rough Heston models\n",
    "*   A modul for signature related computations **Signature_computer.py**, which can compute the signature and log-signature  of various lifts related to volatility modelling, with the additional option of adding polynomials of the state-process and/or volatility.\n",
    "*   The main module for the linear signature approaches **Linear_signature_optimal_stopping.py**, which can be used to derive lower and upper bounds to the optimal stopping problem applying the approaches described in https://arxiv.org/abs/2312.03444\n",
    "*   The main module for deep log-signature approaches **Deep_signature_optimal_stopping.py**, which extends the linear approaches by applying deep neural networks on the log-signature. This code is accompanying a working paper paper on \"American option pricing using signatures\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282196a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe58ed6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for TensorFlow import issues and environment compatibility\n",
    "# Set correct Python path to find modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory and subdirectories to Python path\n",
    "repo_root = os.path.abspath('..')\n",
    "sys.path.append(repo_root)\n",
    "sys.path.append(os.path.join(repo_root, \"Linear signature optimal stopping\"))\n",
    "sys.path.append(os.path.join(repo_root, \"Non linear signature optimal stopping\"))\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e568a2",
   "metadata": {
    "id": "57e568a2"
   },
   "source": [
    "## American Put options in the rough Bergomi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572b688",
   "metadata": {
    "id": "6572b688"
   },
   "source": [
    "\n",
    "\n",
    "Recall the price and volatility dynamics of the latter are given by \\begin{align*}\n",
    "dX_t &= rX_tdt+X_tv_t \\left (\\rho dW_r+\\sqrt{1-\\rho^2}dB_t\\right ), \\\\ v_t & =\\xi_0\\mathcal{E}\\left (\\eta \\int_0^t(t-s)^{H-\\frac{1}{2}}dW_s \\right )\n",
    "\\end{align*} and pricing an American Put-option can be formulated as optimal stopping problem $$y_0=\\sup_{\\tau \\in \\mathcal{S}_0}\\mathbb{E}[e^{-r\\tau}\\left (K-X_{\\tau}\\right )^{+}]$$ for some strike $K$. In this notebook we consider the following choice of paramteres $$ H=0.07,X_0 = 100, r=0.05, \\eta = 1.9, \\rho = -0.9, \\xi_0= 0.09, K = 110.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xlp80cKeRW9z",
   "metadata": {
    "id": "Xlp80cKeRW9z"
   },
   "source": [
    "## Step 1: Simulation rough Bergomi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I7PTV1Z3Rig-",
   "metadata": {
    "id": "I7PTV1Z3Rig-"
   },
   "source": [
    "We start by defining the parameters of the model and importing the rough Berogmi simulation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd54c84b",
   "metadata": {
    "id": "fd54c84b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define all your parameters\n",
    "N1 = 14  # number of exercise-dates\n",
    "N = 252  # discretization-grid\n",
    "T = 14  # Maturity in days\n",
    "T_years = T / 252  # Maturity in years\n",
    "M = 2**13  # number of samples for training\n",
    "M2 = 2**13  # number of samples for testing\n",
    "H = 0.07  # Hurst parameter\n",
    "eta = 1.9\n",
    "X0 = 1\n",
    "r = 0.05\n",
    "rho = -0.9\n",
    "xi = 0.09\n",
    "strike = 1.05  # This is used in phi, not directly in the simulation\n",
    "K = 2  # This is the depth of Signature parameter\n",
    "# Your payoff function uses strike\n",
    "def phi(x):\n",
    "    return np.maximum(strike-x, 0)  # payoff function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61KWkP-iYnFJ",
   "metadata": {
    "id": "61KWkP-iYnFJ"
   },
   "source": [
    "Note that the number of samples should be much bigger to get reliable results, but to keep the complexity low in this presentation we restrict to $2**15$ training and testing paths here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057f779",
   "metadata": {
    "id": "7057f779"
   },
   "outputs": [],
   "source": [
    "# Adjust path to include repository root\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rBergomi_simulation import SimulationofrBergomi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29669ebe",
   "metadata": {
    "id": "29669ebe"
   },
   "source": [
    "Next we define a function generating rough Bergomi prices, volatilies and the corresponding Brownian motion and payoff process. Then we generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd21314",
   "metadata": {
    "id": "9cd21314"
   },
   "outputs": [],
   "source": [
    "def generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r):\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationofrBergomi(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86a8cc",
   "metadata": {
    "id": "7c86a8cc"
   },
   "outputs": [],
   "source": [
    "# Use K in your function call, not strike\n",
    "S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data(M2, N, T_years, phi, rho, K, X0, H, xi, eta, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show Head of the training data\n",
      "S_training [[1.         1.02366161 1.01558289 1.01791001 1.01265106 1.0139968\n",
      "  1.01537034 1.0247938  1.03205779 1.03314347 1.03728687 1.03866331\n",
      "  1.03834839 1.04221617 1.04320711]\n",
      " [1.         1.00404953 0.98661537 1.02117389 1.02137071 1.02069033\n",
      "  1.02503354 1.01508967 1.02479632 1.03215783 1.03307057 1.02356082\n",
      "  1.01810729 0.97814946 1.01635097]\n",
      " [1.         1.01679544 1.01196769 1.01451727 1.03600187 1.03289542\n",
      "  1.02970534 1.0268663  1.00123735 0.99700164 1.01434144 1.01544836\n",
      "  1.00956719 0.99569058 0.9964671 ]\n",
      " [1.         1.00946391 1.01047207 1.00853031 0.98832992 0.96208366\n",
      "  0.9980605  0.99526411 0.99474903 1.00422758 0.98878723 0.96679287\n",
      "  0.96394657 0.95236486 0.93847845]\n",
      " [1.         1.00420544 1.01851017 1.01591762 1.01228992 1.01158605\n",
      "  1.01723176 1.01484748 1.03461381 1.03117454 1.03558265 1.03878542\n",
      "  1.04716524 1.04781743 1.03915182]]\n",
      "V_training [[9.00000000e-02 2.87867832e-02 1.20944629e-02 5.34851416e-02\n",
      "  2.64231033e-02 3.55949787e-03 9.36820386e-03 8.45285941e-03\n",
      "  1.29980694e-03 1.13702035e-02 1.77833802e-02 5.85051990e-03\n",
      "  2.17000819e-03 1.68858305e-03 1.52980135e-02]\n",
      " [9.00000000e-02 1.15190027e-01 6.28568433e-02 2.29486903e-03\n",
      "  4.06384323e-02 3.77295281e-02 2.43890034e-02 9.82671043e-02\n",
      "  5.06740632e-02 1.56500991e-02 3.62254943e-02 3.17634654e-02\n",
      "  1.40413773e-01 2.13335264e-01 3.58481158e-03]\n",
      " [9.00000000e-02 3.50465057e-03 1.99279594e-02 5.38603914e-02\n",
      "  9.72042913e-02 2.70644361e-02 4.25053668e-03 4.08177170e-02\n",
      "  5.65690756e-02 7.54221535e-02 3.10974742e-02 9.80492175e-02\n",
      "  3.51554447e-02 5.16648525e-02 1.35320016e-01]\n",
      " [9.00000000e-02 2.76622717e-02 1.39237398e-02 6.69723518e-02\n",
      "  7.13863844e-02 7.70497012e-02 2.92417600e-02 3.55644461e-03\n",
      "  8.04070050e-02 5.70478843e-02 6.71071070e-01 6.72944117e-02\n",
      "  1.21301272e-01 2.50492564e-02 6.47598841e-01]\n",
      " [9.00000000e-02 3.05950379e-01 1.31613223e-02 3.34687864e-02\n",
      "  4.95859036e-03 3.34287961e-02 8.78173911e-03 4.22921588e-02\n",
      "  4.07967885e-03 6.71040294e-03 1.33824070e-02 7.97826686e-03\n",
      "  3.26863818e-04 4.12198987e-03 8.68988022e-02]]\n",
      "Payoff_training [[0.05       0.02633839 0.03441711 0.03208999 0.03734894 0.0360032\n",
      "  0.03462966 0.0252062  0.01794221 0.01685653 0.01271313 0.01133669\n",
      "  0.01165161 0.00778383 0.00679289]\n",
      " [0.05       0.04595047 0.06338463 0.02882611 0.02862929 0.02930967\n",
      "  0.02496646 0.03491033 0.02520368 0.01784217 0.01692943 0.02643918\n",
      "  0.03189271 0.07185054 0.03364903]\n",
      " [0.05       0.03320456 0.03803231 0.03548273 0.01399813 0.01710458\n",
      "  0.02029466 0.0231337  0.04876265 0.05299836 0.03565856 0.03455164\n",
      "  0.04043281 0.05430942 0.0535329 ]\n",
      " [0.05       0.04053609 0.03952793 0.04146969 0.06167008 0.08791634\n",
      "  0.0519395  0.05473589 0.05525097 0.04577242 0.06121277 0.08320713\n",
      "  0.08605343 0.09763514 0.11152155]\n",
      " [0.05       0.04579456 0.03148983 0.03408238 0.03771008 0.03841395\n",
      "  0.03276824 0.03515252 0.01538619 0.01882546 0.01441735 0.01121458\n",
      "  0.00283476 0.00218257 0.01084818]]\n",
      "dW_training [[[-5.29942813e-02 -4.32669134e-01]\n",
      "  [ 1.20123607e-02 -1.04756460e+00]\n",
      "  [-2.57033377e-02  9.10142216e-01]\n",
      "  [ 3.97990916e-02  1.79458314e-01]\n",
      "  [-4.32712944e-02 -3.03028860e+00]\n",
      "  [-3.17303033e-04 -1.20317273e+00]\n",
      "  [-1.17310160e-01 -1.38323710e+00]\n",
      "  [-6.50631957e-02 -2.95377796e+00]\n",
      "  [-3.22052564e-02  4.72333616e-01]\n",
      "  [-5.24927765e-02  1.16074124e+00]\n",
      "  [-6.02541934e-02 -1.18612636e-01]\n",
      "  [-2.47214306e-02 -1.18302828e+00]\n",
      "  [-5.38688798e-02 -1.56109442e+00]\n",
      "  [ 1.42392900e-02  1.82688320e+00]]\n",
      "\n",
      " [[-1.90772473e-02  1.51786925e+00]\n",
      "  [ 9.03935203e-02  9.59947586e-01]\n",
      "  [-1.40125986e-01 -4.48477138e+00]\n",
      "  [ 1.42349475e-02  1.04751232e+00]\n",
      "  [-1.33297073e-02  6.74223737e-01]\n",
      "  [-3.81117446e-03  1.57315031e-01]\n",
      "  [ 6.04066515e-02  2.11905974e+00]\n",
      "  [-5.59311692e-03  6.13140194e-01]\n",
      "  [-4.07222499e-02 -8.84457863e-01]\n",
      "  [-3.67328792e-03  7.16322655e-01]\n",
      "  [ 1.10566429e-01  5.23646829e-01]\n",
      "  [ 2.05616193e-02  1.58130330e+00]\n",
      "  [ 3.03233573e-02  2.18045062e+00]\n",
      "  [-1.32218069e-01 -3.69777415e+00]]\n",
      "\n",
      " [[-9.41085605e-02 -3.39480361e+00]\n",
      "  [ 8.03881651e-02  3.16108436e-02]\n",
      "  [ 3.12147790e-03  5.93557504e-01]\n",
      "  [-7.07021672e-02  1.50763137e+00]\n",
      "  [ 4.63635531e-02  4.26767432e-01]\n",
      "  [-6.85826870e-03 -2.68463317e+00]\n",
      "  [-5.71814442e-03  6.14765891e-01]\n",
      "  [ 1.34035749e-01  1.14593749e+00]\n",
      "  [ 1.93153314e-02  3.29788977e-01]\n",
      "  [-5.93733795e-02 -8.32745266e-01]\n",
      "  [-6.30257066e-03  1.50785212e+00]\n",
      "  [-1.97259905e-02  1.30398416e-01]\n",
      "  [ 3.56680092e-02  8.68909449e-01]\n",
      "  [ 1.87086799e-02  1.88351926e+00]]\n",
      "\n",
      " [[-5.76558250e-02 -4.88719319e-01]\n",
      "  [-4.63984372e-04 -8.06728902e-01]\n",
      "  [-1.24763340e-02  1.37481039e+00]\n",
      "  [ 1.00372921e-01  1.57672576e+00]\n",
      "  [ 1.06250261e-01  7.48284864e-01]\n",
      "  [-9.11366759e-02 -1.40295413e+00]\n",
      "  [-2.54491198e-03 -3.22970929e+00]\n",
      "  [ 2.44372906e-02  1.18762786e+00]\n",
      "  [-4.18725424e-02  5.09316576e-01]\n",
      "  [ 1.07341937e-01  4.43850372e+00]\n",
      "  [ 3.89985725e-02  1.97747848e-01]\n",
      "  [ 3.05378376e-02  8.72711378e-01]\n",
      "  [-1.03161256e-02 -1.43509368e+00]\n",
      "  [ 1.41781104e-01  3.41411255e+00]]\n",
      "\n",
      " [[ 4.16441685e-02  2.89192928e+00]\n",
      "  [-4.69239976e-02 -1.79583622e+00]\n",
      "  [ 5.42768709e-02  9.99467126e-02]\n",
      "  [ 2.61575351e-03 -3.07320560e+00]\n",
      "  [-2.68139480e-02 -2.86296441e-01]\n",
      "  [-1.20481499e-01 -1.83501333e+00]\n",
      "  [ 3.75448849e-02  1.49384186e+00]\n",
      "  [-1.14890667e-01 -2.33966754e+00]\n",
      "  [ 6.53791460e-02 -6.11202336e-01]\n",
      "  [-8.21368213e-02 -4.69488393e-01]\n",
      "  [-2.31223520e-02 -4.45688502e-01]\n",
      "  [-1.14175008e-01 -4.89435359e+00]\n",
      "  [ 2.35118445e-02 -4.19862734e-01]\n",
      "  [ 1.30017753e-01  3.35433577e+00]]]\n",
      "I_training [[ 0.         -0.01589828 -0.01386019 -0.01668691 -0.00748263 -0.01451646\n",
      "  -0.01453539 -0.02588978 -0.03187165 -0.03303274 -0.03863011 -0.04666527\n",
      "  -0.04855618 -0.05106557 -0.05048044]\n",
      " [ 0.         -0.00572317  0.02495608 -0.01017528 -0.00949336 -0.01218049\n",
      "  -0.01292078 -0.00348709 -0.00524039 -0.01440734 -0.01486687  0.00617724\n",
      "   0.00984179  0.02120451 -0.03986468]\n",
      " [ 0.         -0.02823257 -0.02347358 -0.02303293 -0.03944138 -0.02498634\n",
      "  -0.02611461 -0.02648741  0.00059236  0.00518637 -0.0111194  -0.01223082\n",
      "  -0.01840758 -0.01171991 -0.00746744]\n",
      " [ 0.         -0.01729675 -0.01737392 -0.01884611  0.00712941  0.0355176\n",
      "   0.01022004  0.00978486  0.0112422  -0.00063123  0.02500706  0.05695428\n",
      "   0.06487615  0.06128321  0.08372285]\n",
      " [ 0.          0.01249325 -0.01346172 -0.00723492 -0.00675638 -0.00864455\n",
      "  -0.03067284 -0.02715447 -0.0507818  -0.04660588 -0.05333429 -0.05600914\n",
      "  -0.06620738 -0.0657823  -0.0574348 ]]\n",
      "MM_training [[[1.00000000e+00 9.00000000e-02]\n",
      "  [1.02366161e+00 2.87867832e-02]\n",
      "  [1.01558289e+00 1.20944629e-02]\n",
      "  [1.01791001e+00 5.34851416e-02]\n",
      "  [1.01265106e+00 2.64231033e-02]\n",
      "  [1.01399680e+00 3.55949787e-03]\n",
      "  [1.01537034e+00 9.36820386e-03]\n",
      "  [1.02479380e+00 8.45285941e-03]\n",
      "  [1.03205779e+00 1.29980694e-03]\n",
      "  [1.03314347e+00 1.13702035e-02]\n",
      "  [1.03728687e+00 1.77833802e-02]\n",
      "  [1.03866331e+00 5.85051990e-03]\n",
      "  [1.03834839e+00 2.17000819e-03]\n",
      "  [1.04221617e+00 1.68858305e-03]\n",
      "  [1.04320711e+00 1.52980135e-02]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.00404953e+00 1.15190027e-01]\n",
      "  [9.86615366e-01 6.28568433e-02]\n",
      "  [1.02117389e+00 2.29486903e-03]\n",
      "  [1.02137071e+00 4.06384323e-02]\n",
      "  [1.02069033e+00 3.77295281e-02]\n",
      "  [1.02503354e+00 2.43890034e-02]\n",
      "  [1.01508967e+00 9.82671043e-02]\n",
      "  [1.02479632e+00 5.06740632e-02]\n",
      "  [1.03215783e+00 1.56500991e-02]\n",
      "  [1.03307057e+00 3.62254943e-02]\n",
      "  [1.02356082e+00 3.17634654e-02]\n",
      "  [1.01810729e+00 1.40413773e-01]\n",
      "  [9.78149458e-01 2.13335264e-01]\n",
      "  [1.01635097e+00 3.58481158e-03]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.01679544e+00 3.50465057e-03]\n",
      "  [1.01196769e+00 1.99279594e-02]\n",
      "  [1.01451727e+00 5.38603914e-02]\n",
      "  [1.03600187e+00 9.72042913e-02]\n",
      "  [1.03289542e+00 2.70644361e-02]\n",
      "  [1.02970534e+00 4.25053668e-03]\n",
      "  [1.02686630e+00 4.08177170e-02]\n",
      "  [1.00123735e+00 5.65690756e-02]\n",
      "  [9.97001637e-01 7.54221535e-02]\n",
      "  [1.01434144e+00 3.10974742e-02]\n",
      "  [1.01544836e+00 9.80492175e-02]\n",
      "  [1.00956719e+00 3.51554447e-02]\n",
      "  [9.95690580e-01 5.16648525e-02]\n",
      "  [9.96467096e-01 1.35320016e-01]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.00946391e+00 2.76622717e-02]\n",
      "  [1.01047207e+00 1.39237398e-02]\n",
      "  [1.00853031e+00 6.69723518e-02]\n",
      "  [9.88329920e-01 7.13863844e-02]\n",
      "  [9.62083656e-01 7.70497012e-02]\n",
      "  [9.98060497e-01 2.92417600e-02]\n",
      "  [9.95264107e-01 3.55644461e-03]\n",
      "  [9.94749029e-01 8.04070050e-02]\n",
      "  [1.00422758e+00 5.70478843e-02]\n",
      "  [9.88787225e-01 6.71071070e-01]\n",
      "  [9.66792871e-01 6.72944117e-02]\n",
      "  [9.63946565e-01 1.21301272e-01]\n",
      "  [9.52364861e-01 2.50492564e-02]\n",
      "  [9.38478448e-01 6.47598841e-01]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.00420544e+00 3.05950379e-01]\n",
      "  [1.01851017e+00 1.31613223e-02]\n",
      "  [1.01591762e+00 3.34687864e-02]\n",
      "  [1.01228992e+00 4.95859036e-03]\n",
      "  [1.01158605e+00 3.34287961e-02]\n",
      "  [1.01723176e+00 8.78173911e-03]\n",
      "  [1.01484748e+00 4.22921588e-02]\n",
      "  [1.03461381e+00 4.07967885e-03]\n",
      "  [1.03117454e+00 6.71040294e-03]\n",
      "  [1.03558265e+00 1.33824070e-02]\n",
      "  [1.03878542e+00 7.97826686e-03]\n",
      "  [1.04716524e+00 3.26863818e-04]\n",
      "  [1.04781743e+00 4.12198987e-03]\n",
      "  [1.03915182e+00 8.68988022e-02]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Show Head of the training data\")\n",
    "print(\"S_training\", S_training[:5])\n",
    "print(\"V_training\", V_training[:5])\n",
    "print(\"Payoff_training\", Payoff_training[:5])\n",
    "print(\"dW_training\", dW_training[:5])\n",
    "print(\"I_training\", I_training[:5])\n",
    "print(\"MM_training\", MM_training[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68169f97",
   "metadata": {
    "id": "68169f97"
   },
   "outputs": [],
   "source": [
    "#compute the volatility processes\n",
    "vol_training = np.sqrt(V_training)\n",
    "vol_testing = np.sqrt(V_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LW4SLN7bQy4R",
   "metadata": {
    "id": "LW4SLN7bQy4R"
   },
   "source": [
    "## Step 2: Signature computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NUEKCKWsRPy4",
   "metadata": {
    "id": "NUEKCKWsRPy4"
   },
   "source": [
    "We will make us uf the iisignature package https://pypi.org/project/iisignature/ to compute the signature, and it can be installed using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14790b8",
   "metadata": {
    "id": "d14790b8"
   },
   "source": [
    "We import our signature computation module, which can compute various signature and log signature lift related to the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa867a0",
   "metadata": {
    "id": "1fa867a0"
   },
   "outputs": [],
   "source": [
    "from Signature_computer import SignatureComputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eb781",
   "metadata": {
    "id": "fe2eb781"
   },
   "source": [
    "Next we initialize the **SignatureComputer**, which allows to choose from the linear and the log signature, and various choices of signature lifts. Here are some examples $$ t\\mapsto \\mathrm{Sig}(A_t,X_t),t\\mapsto \\mathrm{Sig}(A_t,\\phi(X)_t),t\\mapsto \\mathrm{Sig}(A_t,X_t,X_{t-\\epsilon}),t\\mapsto \\mathrm{Sig}(A_t,X_t,\\phi(X_t)),t\\mapsto \\mathrm{Sig}(A_t,v_t),$$ where $t\\mapsto A_t$ is a monoton path and in our examples we choose between $$A_t=t, \\quad  A_t = \\langle X\\rangle_t.$$ Additonally we can add Laguerre polynomials of $X$ or $(X,v)$ to the signature, see the module for all the details.\n",
    "\n",
    "In this example we choose the basis $(\\mathrm{Sig}(t,v_t),p_i(X_t))$, which proves to be a solid choice for rough volatility models. We choose both the polynomial and signature degree to be $3$ for this example. (To improve result one should higher truncations levels ($4-5$) for the signature, but to keep the complexity reasonable here we choose level $3$ signatures.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a377c",
   "metadata": {
    "id": "8c4a377c"
   },
   "outputs": [],
   "source": [
    "#initialize signature computer\n",
    "sig_computer = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c365145",
    "outputId": "d26148d9-199d-4ebd-bd3f-60524818666b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing linear signature with polynomial-vol lift\n",
      "Computing linear signature with polynomial-vol lift\n"
     ]
    }
   ],
   "source": [
    "#Compute the signature for training and test data\n",
    "tt = np.linspace(0,T,N+1)\n",
    "A_training = np.zeros((M, N+1)) #time-augmentation\n",
    "A_testing = np.zeros((M2, N+1))\n",
    "A_training[:, 1:] = A_testing[:, 1:] = tt[1:]\n",
    "signatures_training = sig_computer.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_testing = sig_computer.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B0iPgqVDUXyd",
   "metadata": {
    "id": "B0iPgqVDUXyd"
   },
   "source": [
    "Some example of the signature paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qLgGhAQqUa-K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "qLgGhAQqUa-K",
    "outputId": "e276704a-7235-49f0-98cd-04e9f91b9cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x32bc61420>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(signatures_training[0,:,0],label = 'first level (constant)')\n",
    "plt.plot(signatures_training[0,:,1],label = '1st level: time ')\n",
    "plt.plot(signatures_training[0,:,2],label = '2nd level: $v_t-v_0$')\n",
    "plt.plot(signatures_training[0,:,4],label = '3th level: $\\int sdv_s $')\n",
    "plt.plot(signatures_training[0,:,12],label = '4th level: $\\int \\int udv_u dv_s $')\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19711264",
   "metadata": {
    "id": "19711264"
   },
   "source": [
    "## Step 3: Compute pricing intervals with linear signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c7788",
   "metadata": {
    "id": "180c7788"
   },
   "source": [
    "We can now import the linear primal and dual pricers, which compute true lower and upper bounds.\n",
    "- The **LinearLongstaffSchwartzPricer** uses the signature of the training data to recursively approximate continuation values in the spirit of the Longstaff-Schwartz algorithm (descibed in detail in Section 3.1 of https://arxiv.org/abs/2312.03444). The resulting regression coefficients at each exercise date provide a stopping rule, which can be applied to the testing data to get true lower-bounds\n",
    "- The **LinearDualPricer** uses the signature of the training data to minimize over the familiy of linear signature martingales, by solving a corresponding linear program (described in Detail in Section 3.2 of https://arxiv.org/abs/2312.03444). The resulting coefficients yield a Doob martingale approximation, which for the testing data yields a true upper bound.\n",
    "By combining the two values, we receive confidence intervals for the true option price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eTaJQW5V8q6",
   "metadata": {
    "id": "1eTaJQW5V8q6"
   },
   "source": [
    "To solve the linear programm, one can optionally choose to use Gurobi https://www.gurobi.com, which requires a free licence, which is recommended especially for high-dimensional LPs, which occur when choosing large sample-sizes and/or high signature truncations levels. Alternatively, we use the free LP solvers from CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc73ec",
   "metadata": {
    "id": "edbc73ec"
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# add root of repo and the “Linear signature optimal stopping” folder to PYTHONPATH\n",
    "import sys, os\n",
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "\n",
    "# now the module can be imported\n",
    "from Linear_signature_optimal_stopping import LinearLongstaffSchwartzPricer, LinearDualPricer\n",
    "# ────────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a888326",
   "metadata": {
    "id": "5a888326"
   },
   "outputs": [],
   "source": [
    "\n",
    "#initialze the models\n",
    "ls_pricer = LinearLongstaffSchwartzPricer(\n",
    "        N1=N1,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        mode=\"American Option\",\n",
    "        ridge=10**(-9)\n",
    "    )\n",
    "\n",
    "dual_pricer = LinearDualPricer(\n",
    "        N1=N1,\n",
    "        N=N,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        LP_solver=\"CVXPY\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4f4d",
   "metadata": {
    "id": "28db4f4d"
   },
   "source": [
    "The choice mode=\"American Option\" indicates that the Longstaff-Schwartz recursion will only consider \"in-the-money\" paths, which was originally suggested by Longstaff & Schwartz, and is reasonable for non-negative payoffs. For general payoffs we can use mode = \"Standard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889e9a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0889e9a8",
    "outputId": "8afb4b10-9708-46a7-f241-da85ac65ee99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score at exercise date 13 0.9230387994767344\n",
      "Regression score at exercise date 12 0.8547423242465029\n",
      "Regression score at exercise date 11 0.8254892738204396\n",
      "Regression score at exercise date 10 0.7666234289692556\n",
      "Regression score at exercise date 9 0.7140860456598603\n",
      "Regression score at exercise date 8 0.6602038367312737\n",
      "Regression score at exercise date 7 0.5951556651683019\n",
      "Regression score at exercise date 6 0.5382641887519765\n",
      "Regression score at exercise date 5 0.4912708706639649\n",
      "Regression score at exercise date 4 0.4125772563404657\n",
      "Regression score at exercise date 3 0.3382086908456857\n",
      "Regression score at exercise date 2 0.2314366835999213\n",
      "Regression score at exercise date 1 0.11278063759931978\n"
     ]
    }
   ],
   "source": [
    "#compute true lower bounds\n",
    "lower_bound, lower_bound_std, ls_regression_models = ls_pricer.price(\n",
    "        signatures_training,\n",
    "        Payoff_training,\n",
    "        signatures_testing,\n",
    "        Payoff_testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a34fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e64a34fa",
    "outputId": "b116657f-4c30-4379-dbfb-5aa71549133d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Longstaff-Schwartz lower bound: 0.0534315029464392 ± 0.0005098966689766467\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Longstaff-Schwartz lower bound: {lower_bound} ± {lower_bound_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a51c2",
   "metadata": {
    "id": "676a51c2"
   },
   "source": [
    "Similarly let us derive the upper bounds, but we will train the model only for $M= 5000$ paths to reduce computation time, and then compute true prices for all testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba8b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08fba8b2",
    "outputId": "710b6932-eb7b-4111-c511-20de6dd42eb6",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[157], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m M_dual \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n\u001b[0;32m----> 2\u001b[0m upper_bound, upper_bound_std, MG \u001b[38;5;241m=\u001b[39m \u001b[43mdual_pricer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignatures_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_dual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPayoff_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_dual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdW_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_dual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Select only the first component of the Brownian increments\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43msignatures_testing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPayoff_testing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdW_testing\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Select only the first component of the Brownian increments\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/Linear signature optimal stopping/Linear_signature_optimal_stopping.py:237\u001b[0m, in \u001b[0;36mLinearDualPricer.price\u001b[0;34m(self, S_training_sig, Payoff_training, dW_training, S_testing_sig, Payoff_testing, dW_testing)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLP_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCVXPY\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    227\u001b[0m     solver \u001b[38;5;241m=\u001b[39m LPSolver(\n\u001b[1;32m    228\u001b[0m         F\u001b[38;5;241m=\u001b[39mPayoff_training[:,:N_used\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m    229\u001b[0m         tt\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m         subindex\u001b[38;5;241m=\u001b[39msubindex2\n\u001b[1;32m    236\u001b[0m     )\n\u001b[0;32m--> 237\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve_cvxpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid LP solver: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLP_solver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/Linear signature optimal stopping/Linear_signature_optimal_stopping.py:324\u001b[0m, in \u001b[0;36mLPSolver.solve_cvxpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    322\u001b[0m constraints \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mA \u001b[38;5;241m@\u001b[39m x]\n\u001b[1;32m    323\u001b[0m prob \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mProblem(objective, constraints)\n\u001b[0;32m--> 324\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m xx \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    327\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/problems/problem.py:600\u001b[0m, in \u001b[0;36mProblem.solve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    598\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot specify both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver_path\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please choose one.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_solve_solver_path(solve_func,solver_path, args, kwargs)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msolve_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/problems/problem.py:1170\u001b[0m, in \u001b[0;36mProblem._solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpack(chain\u001b[38;5;241m.\u001b[39mretrieve(soln))\n\u001b[1;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m-> 1170\u001b[0m data, solving_chain, inverse_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_problem_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_dpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_dpp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcanon_backend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28mprint\u001b[39m(_NUM_SOLVER_STR)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/problems/problem.py:793\u001b[0m, in \u001b[0;36mProblem.get_problem_data\u001b[0;34m(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\u001b[0m\n\u001b[1;32m    790\u001b[0m     s\u001b[38;5;241m.\u001b[39mLOGGER\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    791\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompiling problem (target solver=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m, solver_name)\n\u001b[1;32m    792\u001b[0m     s\u001b[38;5;241m.\u001b[39mLOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReduction chain: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, reduction_chain_str)\n\u001b[0;32m--> 793\u001b[0m data, inverse_data \u001b[38;5;241m=\u001b[39m \u001b[43msolving_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m safe_to_cache \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m)\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m s\u001b[38;5;241m.\u001b[39mPARAM_PROB \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(reduction, EvalParams)\n\u001b[1;32m    798\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m reduction \u001b[38;5;129;01min\u001b[39;00m solving_chain\u001b[38;5;241m.\u001b[39mreductions)\n\u001b[1;32m    799\u001b[0m )\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compilation_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/reductions/chain.py:76\u001b[0m, in \u001b[0;36mChain.apply\u001b[0;34m(self, problem, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     75\u001b[0m         s\u001b[38;5;241m.\u001b[39mLOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mApplying reduction \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m(r)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     problem, inv \u001b[38;5;241m=\u001b[39m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     inverse_data\u001b[38;5;241m.\u001b[39mappend(inv)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m problem, inverse_data\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/reductions/dcp2cone/cone_matrix_stuffing.py:386\u001b[0m, in \u001b[0;36mConeMatrixStuffing.apply\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Batch expressions together, then split apart.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m expr_list \u001b[38;5;241m=\u001b[39m [arg \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m ordered_cons \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39margs]\n\u001b[0;32m--> 386\u001b[0m params_to_problem_data \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m inverse_data\u001b[38;5;241m.\u001b[39mminimize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(problem\u001b[38;5;241m.\u001b[39mobjective) \u001b[38;5;241m==\u001b[39m Minimize\n\u001b[1;32m    389\u001b[0m variables \u001b[38;5;241m=\u001b[39m problem\u001b[38;5;241m.\u001b[39mvariables()\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/utilities/coeff_extractor.py:72\u001b[0m, in \u001b[0;36mCoeffExtractor.affine\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m     70\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([e\u001b[38;5;241m.\u001b[39msize \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m expr_list])\n\u001b[1;32m     71\u001b[0m op_list \u001b[38;5;241m=\u001b[39m [e\u001b[38;5;241m.\u001b[39mcanonical_form[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m expr_list]\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcanonInterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_problem_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_to_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_id_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                                         \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanon_backend\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/canonInterface.py:297\u001b[0m, in \u001b[0;36mget_problem_matrix\u001b[0;34m(linOps, var_length, id_to_col, param_to_size, param_to_col, constr_length, canon_backend)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canon_backend \u001b[38;5;241m==\u001b[39m s\u001b[38;5;241m.\u001b[39mCPP_CANON_BACKEND:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcvxpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcvxcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcppbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_matrix\n\u001b[0;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid_to_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_to_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_to_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstr_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinOps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m canon_backend \u001b[38;5;129;01min\u001b[39;00m {s\u001b[38;5;241m.\u001b[39mSCIPY_CANON_BACKEND, s\u001b[38;5;241m.\u001b[39mRUST_CANON_BACKEND,\n\u001b[1;32m    300\u001b[0m                        s\u001b[38;5;241m.\u001b[39mNUMPY_CANON_BACKEND}:\n\u001b[1;32m    301\u001b[0m     param_size_plus_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(param_to_size\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cppbackend.py:56\u001b[0m, in \u001b[0;36mbuild_matrix\u001b[0;34m(id_to_col, param_to_size, param_to_col, var_length, constr_length, linOps)\u001b[0m\n\u001b[1;32m     54\u001b[0m linPy_to_linC \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lin \u001b[38;5;129;01min\u001b[39;00m linOps:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mbuild_lin_op_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinPy_to_linC\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     tree \u001b[38;5;241m=\u001b[39m linPy_to_linC[lin]\n\u001b[1;32m     58\u001b[0m     lin_vec\u001b[38;5;241m.\u001b[39mpush_back(tree)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cppbackend.py:226\u001b[0m, in \u001b[0;36mbuild_lin_op_tree\u001b[0;34m(root_linPy, linPy_to_linC)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m post_order_stack:\n\u001b[1;32m    225\u001b[0m     linPy \u001b[38;5;241m=\u001b[39m post_order_stack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m--> 226\u001b[0m     \u001b[43mmake_linC_from_linPy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinPy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinPy_to_linC\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cppbackend.py:185\u001b[0m, in \u001b[0;36mmake_linC_from_linPy\u001b[0;34m(linPy, linPy_to_linC)\u001b[0m\n\u001b[1;32m    183\u001b[0m     linC\u001b[38;5;241m.\u001b[39mset_data_ndim(\u001b[38;5;28mlen\u001b[39m(linPy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[43mset_linC_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinPy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cppbackend.py:157\u001b[0m, in \u001b[0;36mset_linC_data\u001b[0;34m(linC, linPy)\u001b[0m\n\u001b[1;32m    155\u001b[0m     linC\u001b[38;5;241m.\u001b[39mset_data_ndim(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mset_matrix_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinPy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cppbackend.py:244\u001b[0m, in \u001b[0;36mset_matrix_data\u001b[0;34m(linC, linPy)\u001b[0m\n\u001b[1;32m    236\u001b[0m     linC\u001b[38;5;241m.\u001b[39mset_sparse_data(\n\u001b[1;32m    237\u001b[0m         coo\u001b[38;5;241m.\u001b[39mdata,\n\u001b[1;32m    238\u001b[0m         coo\u001b[38;5;241m.\u001b[39mrow\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         coo\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    242\u001b[0m     )\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     \u001b[43mlinC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_dense_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformat_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinPy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinPy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     linC\u001b[38;5;241m.\u001b[39mset_data_ndim(\u001b[38;5;28mlen\u001b[39m(linPy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/cvxpy/cvxcore/python/cvxcore.py:198\u001b[0m, in \u001b[0;36mLinOp.set_dense_data\u001b[0;34m(self, matrix)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_dense_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, matrix):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cvxcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinOp_set_dense_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "M_dual = 5000\n",
    "upper_bound, upper_bound_std, MG = dual_pricer.price(\n",
    "        signatures_training[:M_dual],\n",
    "        Payoff_training[:M_dual],\n",
    "        dW_training[:M_dual,:,0],  # Select only the first component of the Brownian increments\n",
    "        signatures_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # Select only the first component of the Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d591c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d591c8",
    "outputId": "e078ca47-ef36-4133-d227-c6d2b88c391d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Dual upper bound: 0.06699659644005387 ± 2.2986797798329457e-06\n",
      "Pricing interval: (0.0534315029464392, 0.06699659644005387)± 0.0005098966689766467 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Dual upper bound: {upper_bound} ± {upper_bound_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(float(lower_bound),float(upper_bound))}± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab12b0",
   "metadata": {
    "id": "06ab12b0"
   },
   "source": [
    "# Improving the duality gap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hir98wJJXKE8",
   "metadata": {
    "id": "hir98wJJXKE8"
   },
   "source": [
    "Especially in rough regimes (here $H=0.1$), we observe a significant gap between lower and upper bounds, and in this section we present two ways to improve it. The first one still relies on linear signatures, but extends the basis as explained in in Section 4 of https://arxiv.org/abs/2312.03444."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ea7df",
   "metadata": {
    "id": "4d7ea7df"
   },
   "source": [
    "## Part 1: Extending the linear basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb042",
   "metadata": {
    "id": "beffb042"
   },
   "source": [
    "We consider a more involved basis by choosing the extended signature lift of $(t,X_t,\\phi(X_t))$, and additionally add Laguerre polynomials of $(X_t,v_t)$. We can again use the SignatureComputer to compute this extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb54f12",
   "metadata": {
    "id": "1eb54f12"
   },
   "outputs": [],
   "source": [
    "sig_computer_extended = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"payoff-and-polynomial-extended\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6249a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d6249a",
    "outputId": "53179ca8-2f48-4b44-b1eb-99f3548c0900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing linear signature with payoff-and-polynomial-extended lift\n",
      "Computing linear signature with payoff-and-polynomial-extended lift\n"
     ]
    }
   ],
   "source": [
    "signatures_extended_training = sig_computer_extended.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_extended_testing = sig_computer_extended.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zFWMoC-X-kM",
   "metadata": {
    "id": "5zFWMoC-X-kM"
   },
   "source": [
    "Now we repeat the procedure for the extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d3960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1c2d3960",
    "outputId": "009708e1-bce5-42b9-861f-3f437c5136e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score at exercise date 13 0.9233911243553715\n",
      "Regression score at exercise date 12 0.8599873400607905\n",
      "Regression score at exercise date 11 0.8305999636579253\n",
      "Regression score at exercise date 10 0.7689441195976593\n",
      "Regression score at exercise date 9 0.7305343238629651\n",
      "Regression score at exercise date 8 0.6994314294269418\n",
      "Regression score at exercise date 7 0.6400584891405384\n",
      "Regression score at exercise date 6 0.5827565170356735\n",
      "Regression score at exercise date 5 0.5238983918406865\n",
      "Regression score at exercise date 4 0.43953143240069137\n",
      "Regression score at exercise date 3 0.3573490799888033\n",
      "Regression score at exercise date 2 0.2452442530066059\n",
      "Regression score at exercise date 1 0.1108803322688614\n",
      "8.53 seconds needed to solve the linear program using CVXPY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#compute true lower bounds for the new basis\n",
    "lower_bound_extended, lower_bound_extended_std, ls_regression_models_extended = ls_pricer.price(\n",
    "        signatures_extended_training,\n",
    "        Payoff_training,\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing\n",
    "    )\n",
    "#Repeating the dual procedure for the new basis\n",
    "upper_bound_extended, upper_bound_extended_std, MG_extended = dual_pricer.price(\n",
    "        signatures_extended_training[:M_dual,:,:],\n",
    "        Payoff_training[:M_dual,:],\n",
    "        dW_training[:M_dual,:,0],  # select first component of Brownian increments\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # select first component of Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "692d7161",
    "outputId": "5811cd15-9104-4ec6-fb2a-31da9a47991c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improve pricing interval: (0.05343432121740919, 0.06667238204520731)± 0.0005098966689766467 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Improve pricing interval: {(float(lower_bound_extended),float(upper_bound_extended))}± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5c7df",
   "metadata": {
    "id": "17b5c7df"
   },
   "source": [
    "## Part 2: Deep log-signature optimal stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5f596",
   "metadata": {
    "id": "b7e5f596"
   },
   "source": [
    " In forthcoming work about \"American options in rough volatility models\", we will focus on more non-linear apporaches to price American options. More precisely, we extend the primal and dual procecdure by replacing linear functionals of the signature by deep neural networks on the log-signature $\\mathbb{L}=\\mathrm{log}^\\otimes(\\mathbb{X})$. This transformed version of the signature still captures the relevant information about the past of the underlying process, but grows much slower as the signature it self with respect to the truncation. Then, in order to learn highly non-linear functionals, such as the integrand of the Doob martingale (\"derivative of the Snell-envelope\"), we apply deep feedforward neural networks $\\theta$ on the log-signature. Of course, in both methods a optimization of the hyperparameters is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTyO0IKtbYse",
   "metadata": {
    "id": "kTyO0IKtbYse"
   },
   "source": [
    "We proceed as before, but replace the linear signature by the log-signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54699ea8",
   "metadata": {
    "id": "54699ea8"
   },
   "outputs": [],
   "source": [
    "sig_computer_log = SignatureComputer(T, N, 3, \"log\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998aed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e998aed9",
    "outputId": "195d4128-74a3-4f42-a198-6310039f6c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (8192, 15), vol shape: (8192, 15), A shape: (8192, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (8192, 15), vol shape: (8192, 15), A shape: (8192, 253)\n",
      "Using 15 time steps for log signature computation\n"
     ]
    }
   ],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use first component and correct I_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ff201",
   "metadata": {
    "id": "bc7ff201"
   },
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_signatures_optimal_stopping import DeepLongstaffSchwartzPricer, DeepDualPricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bdf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (8192, 15), vol shape: (8192, 15), A shape: (8192, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (8192, 15), vol shape: (8192, 15), A shape: (8192, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "shape of dW_training (8192, 14, 2)\n",
      "shape of dW_testing (8192, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use first component and correct I_testing\n",
    ")\n",
    "print(\"shape of dW_training\", dW_training.shape)\n",
    "print(\"shape of dW_testing\", dW_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d0eff",
   "metadata": {
    "id": "9a1d0eff"
   },
   "source": [
    "The DeepLongstaffSchwartzPricer generalizes the LinearLongstaffSchwartzPrices, where the Ridge Regression at each exercise date is replace by learning the conditional expectations via neural networks. In the following initialization we build a network with $3$ hidden layers and $16$ neurons each, between each hidden layer we apply the activation function $\\mathrm{tanh}(x)$. The remainding parameters are set to 'False'. (One can run the 'Hyperparameter_optimization_primal.py' file to optimize the choice of hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da432f",
   "metadata": {
    "id": "b2da432f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression at exercise date 13\n",
      "Epoch 1/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0046 - mae: 0.0520\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 9.2510e-04 - mae: 0.0232\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 4.0571e-04 - mae: 0.0139\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.5496e-04 - mae: 0.0124\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.4844e-04 - mae: 0.0121\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.4603e-04 - mae: 0.0120\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.4075e-04 - mae: 0.0119\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 3.3634e-04 - mae: 0.0119\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.4719e-04 - mae: 0.0119\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 3.4136e-04 - mae: 0.0121\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 0s 5ms/step - loss: 3.3466e-04 - mae: 0.0118\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.3281e-04 - mae: 0.0117\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.3338e-04 - mae: 0.0118\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.6717e-04 - mae: 0.0130\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 3.4433e-04 - mae: 0.0123\n",
      "201/201 [==============================] - 0s 653us/step\n",
      "Regression at exercise date 12\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 8.6088e-04 - mae: 0.0213\n",
      "203/203 [==============================] - 0s 642us/step\n",
      "Regression at exercise date 11\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 4.7645e-04 - mae: 0.0153\n",
      "207/207 [==============================] - 0s 638us/step\n",
      "Regression at exercise date 10\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 9.8681e-04 - mae: 0.0228\n",
      "213/213 [==============================] - 0s 642us/step\n",
      "Regression at exercise date 9\n",
      "27/27 [==============================] - 0s 4ms/step - loss: 5.3729e-04 - mae: 0.0157\n",
      "216/216 [==============================] - 0s 639us/step\n",
      "Regression at exercise date 8\n",
      "28/28 [==============================] - 0s 4ms/step - loss: 8.1134e-04 - mae: 0.0190\n",
      "221/221 [==============================] - 0s 643us/step\n",
      "Regression at exercise date 7\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 8.7307e-04 - mae: 0.0196\n",
      "226/226 [==============================] - 0s 650us/step\n",
      "Regression at exercise date 6\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0012 - mae: 0.0237\n",
      "232/232 [==============================] - 0s 649us/step\n",
      "Regression at exercise date 5\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 0.0011 - mae: 0.0226\n",
      "237/237 [==============================] - 0s 640us/step\n",
      "Regression at exercise date 4\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 6.3389e-04 - mae: 0.0157\n",
      "243/243 [==============================] - 0s 643us/step\n",
      "Regression at exercise date 3\n",
      "31/31 [==============================] - 0s 4ms/step - loss: 5.3118e-04 - mae: 0.0155\n",
      "248/248 [==============================] - 0s 652us/step\n",
      "Regression at exercise date 2\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 6.6760e-04 - mae: 0.0172\n",
      "252/252 [==============================] - 0s 656us/step\n",
      "Regression at exercise date 1\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 8.4090e-04 - mae: 0.0195\n",
      "255/255 [==============================] - 0s 642us/step\n",
      "256/256 [==============================] - 0s 657us/step\n",
      "256/256 [==============================] - 0s 655us/step\n",
      "256/256 [==============================] - 0s 671us/step\n",
      "256/256 [==============================] - 0s 668us/step\n",
      "256/256 [==============================] - 0s 653us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "256/256 [==============================] - 0s 669us/step\n",
      "256/256 [==============================] - 0s 753us/step\n",
      "256/256 [==============================] - 0s 729us/step\n",
      "256/256 [==============================] - 0s 713us/step\n",
      "256/256 [==============================] - 0s 671us/step\n",
      "256/256 [==============================] - 0s 653us/step\n",
      "256/256 [==============================] - 0s 643us/step\n",
      "Signature data shape: (8192, 15, 8)\n",
      "Payoff data shape: (8192, 15)\n",
      "dW data shape: (8192, 14)\n",
      "Using 14 time steps instead of 252 for model building\n",
      "Using indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] for 14 exercise dates\n",
      "Using indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] for 14 exercise dates\n",
      "Model expects Payoff shape: (batch_size, 14)\n",
      "Trimmed Payoff shape: (8192, 14)\n",
      "Epoch 1/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0848 - val_loss: 0.0825\n",
      "Epoch 2/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0834 - val_loss: 0.0828\n",
      "Epoch 3/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0833 - val_loss: 0.0825\n",
      "Epoch 4/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0832 - val_loss: 0.0824\n",
      "Epoch 5/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.0825\n",
      "Epoch 6/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0832 - val_loss: 0.0826\n",
      "Epoch 7/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0834 - val_loss: 0.0828\n",
      "Epoch 8/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.0827\n",
      "Epoch 9/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.0823\n",
      "Epoch 10/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0823\n",
      "Epoch 11/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0822\n",
      "Epoch 12/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0823\n",
      "Epoch 13/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0829 - val_loss: 0.0821\n",
      "Epoch 14/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0830 - val_loss: 0.0826\n",
      "Epoch 15/15\n",
      "29/29 [==============================] - 0s 8ms/step - loss: 0.0831 - val_loss: 0.0827\n",
      "256/256 [==============================] - 1s 2ms/step\n",
      "256/256 [==============================] - 0s 2ms/step\n",
      "MG shape: (8192, 14)\n",
      "MG with zeros shape: (8192, 15)\n"
     ]
    }
   ],
   "source": [
    "ls_pricer = DeepLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='tanh',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # This is correct as float\n",
    "    dropout=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "\n",
    "dual_pricer = DeepDualPricer(\n",
    "    N1=N1,\n",
    "    N=N,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # ERROR: should be float, not boolean\n",
    "    dropout=False,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "# LS pricer call is correct\n",
    "lower_bound_deep, lower_bound_deep_std, ls_regression_models = ls_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    M_val=0,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Dual pricer call is correct\n",
    "y0, upper_bound_deep, upper_bound_deep_std, dual_model, dual_rule_model = dual_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    dW_training[:,:,0],  # use only first component of Brownian increments\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    dW_testing[:,:,0],  # use only first component of Brownian increments\n",
    "    M_val=int(0.9*M),\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jX2tyIWchsd",
   "metadata": {
    "id": "9jX2tyIWchsd"
   },
   "source": [
    "Similarly for the dual problem, we consider the same network but use the $relu(x)$ activation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186917ea",
   "metadata": {
    "id": "186917ea"
   },
   "outputs": [],
   "source": [
    "# Consistent parameter usage for validation set size\n",
    "M_val_percentage = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cDQUUvN_c6wl",
   "metadata": {
    "id": "cDQUUvN_c6wl"
   },
   "source": [
    "The Deep Longstaff Schwartz uses $15$ epochs for at the last exercise date, and then one epochs at the remainding ones by initiliazing smartly. The learning rate for the Stochastic Gradient Descent is choosen as $0.001$, and we use batch sizes of $2^8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e9ff6",
   "metadata": {
    "id": "d04e9ff6",
    "outputId": "8fd83460-c4c2-40ca-8718-2b02f3227db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Longstaff-Schwartz lower bound: 0.05024243806202926 ± 0.00024894886845755265\n"
     ]
    }
   ],
   "source": [
    "print(f\"Deep Longstaff-Schwartz lower bound: {lower_bound_deep} ± {lower_bound_deep_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RsfNfQyIebJD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsfNfQyIebJD",
    "outputId": "7fa5e436-d332-4232-c5e7-1a2784f5d8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Dual upper bound: 0.07187956871392982 ± 0.000286167717180184\n",
      "Pricing interval: (0.05024243806202926, 0.07187956871392982)± 0.000286167717180184 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Deep Dual upper bound: {upper_bound_deep} ± {upper_bound_deep_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(lower_bound_deep,upper_bound_deep)}± {np.maximum(upper_bound_deep_std,lower_bound_deep_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_kernel_signature_optimal_stopping import DeepKernelLongstaffSchwartzPricer, DeepKernelDualPricer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96187943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Define the RFF feature computation functions with corrected implementation\n",
    "\n",
    "def compute_rff_kernel_features(signatures, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for lower bound pricer (list format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of tensors with shape [M, rff_dim*2, 1] for each exercise date\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices based on actual data dimensions\n",
    "    actual_steps = T_steps - 1\n",
    "    subindex = [min(int((j+1)*actual_steps/N1), actual_steps) for j in range(N1)]\n",
    "    \n",
    "    print(f\"Signature data has {T_steps} time points\")\n",
    "    print(f\"Using exercise indices: {subindex}\")\n",
    "    \n",
    "    # Create list to hold RFF features for each exercise date\n",
    "    rff_features_list = []\n",
    "    \n",
    "    # For each exercise date\n",
    "    for t in range(len(subindex)):\n",
    "        idx = min(subindex[t], T_steps-1)\n",
    "        X_t = signatures[:, idx, :]\n",
    "        \n",
    "        # Generate random projection matrix for RBF kernel approximation\n",
    "        np.random.seed(42 + t)  # Different seed for each exercise date\n",
    "        W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "        \n",
    "        # Compute RFF: [cos(Wx), sin(Wx)]\n",
    "        projection = X_t @ W\n",
    "        rff_features = np.column_stack([\n",
    "            np.cos(projection),\n",
    "            np.sin(projection)\n",
    "        ]) * np.sqrt(1/rff_dim)\n",
    "        \n",
    "        # Reshape to match expected format: [M, rff_dim*2, 1]\n",
    "        rff_features = rff_features.reshape(M, rff_dim*2, 1)\n",
    "        \n",
    "        rff_features_list.append(rff_features)\n",
    "    \n",
    "    return rff_features_list\n",
    "\n",
    "def compute_rff_kernel_features_dual(signatures, N, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for dual pricer (3D tensor format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N: Number of time steps in the discretization\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with shape [M, features, time] for all time points\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices proportional to exercise dates\n",
    "    # We need to map our exercise indices to the full discretization grid\n",
    "    actual_steps = min(T_steps - 1, N)\n",
    "    all_indices = np.minimum(np.array([int(t * T_steps / (N+1)) for t in range(N+1)]), T_steps-1)\n",
    "    \n",
    "    print(f\"Using exercise indices for dual pricer: {all_indices[:5]}...{all_indices[-5:]}\")\n",
    "    \n",
    "    # Generate random projection matrix once\n",
    "    np.random.seed(42)\n",
    "    W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "    \n",
    "    # Extract all required signature data at once\n",
    "    X_all = signatures[:, all_indices, :]  # Shape: [M, N+1, feature_dim]\n",
    "    \n",
    "    # Reshape for batch matrix multiplication\n",
    "    X_reshaped = X_all.reshape(-1, feature_dim)  # Shape: [M*(N+1), feature_dim]\n",
    "    \n",
    "    # Compute all projections at once\n",
    "    projections = X_reshaped @ W  # Shape: [M*(N+1), rff_dim]\n",
    "    \n",
    "    # Compute RFF features\n",
    "    cos_features = np.cos(projections)\n",
    "    sin_features = np.sin(projections)\n",
    "    rff_features = np.column_stack([cos_features, sin_features]) * np.sqrt(1/rff_dim)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    full_rff = rff_features.reshape(M, N+1, rff_dim*2)\n",
    "    \n",
    "    # Transpose to match expected format: [M, features, time]\n",
    "    return np.transpose(full_rff, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565afbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing kernel features for lower bound...\n",
      "Signature data has 15 time points\n",
      "Using exercise indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Signature data has 15 time points\n",
      "Using exercise indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Computing kernel features for upper bound...\n",
      "Using exercise indices for dual pricer: [0 1 2 3 4]...[10 11 12 13 14]\n",
      "Using exercise indices for dual pricer: [0 1 2 3 4]...[10 11 12 13 14]\n",
      "Initializing kernel-based lower bound...\n",
      "Computing kernel-based lower bound...\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_30 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_180 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_304 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_181 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_182 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_305 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_183 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_184 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_306 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_185 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_307 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 13\n",
      "Epoch 1/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.3145 - r2_score: -49.7777\n",
      "Epoch 2/15\n",
      "26/26 [==============================] - 0s 15ms/step - loss: 0.1544 - r2_score: -8.7776\n",
      "Epoch 3/15\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1378 - r2_score: -4.6126\n",
      "Epoch 4/15\n",
      "26/26 [==============================] - 0s 14ms/step - loss: 0.1310 - r2_score: -2.9759\n",
      "Epoch 5/15\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1320 - r2_score: -3.3535\n",
      "Epoch 6/15\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1290 - r2_score: -2.6804\n",
      "Epoch 7/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1270 - r2_score: -2.3082\n",
      "Epoch 8/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1227 - r2_score: -1.3214\n",
      "Epoch 9/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1205 - r2_score: -0.9050\n",
      "Epoch 10/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1204 - r2_score: -1.0230\n",
      "Epoch 11/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1182 - r2_score: -0.6124\n",
      "Epoch 12/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1186 - r2_score: -0.8603\n",
      "Epoch 13/15\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1158 - r2_score: -0.3015\n",
      "Epoch 14/15\n",
      "26/26 [==============================] - 0s 12ms/step - loss: 0.1151 - r2_score: -0.2657\n",
      "Epoch 15/15\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1137 - r2_score: -0.0782\n",
      "201/201 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_31 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_186 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_308 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_187 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_188 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_309 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_189 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_190 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_310 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_191 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_311 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 12\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.2240 - r2_score: -29.3449\n",
      "203/203 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_32 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_192 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_312 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_193 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_194 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_313 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_195 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_196 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_314 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_197 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_315 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 11\n",
      "26/26 [==============================] - 0s 13ms/step - loss: 0.1765 - r2_score: -17.7515\n",
      "207/207 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_33 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_198 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_316 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_199 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_200 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_317 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_201 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_202 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_318 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_203 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_319 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 10\n",
      "27/27 [==============================] - 0s 12ms/step - loss: 0.2636 - r2_score: -46.5331\n",
      "213/213 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_34 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_204 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_320 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_205 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_206 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_321 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_207 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_208 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_322 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_209 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_323 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 9\n",
      "27/27 [==============================] - 0s 14ms/step - loss: 0.1819 - r2_score: -21.5629\n",
      "216/216 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_35 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_210 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_324 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_211 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_212 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_325 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_213 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_214 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_326 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_215 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_327 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 8\n",
      "28/28 [==============================] - 0s 13ms/step - loss: 0.1444 - r2_score: -12.6710\n",
      "221/221 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_36 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_216 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_328 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_217 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_218 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_329 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_219 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_220 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_330 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_221 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_331 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 7\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.1543 - r2_score: -18.0628\n",
      "226/226 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_37 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_222 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_332 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_223 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_224 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_333 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_225 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_226 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_334 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_227 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_335 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 6\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.1203 - r2_score: -5.4785\n",
      "232/232 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_38 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_228 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_336 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_229 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_230 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_337 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_231 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_232 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_338 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_233 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_339 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 5\n",
      "30/30 [==============================] - 0s 13ms/step - loss: 0.1247 - r2_score: -8.6237\n",
      "237/237 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_39 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_234 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_340 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_235 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_236 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_341 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_237 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_238 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_342 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_239 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_343 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 4\n",
      "31/31 [==============================] - 0s 13ms/step - loss: 0.1249 - r2_score: -11.6558\n",
      "243/243 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_40 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_240 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_344 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_241 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_242 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_243 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_244 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_245 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_347 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 3\n",
      "31/31 [==============================] - 0s 14ms/step - loss: 0.1115 - r2_score: -4.6517\n",
      "248/248 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_41 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_246 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_348 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_247 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_248 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_349 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_249 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_250 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_350 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_251 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_351 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 2\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1152 - r2_score: -9.0670\n",
      "252/252 [==============================] - 1s 3ms/step\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_42 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_252 (L  (None, 128)               256       \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_352 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_253 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_254 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_353 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_255 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " layer_normalization_256 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_354 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_257 (L  (None, 32)                64        \n",
      " ayerNormalization)                                              \n",
      "                                                                 \n",
      " dense_355 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 1\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.1158 - r2_score: -24.4147\n",
      "255/255 [==============================] - 1s 4ms/step\n",
      "256/256 [==============================] - 1s 4ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "256/256 [==============================] - 1s 3ms/step\n",
      "Initializing kernel-based upper bound...\n",
      "Computing kernel-based upper bound...\n",
      "Building network with parameters: I=4, q=160, d=128, activation=relu\n",
      "Building network with parameters: I=4, q=160, d=128, activation=relu\n",
      "Model: \"model_22\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " sig (InputLayer)            [(None, 15, 128)]            0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 15, 128)              512       ['sig[0][0]']                 \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_361 (Dense)           (None, 15, 160)              20640     ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_264 (L  (None, 15, 160)              320       ['dense_361[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_362 (Dense)           (None, 15, 160)              25760     ['layer_normalization_264[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_265 (L  (None, 15, 160)              320       ['dense_362[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)        (None, 15, 160)              0         ['layer_normalization_265[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_266 (L  (None, 15, 160)              320       ['dropout_15[0][0]']          \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_363 (Dense)           (None, 15, 160)              25760     ['layer_normalization_266[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_267 (L  (None, 15, 160)              320       ['dense_363[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)        (None, 15, 160)              0         ['layer_normalization_267[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_268 (L  (None, 15, 160)              320       ['dropout_16[0][0]']          \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dense_364 (Dense)           (None, 15, 160)              25760     ['layer_normalization_268[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " layer_normalization_269 (L  (None, 15, 160)              320       ['dense_364[0][0]']           \n",
      " ayerNormalization)                                                                               \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)        (None, 15, 160)              0         ['layer_normalization_269[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dense_365 (Dense)           (None, 15, 1)                161       ['dropout_17[0][0]']          \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)        (None, 15)                   0         ['dense_365[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  (None, 14)                   0         ['flatten_11[0][0]']          \n",
      " 1 (SlicingOpLambda)                                                                              \n",
      "                                                                                                  \n",
      " dW (InputLayer)             [(None, 14)]                 0         []                            \n",
      "                                                                                                  \n",
      " deep_martingales_11 (DeepM  (None, 14)                   0         ['tf.__operators__.getitem_21[\n",
      " artingales)                                                        0][0]',                       \n",
      "                                                                     'dW[0][0]']                  \n",
      "                                                                                                  \n",
      " Y (InputLayer)              [(None, 14)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  (None, 14)                   0         ['deep_martingales_11[0][0]'] \n",
      " 2 (SlicingOpLambda)                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_2  (None, 14)                   0         ['Y[0][0]']                   \n",
      " 3 (SlicingOpLambda)                                                                              \n",
      "                                                                                                  \n",
      " dual_stopping_loss_11 (Dua  ()                           0         ['tf.__operators__.getitem_22[\n",
      " lStoppingLoss)                                                     0][0]',                       \n",
      "                                                                     'tf.__operators__.getitem_23[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 100513 (392.63 KB)\n",
      "Trainable params: 100257 (391.63 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.8993 - val_loss: 0.7334\n",
      "Epoch 2/15\n",
      "29/29 [==============================] - 1s 52ms/step - loss: 0.8221 - val_loss: 0.7022\n",
      "Epoch 3/15\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.7664 - val_loss: 0.6676\n",
      "Epoch 4/15\n",
      "29/29 [==============================] - 2s 51ms/step - loss: 0.7182 - val_loss: 0.6420\n",
      "Epoch 5/15\n",
      "29/29 [==============================] - 1s 49ms/step - loss: 0.6698 - val_loss: 0.6014\n",
      "Epoch 6/15\n",
      "29/29 [==============================] - 1s 50ms/step - loss: 0.6298 - val_loss: 0.5755\n",
      "Epoch 7/15\n",
      "29/29 [==============================] - 2s 57ms/step - loss: 0.5910 - val_loss: 0.5505\n",
      "Epoch 8/15\n",
      "29/29 [==============================] - 2s 73ms/step - loss: 0.5582 - val_loss: 0.5210\n",
      "Epoch 9/15\n",
      "29/29 [==============================] - 2s 53ms/step - loss: 0.5265 - val_loss: 0.4943\n",
      "Epoch 10/15\n",
      "29/29 [==============================] - 2s 58ms/step - loss: 0.4954 - val_loss: 0.4674\n",
      "Epoch 11/15\n",
      "29/29 [==============================] - 2s 63ms/step - loss: 0.4664 - val_loss: 0.4420\n",
      "Epoch 12/15\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.4390 - val_loss: 0.4176\n",
      "Epoch 13/15\n",
      "28/29 [===========================>..] - ETA: 0s - loss: 0.4144"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputing kernel-based upper bound...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m     y0_kernel, upper_bound_kernel, upper_bound_kernel_std, kernel_model, kernel_rule_model \u001b[38;5;241m=\u001b[39m \u001b[43mkernel_dual_pricer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_training_dual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPayoff_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdW_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_testing_dual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mPayoff_testing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdW_testing\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mM_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;66;03m# Report results\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeep Kernel Longstaff-Schwartz lower bound: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlower_bound_kernel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlower_bound_kernel_std\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(M2)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/Non linear signature optimal stopping/Deep_kernel_signature_optimal_stopping.py:374\u001b[0m, in \u001b[0;36mDeepKernelDualPricer.price\u001b[0;34m(self, kernel_training, Payoff_training, dW_training, kernel_testing, Payoff_testing, dW_testing, M_val, batch, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    372\u001b[0m kernel_testing \u001b[38;5;241m=\u001b[39m kernel_testing\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode_dim \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1-dim\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mkernel_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPayoff_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdW_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkernel_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPayoff_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdW_training\u001b[49m\u001b[43m[\u001b[49m\u001b[43mM_val\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Testing for fresh samples\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     res \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict([kernel_testing, Payoff_testing[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], dW_testing])\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1377\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1360\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1356\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1357\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m     )\n\u001b[1;32m   1359\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1362\u001b[0m     outputs,\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1364\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1365\u001b[0m )\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1679\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1675\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1676\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1678\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1679\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3269\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3267\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3268\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3269\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4067\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4065\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4066\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 4067\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:596\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 596\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1349\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1349\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1130\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1130\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_variables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:598\u001b[0m, in \u001b[0;36mOptimizerV2.minimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, loss, var_list, grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    568\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m \n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 598\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtape\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_gradients(grads_and_vars, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:656\u001b[0m, in \u001b[0;36mOptimizerV2._compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    654\u001b[0m var_list \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(var_list)\n\u001b[1;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 656\u001b[0m     grads_and_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gradients\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_valid_dtypes(\n\u001b[1;32m    661\u001b[0m     [\n\u001b[1;32m    662\u001b[0m         v\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     ]\n\u001b[1;32m    666\u001b[0m )\n\u001b[1;32m    668\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads_and_vars\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/keras/src/optimizers/legacy/optimizer_v2.py:532\u001b[0m, in \u001b[0;36mOptimizerV2._get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, tape, loss, var_list, grad_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    531\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1065\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1059\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1060\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1061\u001b[0m           output_gradients))\n\u001b[1;32m   1062\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1063\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1065\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m \u001b[43mimperative_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1074\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:147\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    145\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/Downloads/Optimal_Stopping_with_signatures-main/.venv/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py:251\u001b[0m, in \u001b[0;36m_MeanGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    249\u001b[0m sum_grad \u001b[38;5;241m=\u001b[39m _SumGrad(op, grad)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    250\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39minputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_shape_tuple()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (input_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m output_shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m input_shape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_shape):\n\u001b[1;32m    254\u001b[0m   input_size \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(input_shape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Cell 2: Calculate and use RFF features for pricing with corrected implementation\n",
    "\n",
    "# Calculate both sets of features\n",
    "rff_dim = 64\n",
    "print(\"Computing kernel features for lower bound...\")\n",
    "kernel_training = compute_rff_kernel_features(log_signatures_training, N1, rff_dim=rff_dim)\n",
    "kernel_testing = compute_rff_kernel_features(log_signatures_testing, N1, rff_dim=rff_dim)\n",
    "\n",
    "# IMPORTANT: For the dual approach, we need to generate features for N1 steps\n",
    "print(\"Computing kernel features for upper bound...\")\n",
    "kernel_training_dual = compute_rff_kernel_features_dual(log_signatures_training, N1, N1, rff_dim=rff_dim)\n",
    "kernel_testing_dual = compute_rff_kernel_features_dual(log_signatures_testing, N1, N1, rff_dim=rff_dim)\n",
    "print(\"Initializing kernel-based lower bound...\")\n",
    "# 1. LOWER BOUND calculation - this works correctly\n",
    "kernel_pricer = DeepKernelLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    L=rff_dim*2,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=False,\n",
    "    layer_normalization=True\n",
    ")\n",
    "\n",
    "print(\"Computing kernel-based lower bound...\")\n",
    "lower_bound_kernel, lower_bound_kernel_std, kernel_models = kernel_pricer.price(\n",
    "    kernel_training,\n",
    "    kernel_testing,\n",
    "    Payoff_training,\n",
    "    Payoff_testing,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "# 2. UPPER BOUND calculation - use direct payoff, no need to expand\n",
    "print(\"Initializing kernel-based upper bound...\")\n",
    "kernel_dual_pricer = DeepKernelDualPricer(\n",
    "    N1=N1,\n",
    "    N=N1,  # Using N1 instead of N=252 here is the key change\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=4,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=True,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=True,\n",
    "    mode_dim=\"1-dim\"\n",
    ")\n",
    "print(\"Computing kernel-based upper bound...\")\n",
    "try:\n",
    "    y0_kernel, upper_bound_kernel, upper_bound_kernel_std, kernel_model, kernel_rule_model = kernel_dual_pricer.price(\n",
    "        kernel_training_dual,\n",
    "        Payoff_training,\n",
    "        dW_training[:,:,0],\n",
    "        kernel_testing_dual,\n",
    "        Payoff_testing,      \n",
    "        dW_testing[:,:,0],\n",
    "        M_val=int(0.9*M),\n",
    "        batch=2**8,\n",
    "        epochs=15,\n",
    "        learning_rate=0.0005\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} ± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Deep Kernel Dual upper bound: {upper_bound_kernel} ± {upper_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Kernel-based pricing interval: [{lower_bound_kernel}, {upper_bound_kernel}] ± {np.maximum(upper_bound_kernel_std, lower_bound_kernel_std)/np.sqrt(M2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in dual pricer: {e}\")\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} ± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(\"Upper bound calculation failed - using only lower bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2K5Yrl-ejcN",
   "metadata": {
    "id": "a2K5Yrl-ejcN"
   },
   "source": [
    "We once again stress that the parameters for the the discretization (here $J=120$), the sample size (here $M=10^{15}$), and the signature trunaction level (here $K=3$) are not choosen big enough to get narrow gaps, but we can still already observe an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b1b07",
   "metadata": {},
   "source": [
    "## Step 5: Contextualizing Theoretical Price in USD\n",
    "Convert the normalized model price bounds into USD per share and per contract, and print actionable trading recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Method</th>\n",
       "      <th>Lower Bound (USD)</th>\n",
       "      <th>Upper Bound (USD)</th>\n",
       "      <th>Std Error (USD)</th>\n",
       "      <th>Price Gap (USD)</th>\n",
       "      <th>Gap (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Linear Signature</td>\n",
       "      <td>$5.37</td>\n",
       "      <td>$6.74</td>\n",
       "      <td>$0.08</td>\n",
       "      <td>$1.37</td>\n",
       "      <td>25.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Extended Linear Signature</td>\n",
       "      <td>$5.35</td>\n",
       "      <td>$6.70</td>\n",
       "      <td>$0.07</td>\n",
       "      <td>$1.36</td>\n",
       "      <td>25.39%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Deep Log-Signature</td>\n",
       "      <td>$4.99</td>\n",
       "      <td>$7.29</td>\n",
       "      <td>$0.03</td>\n",
       "      <td>$2.30</td>\n",
       "      <td>46.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Deep Kernel Method</td>\n",
       "      <td>$5.10</td>\n",
       "      <td>$7.22</td>\n",
       "      <td>$0.05</td>\n",
       "      <td>$2.11</td>\n",
       "      <td>41.40%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the normalized price bounds to actual USD values\n",
    "actual_stock_price = X0 * 100  # USD per share\n",
    "actual_strike = strike * actual_stock_price  # USD per share\n",
    "\n",
    "# Include all four methods in the list\n",
    "methods = [\n",
    "    \"Linear Signature\", \n",
    "    \"Extended Linear Signature\", \n",
    "    \"Deep Log-Signature\",\n",
    "    \"Deep Kernel Method\"  # Added the kernel method\n",
    "]\n",
    "\n",
    "# Collect all price bounds\n",
    "lower_bounds = [lower_bound, lower_bound_extended, lower_bound_deep, lower_bound_kernel]\n",
    "upper_bounds = [upper_bound, upper_bound_extended, upper_bound_deep, upper_bound_kernel]\n",
    "stds = [lower_bound_std, lower_bound_extended_std, lower_bound_deep_std, lower_bound_kernel_std]\n",
    "\n",
    "# Create a table of results\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "results = []\n",
    "for i, method in enumerate(methods):\n",
    "    usd_lower = float(lower_bounds[i]) * actual_stock_price\n",
    "    \n",
    "    # Handle the case where upper bound might not be available for kernel method\n",
    "    if i == 3 and 'upper_bound_kernel' not in locals():\n",
    "        usd_upper = float('nan')  # Use NaN if upper bound isn't available\n",
    "    else:\n",
    "        usd_upper = float(upper_bounds[i]) * actual_stock_price\n",
    "    \n",
    "    usd_std = float(stds[i]) * actual_stock_price / np.sqrt(M2)\n",
    "    \n",
    "    # Calculate gap only if upper bound exists\n",
    "    if not np.isnan(usd_upper):\n",
    "        gap = usd_upper - usd_lower\n",
    "        gap_percent = gap / usd_lower * 100\n",
    "    else:\n",
    "        gap = float('nan')\n",
    "        gap_percent = float('nan')\n",
    "    \n",
    "    results.append({\n",
    "        \"Method\": method,\n",
    "        \"Lower Bound (USD)\": f\"${usd_lower:.2f}\",\n",
    "        \"Upper Bound (USD)\": f\"${usd_upper:.2f}\" if not np.isnan(usd_upper) else \"N/A\",\n",
    "        \"Std Error (USD)\": f\"${usd_std:.2f}\",\n",
    "        \"Price Gap (USD)\": f\"${gap:.2f}\" if not np.isnan(gap) else \"N/A\",\n",
    "        \"Gap (%)\": f\"{gap_percent:.2f}%\" if not np.isnan(gap_percent) else \"N/A\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(HTML(results_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b75c6",
   "metadata": {},
   "source": [
    "## Option Trading Interpretation\n",
    "\n",
    "Now let's interpret these results from a trading perspective. We'll evaluate the fair price range for an American put option contract (which typically represents 100 shares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Put Option Contract Analysis (for 100 shares)\n",
      "=====================================================================\n",
      "Stock Price: $100.00\n",
      "Strike Price: $105.00\n",
      "Time to Maturity: 14 days\n",
      "Interest Rate: 5.00%\n",
      "Rough Volatility Parameters: H=0.07, η=1.9, ρ=-0.9, ξ₀=0.09\n",
      "=====================================================================\n",
      "Fair Price Range: $4.99 to $7.29 per contract\n",
      "Midpoint Price: $6.14\n",
      "=====================================================================\n",
      "Trading Recommendations:\n",
      "\n",
      "If market price is $3.99 (Below Fair Value):\n",
      "→ BUY: Market price is below fair value range\n",
      "→ Expected edge: $1.00 to $3.30 per contract\n",
      "→ Consider buying puts for protection or speculative profit\n",
      "\n",
      "If market price is $6.14 (At Fair Value):\n",
      "→ NEUTRAL: Market price is within fair value range\n",
      "→ Price is positioned 50% through the fair value range\n",
      "→ No clear edge for buying or selling\n",
      "\n",
      "If market price is $8.74 (Above Fair Value):\n",
      "→ SELL: Market price is above fair value range\n",
      "→ Expected edge: $1.46 to $3.76 per contract\n",
      "→ Consider writing puts, potentially as part of a spread strategy to limit risk\n"
     ]
    }
   ],
   "source": [
    "# Now we can calculate the fair price range for a standard options contract\n",
    "# Cell under “# For a standard options contract (100 shares)”\n",
    "shares_per_contract = 100\n",
    "contract_lower   = float(lower_bound_deep) * actual_stock_price\n",
    "contract_upper   = float(upper_bound_deep) * actual_stock_price\n",
    "contract_midpoint = (contract_lower + contract_upper) / 2\n",
    "\n",
    "print(f\"American Put Option Contract Analysis (for {shares_per_contract} shares)\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Stock Price: ${actual_stock_price:.2f}\")\n",
    "print(f\"Strike Price: ${actual_strike:.2f}\")\n",
    "print(f\"Time to Maturity: {T} days\")\n",
    "print(f\"Interest Rate: {r*100:.2f}%\")\n",
    "print(f\"Rough Volatility Parameters: H={H}, η={eta}, ρ={rho}, ξ₀={xi}\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Fair Price Range: ${contract_lower:.2f} to ${contract_upper:.2f} per contract\")\n",
    "print(f\"Midpoint Price: ${contract_midpoint:.2f}\")\n",
    "print(f\"=====================================================================\")\n",
    "\n",
    "# Trading recommendations based on market prices\n",
    "hypothetical_market_prices = [contract_lower * 0.8, contract_midpoint, contract_upper * 1.2]\n",
    "labels = [\"Below Fair Value\", \"At Fair Value\", \"Above Fair Value\"]\n",
    "\n",
    "print(\"Trading Recommendations:\")\n",
    "for price, label in zip(hypothetical_market_prices, labels):\n",
    "    print(f\"\\nIf market price is ${price:.2f} ({label}):\")\n",
    "    \n",
    "    if price < contract_lower:\n",
    "        print(\"→ BUY: Market price is below fair value range\")\n",
    "        print(f\"→ Expected edge: ${(contract_lower - price):.2f} to ${(contract_upper - price):.2f} per contract\")\n",
    "        print(\"→ Consider buying puts for protection or speculative profit\")\n",
    "    elif price > contract_upper:\n",
    "        print(\"→ SELL: Market price is above fair value range\")\n",
    "        print(f\"→ Expected edge: ${price - contract_upper:.2f} to ${price - contract_lower:.2f} per contract\")\n",
    "        print(\"→ Consider writing puts, potentially as part of a spread strategy to limit risk\")\n",
    "    else:\n",
    "        print(\"→ NEUTRAL: Market price is within fair value range\")\n",
    "        position = (price - contract_lower) / (contract_upper - contract_lower)\n",
    "        print(f\"→ Price is positioned {position:.0%} through the fair value range\")\n",
    "        if position < 0.4:\n",
    "            print(\"→ Slight bias toward buying\")\n",
    "        elif position > 0.6:\n",
    "            print(\"→ Slight bias toward selling\")\n",
    "        else:\n",
    "            print(\"→ No clear edge for buying or selling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30e089",
   "metadata": {},
   "source": [
    "## Risk Management Considerations\n",
    "\n",
    "When trading American put options in a rough volatility environment, several risk management considerations are important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Management Considerations:\n",
      "=====================================================================\n",
      "Moneyness: 0.95 (5% in-the-money)\n",
      "Time Value: $-0.01 per share\n",
      "Uncertainty Range: $2.30 per contract\n",
      "\n",
      "Recommended Risk Management Strategies:\n",
      "---------------------------------------------------------------------\n",
      "1. Position Sizing: Limit exposure to <5% of portfolio per trade\n",
      "2. Early Exercise Consideration: Monitor optimal stopping boundaries\n",
      "3. Hedging: Consider delta and vega hedging for larger positions\n",
      "4. Model Risk: Be aware model assumes H=0.07, may differ from market\n",
      "\n",
      "Practical Implementation:\n",
      "---------------------------------------------------------------------\n",
      "→ ATM option: Maximum gamma/vega exposure\n",
      "→ Most sensitive to changes in volatility and rough volatility parameters\n",
      "→ Actively monitor for optimal early exercise conditions near expiration\n",
      "\n",
      "Note: This model incorporates rough volatility effects (H=0.07) which\n",
      "traditional models like Black-Scholes miss. This can be particularly\n",
      "important for managing risk in volatile market conditions.\n"
     ]
    }
   ],
   "source": [
    "# Calculate additional risk metrics\n",
    "# In the “# Calculate additional risk metrics” cell\n",
    "percent_itm = max(0, (actual_strike - actual_stock_price) / actual_strike * 100)\n",
    "\n",
    "moneyness = actual_stock_price / actual_strike\n",
    "time_value = float(lower_bound_deep) * actual_stock_price - max(0, actual_strike - actual_stock_price)\n",
    "model_implied_volatility = 0.3  # This would typically be backed out from the model price\n",
    "\n",
    "print(\"Risk Management Considerations:\")\n",
    "print(\"=====================================================================\")\n",
    "print(f\"Moneyness: {moneyness:.2f} ({percent_itm:.0f}% in-the-money)\")\n",
    "print(f\"Time Value: ${time_value:.2f} per share\")\n",
    "print(f\"Uncertainty Range: ${(contract_upper - contract_lower):.2f} per contract\")\n",
    "print(\"\\nRecommended Risk Management Strategies:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"1. Position Sizing: Limit exposure to <5% of portfolio per trade\")\n",
    "print(\"2. Early Exercise Consideration: Monitor optimal stopping boundaries\")\n",
    "print(\"3. Hedging: Consider delta and vega hedging for larger positions\")\n",
    "print(\"4. Model Risk: Be aware model assumes H={:.2f}, may differ from market\".format(H))\n",
    "\n",
    "# Additional practical advice\n",
    "print(\"\\nPractical Implementation:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "if moneyness < 0.95:\n",
    "    print(\"→ Deep ITM option: Consider early exercise if dividend yield > interest rate\")\n",
    "    print(\"→ Watch for significant changes in volatility that could shift optimal exercise boundary\")\n",
    "elif moneyness > 1.05:\n",
    "    print(\"→ OTM option: Early exercise unlikely, trade like European option\")\n",
    "    print(\"→ Primary value is in insurance against downside moves\")\n",
    "else:\n",
    "    print(\"→ ATM option: Maximum gamma/vega exposure\")\n",
    "    print(\"→ Most sensitive to changes in volatility and rough volatility parameters\")\n",
    "    print(\"→ Actively monitor for optimal early exercise conditions near expiration\")\n",
    "\n",
    "print(\"\\nNote: This model incorporates rough volatility effects (H={:.2f}) which\".format(H))\n",
    "print(\"traditional models like Black-Scholes miss. This can be particularly\")\n",
    "print(\"important for managing risk in volatile market conditions.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
