{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccc9cab",
   "metadata": {
    "id": "eccc9cab"
   },
   "source": [
    "# Pricing American options in rough Bergomi with linear and deep signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ckOdm_OAGE",
   "metadata": {
    "id": "c9ckOdm_OAGE"
   },
   "source": [
    "In this notebook we show how to use the code from https://github.com/lucapelizzari/Optimal_Stopping_with_signatures/tree/main, to compute lower and upper bounds for American options in the rough Bergomi model using different signature methods, see for example Section 4.2 of https://arxiv.org/abs/2312.03444 for the linear approach, whereas the deep neural network approaches will be discussed in a forthcoming paper.\n",
    "\n",
    "The repository consists of:\n",
    "\n",
    "*   Simulation packages for fractional Brownian motion, rough Bergomi and rough Heston models\n",
    "*   A modul for signature related computations **Signature_computer.py**, which can compute the signature and log-signature  of various lifts related to volatility modelling, with the additional option of adding polynomials of the state-process and/or volatility.\n",
    "*   The main module for the linear signature approaches **Linear_signature_optimal_stopping.py**, which can be used to derive lower and upper bounds to the optimal stopping problem applying the approaches described in https://arxiv.org/abs/2312.03444\n",
    "*   The main module for deep log-signature approaches **Deep_signature_optimal_stopping.py**, which extends the linear approaches by applying deep neural networks on the log-signature. This code is accompanying a working paper paper on \"American option pricing using signatures\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5c7aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for TensorFlow import issues and environment compatibility\n",
    "# Set correct Python path to find modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory and subdirectories to Python path\n",
    "repo_root = os.path.abspath('..')\n",
    "sys.path.append(repo_root)\n",
    "sys.path.append(os.path.join(repo_root, \"Linear signature optimal stopping\"))\n",
    "sys.path.append(os.path.join(repo_root, \"Non linear signature optimal stopping\"))\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e568a2",
   "metadata": {
    "id": "57e568a2"
   },
   "source": [
    "## American Put options in the rough Bergomi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572b688",
   "metadata": {
    "id": "6572b688"
   },
   "source": [
    "\n",
    "\n",
    "Recall the price and volatility dynamics of the latter are given by \\begin{align*}\n",
    "dX_t &= rX_tdt+X_tv_t \\left (\\rho dW_r+\\sqrt{1-\\rho^2}dB_t\\right ), \\\\ v_t & =\\xi_0\\mathcal{E}\\left (\\eta \\int_0^t(t-s)^{H-\\frac{1}{2}}dW_s \\right )\n",
    "\\end{align*} and pricing an American Put-option can be formulated as optimal stopping problem $$y_0=\\sup_{\\tau \\in \\mathcal{S}_0}\\mathbb{E}[e^{-r\\tau}\\left (K-X_{\\tau}\\right )^{+}]$$ for some strike $K$. In this notebook we consider the following choice of paramteres $$ H=0.07,X_0 = 100, r=0.05, \\eta = 1.9, \\rho = -0.9, \\xi_0= 0.09, K = 110.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xlp80cKeRW9z",
   "metadata": {
    "id": "Xlp80cKeRW9z"
   },
   "source": [
    "## Step 1: Simulation rough Bergomi model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I7PTV1Z3Rig-",
   "metadata": {
    "id": "I7PTV1Z3Rig-"
   },
   "source": [
    "We start by defining the parameters of the model and importing the rough Berogmi simulation package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd54c84b",
   "metadata": {
    "id": "fd54c84b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Define all your parameters\n",
    "N1 = 14  # number of exercise-dates\n",
    "N = 252  # discretization-grid\n",
    "T = 14  # Maturity in days\n",
    "T_years = T / 252  # Maturity in years\n",
    "M = 2**17  # number of samples for training\n",
    "M2 = 2**17  # number of samples for testing\n",
    "H = 0.07  # Hurst parameter\n",
    "eta = 1.9\n",
    "X0 = 1\n",
    "r = 0.05\n",
    "rho = -0.9\n",
    "xi = 0.09\n",
    "strike = 1.05  # This is used in phi, not directly in the simulation\n",
    "K = 2  # This is the depth of Signature parameter\n",
    "# Your payoff function uses strike\n",
    "def phi(x):\n",
    "    return np.maximum(strike-x, 0)  # payoff function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61KWkP-iYnFJ",
   "metadata": {
    "id": "61KWkP-iYnFJ"
   },
   "source": [
    "Note that the number of samples should be much bigger to get reliable results, but to keep the complexity low in this presentation we restrict to $2**15$ training and testing paths here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7057f779",
   "metadata": {
    "id": "7057f779"
   },
   "outputs": [],
   "source": [
    "# Adjust path to include repository root\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rBergomi_simulation import SimulationofrBergomi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29669ebe",
   "metadata": {
    "id": "29669ebe"
   },
   "source": [
    "Next we define a function generating rough Bergomi prices, volatilies and the corresponding Brownian motion and payoff process. Then we generate training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cd21314",
   "metadata": {
    "id": "9cd21314"
   },
   "outputs": [],
   "source": [
    "def generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r):\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationofrBergomi(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c86a8cc",
   "metadata": {
    "id": "7c86a8cc"
   },
   "outputs": [],
   "source": [
    "# Use K in your function call, not strike\n",
    "S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data(M, N, T, phi, rho, K, X0, H, xi, eta, r)\n",
    "S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data(M2, N, T, phi, rho, K, X0, H, xi, eta, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb1fef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show Head of the training data\n",
      "S_training [[1.         1.01245418 1.00885189 0.99691838 1.00299428 0.98406061\n",
      "  0.97479064 0.98577288 0.98765326 1.00055605 1.02758306 1.04596186\n",
      "  1.04900607 1.0504907  1.05642847]\n",
      " [1.         1.03298353 1.03184662 1.03486873 1.05566327 1.05563455\n",
      "  1.06180082 1.06162445 1.06425758 1.06050868 1.06485823 1.05486137\n",
      "  1.05012205 1.06384281 1.06649534]\n",
      " [1.         0.97503986 0.97571065 0.97717886 0.95219762 0.92817063\n",
      "  0.92279866 0.88678035 0.9127524  0.91713112 0.91000248 0.8261809\n",
      "  0.84842891 0.8401957  0.76851771]\n",
      " [1.         1.0203757  1.01007883 1.03530838 1.04482952 1.04213932\n",
      "  1.0694246  1.06833684 1.06729816 1.07001515 1.0833717  1.07502874\n",
      "  1.0610243  1.05842682 1.05125967]\n",
      " [1.         1.0105417  1.01220105 1.02361013 1.03772722 1.03908937\n",
      "  1.04126733 1.04295557 1.04764943 1.03730417 1.04039195 1.04241369\n",
      "  1.0454334  1.04244252 1.0581186 ]]\n",
      "V_training [[9.00000000e-02 3.54043111e-03 2.62680315e-01 7.06279264e-02\n",
      "  3.76575938e-02 3.80393083e-01 5.98800593e-02 2.63774847e-02\n",
      "  1.31025115e-01 8.03630818e-02 8.73159265e-03 1.98839351e-03\n",
      "  3.54667845e-03 2.00412678e-03 2.52239096e-03]\n",
      " [9.00000000e-02 3.73591125e-03 3.65961036e-02 1.79680102e-02\n",
      "  3.22318816e-03 1.49913178e-03 1.23572437e-03 7.53268024e-03\n",
      "  1.22000141e-03 4.26872742e-03 1.17886050e-02 3.05132655e-03\n",
      "  6.53735226e-02 4.79543786e-03 8.09209866e-03]\n",
      " [9.00000000e-02 7.98446888e-02 2.97408219e-02 1.58369073e-01\n",
      "  1.19202761e-01 9.60445117e-02 1.10006415e-01 1.13695436e-01\n",
      "  8.20854397e-02 1.90949757e-02 2.74394499e-01 1.97856880e+00\n",
      "  9.31656683e-02 3.52533207e-01 1.54594187e+00]\n",
      " [9.00000000e-02 3.78777393e-02 3.29805441e-02 2.89937649e-03\n",
      "  2.67462901e-03 3.37792011e-02 3.69730012e-03 5.18917010e-03\n",
      "  9.17870156e-03 2.26047183e-02 8.43001632e-03 1.62747671e-02\n",
      "  4.51985143e-02 3.15798338e-02 1.61283535e-02]\n",
      " [9.00000000e-02 3.48551667e-03 2.44391791e-02 1.46589589e-02\n",
      "  1.95633486e-03 5.41969481e-03 6.20351882e-03 1.09285759e-02\n",
      "  1.52150013e-02 1.44296333e-02 1.37557893e-02 1.09532338e-02\n",
      "  3.79983803e-03 2.38159477e-02 6.99549927e-03]]\n",
      "Payoff_training [[0.05       0.03754582 0.04114811 0.05308162 0.04700572 0.06593939\n",
      "  0.07520936 0.06422712 0.06234674 0.04944395 0.02241694 0.00403814\n",
      "  0.00099393 0.         0.        ]\n",
      " [0.05       0.01701647 0.01815338 0.01513127 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.05       0.07496014 0.07428935 0.07282114 0.09780238 0.12182937\n",
      "  0.12720134 0.16321965 0.1372476  0.13286888 0.13999752 0.2238191\n",
      "  0.20157109 0.2098043  0.28148229]\n",
      " [0.05       0.0296243  0.03992117 0.01469162 0.00517048 0.00786068\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.05       0.0394583  0.03779895 0.02638987 0.01227278 0.01091063\n",
      "  0.00873267 0.00704443 0.00235057 0.01269583 0.00960805 0.00758631\n",
      "  0.0045666  0.00755748 0.        ]]\n",
      "dW_training [[[-1.09349429e-01 -3.38051543e+00]\n",
      "  [ 5.94995410e-02  3.79872250e+00]\n",
      "  [ 5.22501242e-03  1.27745470e+00]\n",
      "  [-4.15719759e-02  4.03014326e-01]\n",
      "  [ 1.43218937e-01  4.08003545e+00]\n",
      "  [ 7.43718214e-03  1.20034333e-01]\n",
      "  [-3.43931805e-02 -8.51096678e-01]\n",
      "  [ 9.06639299e-03  1.86958316e+00]\n",
      "  [-1.90217056e-02  1.13490248e+00]\n",
      "  [-5.71481853e-02 -1.75589510e+00]\n",
      "  [-1.37669191e-01 -3.30317392e+00]\n",
      "  [-7.25697083e-02 -1.31436525e+00]\n",
      "  [-2.22864156e-02 -1.74442416e+00]\n",
      "  [-8.76977724e-02 -1.50524035e+00]]\n",
      "\n",
      " [[-1.53332673e-01 -3.30491811e+00]\n",
      "  [ 4.30789135e-02  1.42926716e+00]\n",
      "  [-5.06527723e-02 -1.76476635e-01]\n",
      "  [-1.22521562e-01 -2.14561621e+00]\n",
      "  [ 2.48538745e-02 -2.20579343e+00]\n",
      "  [-1.56909889e-01 -2.99018016e+00]\n",
      "  [ 1.77530098e-02  8.79324282e-01]\n",
      "  [-7.06171543e-02 -2.21481100e+00]\n",
      "  [ 4.84110511e-02  1.31455612e-02]\n",
      "  [-5.05000280e-02  7.38472116e-01]\n",
      "  [ 7.51190615e-02 -7.76767096e-01]\n",
      "  [ 9.89404374e-02  2.67782247e+00]\n",
      "  [-6.28627039e-02 -1.87708246e+00]\n",
      "  [ 5.97303697e-03 -3.92418887e-01]]\n",
      "\n",
      " [[ 4.56887826e-02  1.00233664e+00]\n",
      "  [-2.96998893e-02 -6.86152323e-01]\n",
      "  [-1.47211759e-02  2.09895541e+00]\n",
      "  [ 5.03050242e-02  1.88050129e+00]\n",
      "  [ 9.46980532e-02  1.13350039e+00]\n",
      "  [-1.45110459e-02  5.75825445e-01]\n",
      "  [ 1.36175564e-01  1.00755244e+00]\n",
      "  [-3.03606764e-02 -5.73876247e-01]\n",
      "  [-6.38610757e-03 -1.99946062e+00]\n",
      "  [ 6.87712006e-02  1.96171221e+00]\n",
      "  [ 1.74353882e-01  4.21883742e+00]\n",
      "  [-6.36586414e-02 -1.45819761e+00]\n",
      "  [ 3.19488672e-02  1.46732934e+00]\n",
      "  [ 1.30027976e-01  3.42006894e+00]]\n",
      "\n",
      " [[-1.67033800e-02 -4.66212124e-02]\n",
      "  [ 6.54505062e-02  3.09956138e-02]\n",
      "  [-1.74287895e-01 -3.94464420e+00]\n",
      "  [-1.31314008e-01 -2.29967632e+00]\n",
      "  [ 8.95436107e-02  2.24550190e+00]\n",
      "  [-1.23020378e-01 -2.03251935e+00]\n",
      "  [ 3.03315751e-03 -4.48232625e-01]\n",
      "  [-7.38646168e-03  6.66940224e-02]\n",
      "  [ 1.80152500e-02  1.25995039e+00]\n",
      "  [-6.48730257e-02 -4.03019027e-01]\n",
      "  [ 8.95799447e-02  1.07203622e+00]\n",
      "  [ 1.34966739e-01  1.52576248e+00]\n",
      "  [ 5.71356722e-02 -1.47989619e-01]\n",
      "  [-1.34425944e-02 -1.34748147e+00]]\n",
      "\n",
      " [[-5.41985264e-02 -3.40250429e+00]\n",
      "  [-5.57881129e-02 -4.70447407e-02]\n",
      "  [-6.10723077e-02 -2.80490194e-01]\n",
      "  [-9.85727145e-02 -2.65566251e+00]\n",
      "  [-4.07081877e-02 -4.78644204e-01]\n",
      "  [-3.85432021e-02 -1.83978229e-01]\n",
      "  [-1.37913673e-02  7.39035646e-01]\n",
      "  [-4.79414396e-02  1.11896301e+00]\n",
      "  [ 6.34828689e-02  1.32188334e+00]\n",
      "  [-2.36611883e-02  4.71992768e-01]\n",
      "  [-1.74804959e-02  3.55300748e-01]\n",
      "  [-6.54662224e-02 -1.05130374e+00]\n",
      "  [ 5.56854371e-02  2.03987644e+00]\n",
      "  [-8.38600914e-02 -3.79824422e-01]]]\n",
      "I_training [[ 0.         -0.03280483 -0.02926452 -0.02658657 -0.03763471 -0.00984226\n",
      "  -0.00525531 -0.01367146 -0.01219897 -0.01908433 -0.03528492 -0.04814914\n",
      "  -0.05138512 -0.05271237 -0.05663838]\n",
      " [ 0.         -0.0459998  -0.04336673 -0.05305666 -0.06948004 -0.06806901\n",
      "  -0.07414434 -0.07352027 -0.07964921 -0.07795828 -0.08125772 -0.07310165\n",
      "  -0.06763629 -0.08370918 -0.08329556]\n",
      " [ 0.          0.01370663  0.0053144   0.00277565  0.02279484  0.05549006\n",
      "   0.05099293  0.09615858  0.08592133  0.08409168  0.0935948   0.18492603\n",
      "   0.09538276  0.10513453  0.182338  ]\n",
      " [ 0.         -0.00501101  0.0077271  -0.02392454 -0.03099525 -0.02636434\n",
      "  -0.0489744  -0.04878997 -0.04932206 -0.0475961  -0.05734967 -0.04912488\n",
      "  -0.03190682 -0.01975981 -0.02214866]\n",
      " [ 0.         -0.01625956 -0.01955319 -0.02910065 -0.04103526 -0.0428358\n",
      "  -0.0456733  -0.04675954 -0.05177133 -0.04394077 -0.04678304 -0.04883324\n",
      "  -0.05568478 -0.05225217 -0.06519381]]\n",
      "MM_training [[[1.00000000e+00 9.00000000e-02]\n",
      "  [1.01245418e+00 3.54043111e-03]\n",
      "  [1.00885189e+00 2.62680315e-01]\n",
      "  [9.96918379e-01 7.06279264e-02]\n",
      "  [1.00299428e+00 3.76575938e-02]\n",
      "  [9.84060610e-01 3.80393083e-01]\n",
      "  [9.74790637e-01 5.98800593e-02]\n",
      "  [9.85772883e-01 2.63774847e-02]\n",
      "  [9.87653264e-01 1.31025115e-01]\n",
      "  [1.00055605e+00 8.03630818e-02]\n",
      "  [1.02758306e+00 8.73159265e-03]\n",
      "  [1.04596186e+00 1.98839351e-03]\n",
      "  [1.04900607e+00 3.54667845e-03]\n",
      "  [1.05049070e+00 2.00412678e-03]\n",
      "  [1.05642847e+00 2.52239096e-03]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.03298353e+00 3.73591125e-03]\n",
      "  [1.03184662e+00 3.65961036e-02]\n",
      "  [1.03486873e+00 1.79680102e-02]\n",
      "  [1.05566327e+00 3.22318816e-03]\n",
      "  [1.05563455e+00 1.49913178e-03]\n",
      "  [1.06180082e+00 1.23572437e-03]\n",
      "  [1.06162445e+00 7.53268024e-03]\n",
      "  [1.06425758e+00 1.22000141e-03]\n",
      "  [1.06050868e+00 4.26872742e-03]\n",
      "  [1.06485823e+00 1.17886050e-02]\n",
      "  [1.05486137e+00 3.05132655e-03]\n",
      "  [1.05012205e+00 6.53735226e-02]\n",
      "  [1.06384281e+00 4.79543786e-03]\n",
      "  [1.06649534e+00 8.09209866e-03]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [9.75039863e-01 7.98446888e-02]\n",
      "  [9.75710649e-01 2.97408219e-02]\n",
      "  [9.77178864e-01 1.58369073e-01]\n",
      "  [9.52197616e-01 1.19202761e-01]\n",
      "  [9.28170625e-01 9.60445117e-02]\n",
      "  [9.22798658e-01 1.10006415e-01]\n",
      "  [8.86780352e-01 1.13695436e-01]\n",
      "  [9.12752395e-01 8.20854397e-02]\n",
      "  [9.17131119e-01 1.90949757e-02]\n",
      "  [9.10002484e-01 2.74394499e-01]\n",
      "  [8.26180897e-01 1.97856880e+00]\n",
      "  [8.48428915e-01 9.31656683e-02]\n",
      "  [8.40195700e-01 3.52533207e-01]\n",
      "  [7.68517713e-01 1.54594187e+00]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.02037570e+00 3.78777393e-02]\n",
      "  [1.01007883e+00 3.29805441e-02]\n",
      "  [1.03530838e+00 2.89937649e-03]\n",
      "  [1.04482952e+00 2.67462901e-03]\n",
      "  [1.04213932e+00 3.37792011e-02]\n",
      "  [1.06942460e+00 3.69730012e-03]\n",
      "  [1.06833684e+00 5.18917010e-03]\n",
      "  [1.06729816e+00 9.17870156e-03]\n",
      "  [1.07001515e+00 2.26047183e-02]\n",
      "  [1.08337170e+00 8.43001632e-03]\n",
      "  [1.07502874e+00 1.62747671e-02]\n",
      "  [1.06102430e+00 4.51985143e-02]\n",
      "  [1.05842682e+00 3.15798338e-02]\n",
      "  [1.05125967e+00 1.61283535e-02]]\n",
      "\n",
      " [[1.00000000e+00 9.00000000e-02]\n",
      "  [1.01054170e+00 3.48551667e-03]\n",
      "  [1.01220105e+00 2.44391791e-02]\n",
      "  [1.02361013e+00 1.46589589e-02]\n",
      "  [1.03772722e+00 1.95633486e-03]\n",
      "  [1.03908937e+00 5.41969481e-03]\n",
      "  [1.04126733e+00 6.20351882e-03]\n",
      "  [1.04295557e+00 1.09285759e-02]\n",
      "  [1.04764943e+00 1.52150013e-02]\n",
      "  [1.03730417e+00 1.44296333e-02]\n",
      "  [1.04039195e+00 1.37557893e-02]\n",
      "  [1.04241369e+00 1.09532338e-02]\n",
      "  [1.04543340e+00 3.79983803e-03]\n",
      "  [1.04244252e+00 2.38159477e-02]\n",
      "  [1.05811860e+00 6.99549927e-03]]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Show Head of the training data\")\n",
    "print(\"S_training\", S_training[:5])\n",
    "print(\"V_training\", V_training[:5])\n",
    "print(\"Payoff_training\", Payoff_training[:5])\n",
    "print(\"dW_training\", dW_training[:5])\n",
    "print(\"I_training\", I_training[:5])\n",
    "print(\"MM_training\", MM_training[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68169f97",
   "metadata": {
    "id": "68169f97"
   },
   "outputs": [],
   "source": [
    "#compute the volatility processes\n",
    "vol_training = np.sqrt(V_training)\n",
    "vol_testing = np.sqrt(V_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LW4SLN7bQy4R",
   "metadata": {
    "id": "LW4SLN7bQy4R"
   },
   "source": [
    "## Step 2: Signature computations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NUEKCKWsRPy4",
   "metadata": {
    "id": "NUEKCKWsRPy4"
   },
   "source": [
    "We will make us uf the iisignature package https://pypi.org/project/iisignature/ to compute the signature, and it can be installed using pip:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14790b8",
   "metadata": {
    "id": "d14790b8"
   },
   "source": [
    "We import our signature computation module, which can compute various signature and log signature lift related to the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1fa867a0",
   "metadata": {
    "id": "1fa867a0"
   },
   "outputs": [],
   "source": [
    "from Signature_computer import SignatureComputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eb781",
   "metadata": {
    "id": "fe2eb781"
   },
   "source": [
    "Next we initialize the **SignatureComputer**, which allows to choose from the linear and the log signature, and various choices of signature lifts. Here are some examples $$ t\\mapsto \\mathrm{Sig}(A_t,X_t),t\\mapsto \\mathrm{Sig}(A_t,\\phi(X)_t),t\\mapsto \\mathrm{Sig}(A_t,X_t,X_{t-\\epsilon}),t\\mapsto \\mathrm{Sig}(A_t,X_t,\\phi(X_t)),t\\mapsto \\mathrm{Sig}(A_t,v_t),$$ where $t\\mapsto A_t$ is a monoton path and in our examples we choose between $$A_t=t, \\quad  A_t = \\langle X\\rangle_t.$$ Additonally we can add Laguerre polynomials of $X$ or $(X,v)$ to the signature, see the module for all the details.\n",
    "\n",
    "In this example we choose the basis $(\\mathrm{Sig}(t,v_t),p_i(X_t))$, which proves to be a solid choice for rough volatility models. We choose both the polynomial and signature degree to be $3$ for this example. (To improve result one should higher truncations levels ($4-5$) for the signature, but to keep the complexity reasonable here we choose level $3$ signatures.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c4a377c",
   "metadata": {
    "id": "8c4a377c"
   },
   "outputs": [],
   "source": [
    "#initialize signature computer\n",
    "sig_computer = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c365145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c365145",
    "outputId": "d26148d9-199d-4ebd-bd3f-60524818666b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing linear signature with polynomial-vol lift\n",
      "Computing linear signature with polynomial-vol lift\n"
     ]
    }
   ],
   "source": [
    "#Compute the signature for training and test data\n",
    "tt = np.linspace(0,T,N+1)\n",
    "A_training = np.zeros((M, N+1)) #time-augmentation\n",
    "A_testing = np.zeros((M2, N+1))\n",
    "A_training[:, 1:] = A_testing[:, 1:] = tt[1:]\n",
    "signatures_training = sig_computer.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_testing = sig_computer.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B0iPgqVDUXyd",
   "metadata": {
    "id": "B0iPgqVDUXyd"
   },
   "source": [
    "Some example of the signature paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "qLgGhAQqUa-K",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "qLgGhAQqUa-K",
    "outputId": "e276704a-7235-49f0-98cd-04e9f91b9cdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x158e73df0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(signatures_training[0,:,0],label = 'first level (constant)')\n",
    "plt.plot(signatures_training[0,:,1],label = '1st level: time ')\n",
    "plt.plot(signatures_training[0,:,2],label = '2nd level: $v_t-v_0$')\n",
    "plt.plot(signatures_training[0,:,4],label = '3th level: $\\int sdv_s $')\n",
    "plt.plot(signatures_training[0,:,12],label = '4th level: $\\int \\int udv_u dv_s $')\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19711264",
   "metadata": {
    "id": "19711264"
   },
   "source": [
    "## Step 3: Compute pricing intervals with linear signatures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c7788",
   "metadata": {
    "id": "180c7788"
   },
   "source": [
    "We can now import the linear primal and dual pricers, which compute true lower and upper bounds.\n",
    "- The **LinearLongstaffSchwartzPricer** uses the signature of the training data to recursively approximate continuation values in the spirit of the Longstaff-Schwartz algorithm (descibed in detail in Section 3.1 of https://arxiv.org/abs/2312.03444). The resulting regression coefficients at each exercise date provide a stopping rule, which can be applied to the testing data to get true lower-bounds\n",
    "- The **LinearDualPricer** uses the signature of the training data to minimize over the familiy of linear signature martingales, by solving a corresponding linear program (described in Detail in Section 3.2 of https://arxiv.org/abs/2312.03444). The resulting coefficients yield a Doob martingale approximation, which for the testing data yields a true upper bound.\n",
    "By combining the two values, we receive confidence intervals for the true option price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eTaJQW5V8q6",
   "metadata": {
    "id": "1eTaJQW5V8q6"
   },
   "source": [
    "To solve the linear programm, one can optionally choose to use Gurobi https://www.gurobi.com, which requires a free licence, which is recommended especially for high-dimensional LPs, which occur when choosing large sample-sizes and/or high signature truncations levels. Alternatively, we use the free LP solvers from CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edbc73ec",
   "metadata": {
    "id": "edbc73ec"
   },
   "outputs": [],
   "source": [
    "# \n",
    "# add root of repo and the Linear signature optimal stopping folder to PYTHONPATH\n",
    "import sys, os\n",
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "\n",
    "# now the module can be imported\n",
    "from Linear_signature_optimal_stopping import LinearLongstaffSchwartzPricer, LinearDualPricer\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a888326",
   "metadata": {
    "id": "5a888326"
   },
   "outputs": [],
   "source": [
    "\n",
    "#initialze the models\n",
    "ls_pricer = LinearLongstaffSchwartzPricer(\n",
    "        N1=N1,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        mode=\"American Option\",\n",
    "        ridge=10**(-9)\n",
    "    )\n",
    "\n",
    "dual_pricer = LinearDualPricer(\n",
    "        N1=N1,\n",
    "        N=N,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        LP_solver=\"CVXPY\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4f4d",
   "metadata": {
    "id": "28db4f4d"
   },
   "source": [
    "The choice mode=\"American Option\" indicates that the Longstaff-Schwartz recursion will only consider \"in-the-money\" paths, which was originally suggested by Longstaff & Schwartz, and is reasonable for non-negative payoffs. For general payoffs we can use mode = \"Standard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0889e9a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0889e9a8",
    "outputId": "8afb4b10-9708-46a7-f241-da85ac65ee99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score at exercise date 13 0.9115377840307282\n",
      "Regression score at exercise date 12 0.8515875063488605\n",
      "Regression score at exercise date 11 0.8062383759404195\n",
      "Regression score at exercise date 10 0.755539971822289\n",
      "Regression score at exercise date 9 0.6970165609590369\n",
      "Regression score at exercise date 8 0.64781775230349\n",
      "Regression score at exercise date 7 0.5949917738082433\n",
      "Regression score at exercise date 6 0.5422815270461379\n",
      "Regression score at exercise date 5 0.4905739821727637\n",
      "Regression score at exercise date 4 0.4176305066908803\n",
      "Regression score at exercise date 3 0.33707859488420133\n",
      "Regression score at exercise date 2 0.23909997784641102\n",
      "Regression score at exercise date 1 0.13336249960477276\n"
     ]
    }
   ],
   "source": [
    "#compute true lower bounds\n",
    "lower_bound, lower_bound_std, ls_regression_models = ls_pricer.price(\n",
    "        signatures_training,\n",
    "        Payoff_training,\n",
    "        signatures_testing,\n",
    "        Payoff_testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e64a34fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e64a34fa",
    "outputId": "b116657f-4c30-4379-dbfb-5aa71549133d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Longstaff-Schwartz lower bound: 0.053620344280188206  0.00012266509655457878\n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Longstaff-Schwartz lower bound: {lower_bound}  {lower_bound_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a51c2",
   "metadata": {
    "id": "676a51c2"
   },
   "source": [
    "Similarly let us derive the upper bounds, but we will train the model only for $M= 5000$ paths to reduce computation time, and then compute true prices for all testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08fba8b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08fba8b2",
    "outputId": "710b6932-eb7b-4111-c511-20de6dd42eb6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.77 seconds needed to solve the linear program using CVXPY\n"
     ]
    }
   ],
   "source": [
    "M_dual = 5000\n",
    "upper_bound, upper_bound_std, MG = dual_pricer.price(\n",
    "        signatures_training[:M_dual],\n",
    "        Payoff_training[:M_dual],\n",
    "        dW_training[:M_dual,:,0],  # Select only the first component of the Brownian increments\n",
    "        signatures_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # Select only the first component of the Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39d591c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d591c8",
    "outputId": "e078ca47-ef36-4133-d227-c6d2b88c391d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Dual upper bound: 0.067181552774298  1.469314494101849e-07\n",
      "Pricing interval: (0.053620344280188206, 0.067181552774298) 0.00012266509655457878 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Linear Dual upper bound: {upper_bound}  {upper_bound_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(float(lower_bound),float(upper_bound))} {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab12b0",
   "metadata": {
    "id": "06ab12b0"
   },
   "source": [
    "# Improving the duality gap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hir98wJJXKE8",
   "metadata": {
    "id": "hir98wJJXKE8"
   },
   "source": [
    "Especially in rough regimes (here $H=0.1$), we observe a significant gap between lower and upper bounds, and in this section we present two ways to improve it. The first one still relies on linear signatures, but extends the basis as explained in in Section 4 of https://arxiv.org/abs/2312.03444."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ea7df",
   "metadata": {
    "id": "4d7ea7df"
   },
   "source": [
    "## Part 1: Extending the linear basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb042",
   "metadata": {
    "id": "beffb042"
   },
   "source": [
    "We consider a more involved basis by choosing the extended signature lift of $(t,X_t,\\phi(X_t))$, and additionally add Laguerre polynomials of $(X_t,v_t)$. We can again use the SignatureComputer to compute this extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1eb54f12",
   "metadata": {
    "id": "1eb54f12"
   },
   "outputs": [],
   "source": [
    "sig_computer_extended = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"payoff-and-polynomial-extended\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33d6249a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d6249a",
    "outputId": "53179ca8-2f48-4b44-b1eb-99f3548c0900"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing linear signature with payoff-and-polynomial-extended lift\n",
      "Computing linear signature with payoff-and-polynomial-extended lift\n"
     ]
    }
   ],
   "source": [
    "signatures_extended_training = sig_computer_extended.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_extended_testing = sig_computer_extended.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zFWMoC-X-kM",
   "metadata": {
    "id": "5zFWMoC-X-kM"
   },
   "source": [
    "Now we repeat the procedure for the extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c2d3960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1c2d3960",
    "outputId": "009708e1-bce5-42b9-861f-3f437c5136e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression score at exercise date 13 0.9120171195006237\n",
      "Regression score at exercise date 12 0.8691237773219215\n",
      "Regression score at exercise date 11 0.8261874387705715\n",
      "Regression score at exercise date 10 0.7812251922621689\n",
      "Regression score at exercise date 9 0.7228106216801085\n",
      "Regression score at exercise date 8 0.6753803972051351\n",
      "Regression score at exercise date 7 0.6239685161233455\n",
      "Regression score at exercise date 6 0.5812610789100479\n",
      "Regression score at exercise date 5 0.5235485839100782\n",
      "Regression score at exercise date 4 0.4482888724578813\n",
      "Regression score at exercise date 3 0.35927315779478464\n",
      "Regression score at exercise date 2 0.25179943871678845\n",
      "Regression score at exercise date 1 0.13809326678235068\n",
      "15.25 seconds needed to solve the linear program using CVXPY\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#compute true lower bounds for the new basis\n",
    "lower_bound_extended, lower_bound_extended_std, ls_regression_models_extended = ls_pricer.price(\n",
    "        signatures_extended_training,\n",
    "        Payoff_training,\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing\n",
    "    )\n",
    "#Repeating the dual procedure for the new basis\n",
    "upper_bound_extended, upper_bound_extended_std, MG_extended = dual_pricer.price(\n",
    "        signatures_extended_training[:M_dual,:,:],\n",
    "        Payoff_training[:M_dual,:],\n",
    "        dW_training[:M_dual,:,0],  # select first component of Brownian increments\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # select first component of Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "692d7161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "692d7161",
    "outputId": "5811cd15-9104-4ec6-fb2a-31da9a47991c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improve pricing interval: (0.05363693513009203, 0.06685287899878428) 0.00012266509655457878 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Improve pricing interval: {(float(lower_bound_extended),float(upper_bound_extended))} {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5c7df",
   "metadata": {
    "id": "17b5c7df"
   },
   "source": [
    "## Part 2: Deep log-signature optimal stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5f596",
   "metadata": {
    "id": "b7e5f596"
   },
   "source": [
    " In forthcoming work about \"American options in rough volatility models\", we will focus on more non-linear apporaches to price American options. More precisely, we extend the primal and dual procecdure by replacing linear functionals of the signature by deep neural networks on the log-signature $\\mathbb{L}=\\mathrm{log}^\\otimes(\\mathbb{X})$. This transformed version of the signature still captures the relevant information about the past of the underlying process, but grows much slower as the signature it self with respect to the truncation. Then, in order to learn highly non-linear functionals, such as the integrand of the Doob martingale (\"derivative of the Snell-envelope\"), we apply deep feedforward neural networks $\\theta$ on the log-signature. Of course, in both methods a optimization of the hyperparameters is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTyO0IKtbYse",
   "metadata": {
    "id": "kTyO0IKtbYse"
   },
   "source": [
    "We proceed as before, but replace the linear signature by the log-signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54699ea8",
   "metadata": {
    "id": "54699ea8"
   },
   "outputs": [],
   "source": [
    "sig_computer_log = SignatureComputer(T, N, 3, \"log\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e998aed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e998aed9",
    "outputId": "195d4128-74a3-4f42-a198-6310039f6c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (131072, 15), vol shape: (131072, 15), A shape: (131072, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (131072, 15), vol shape: (131072, 15), A shape: (131072, 253)\n",
      "Using 15 time steps for log signature computation\n"
     ]
    }
   ],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use first component and correct I_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc7ff201",
   "metadata": {
    "id": "bc7ff201"
   },
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_signatures_optimal_stopping import DeepLongstaffSchwartzPricer, DeepDualPricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "747bdf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (131072, 15), vol shape: (131072, 15), A shape: (131072, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "Computing log signature with polynomial-vol lift\n",
      "X shape: (131072, 15), vol shape: (131072, 15), A shape: (131072, 253)\n",
      "Using 15 time steps for log signature computation\n",
      "shape of dW_training (131072, 14, 2)\n",
      "shape of dW_testing (131072, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use first component and correct I_testing\n",
    ")\n",
    "print(\"shape of dW_training\", dW_training.shape)\n",
    "print(\"shape of dW_testing\", dW_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d0eff",
   "metadata": {
    "id": "9a1d0eff"
   },
   "source": [
    "The DeepLongstaffSchwartzPricer generalizes the LinearLongstaffSchwartzPrices, where the Ridge Regression at each exercise date is replace by learning the conditional expectations via neural networks. In the following initialization we build a network with $3$ hidden layers and $16$ neurons each, between each hidden layer we apply the activation function $\\mathrm{tanh}(x)$. The remainding parameters are set to 'False'. (One can run the 'Hyperparameter_optimization_primal.py' file to optimize the choice of hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2da432f",
   "metadata": {
    "id": "b2da432f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression at exercise date 13\n",
      "Epoch 1/15\n",
      "402/402 [==============================] - 2s 5ms/step - loss: 0.0068 - mae: 0.0381\n",
      "Epoch 2/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.5770e-04 - mae: 0.0123\n",
      "Epoch 3/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.4104e-04 - mae: 0.0117\n",
      "Epoch 4/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3886e-04 - mae: 0.0117\n",
      "Epoch 5/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3928e-04 - mae: 0.0117\n",
      "Epoch 6/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3773e-04 - mae: 0.0117\n",
      "Epoch 7/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3759e-04 - mae: 0.0117\n",
      "Epoch 8/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3876e-04 - mae: 0.0117\n",
      "Epoch 9/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3968e-04 - mae: 0.0118\n",
      "Epoch 10/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3925e-04 - mae: 0.0118\n",
      "Epoch 11/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3946e-04 - mae: 0.0118\n",
      "Epoch 12/15\n",
      "402/402 [==============================] - 2s 4ms/step - loss: 3.3906e-04 - mae: 0.0118\n",
      "3215/3215 [==============================] - 2s 626us/step\n",
      "Regression at exercise date 12\n",
      "410/410 [==============================] - 2s 4ms/step - loss: 3.8214e-04 - mae: 0.0123\n",
      "3274/3274 [==============================] - 2s 634us/step\n",
      "Regression at exercise date 11\n",
      "418/418 [==============================] - 2s 4ms/step - loss: 5.9546e-04 - mae: 0.0152\n",
      "3339/3339 [==============================] - 2s 636us/step\n",
      "Regression at exercise date 10\n",
      "426/426 [==============================] - 2s 4ms/step - loss: 5.5621e-04 - mae: 0.0159\n",
      "3402/3402 [==============================] - 2s 644us/step\n",
      "Regression at exercise date 9\n",
      "434/434 [==============================] - 2s 5ms/step - loss: 8.5607e-04 - mae: 0.0204\n",
      "3472/3472 [==============================] - 2s 647us/step\n",
      "Regression at exercise date 8\n",
      "445/445 [==============================] - 2s 4ms/step - loss: 0.0010 - mae: 0.0228\n",
      "3553/3553 [==============================] - 2s 645us/step\n",
      "Regression at exercise date 7\n",
      "454/454 [==============================] - 2s 4ms/step - loss: 0.0013 - mae: 0.0262\n",
      "3630/3630 [==============================] - 2s 646us/step\n",
      "Regression at exercise date 6\n",
      "464/464 [==============================] - 2s 4ms/step - loss: 0.0015 - mae: 0.0287\n",
      "3707/3707 [==============================] - 2s 646us/step\n",
      "Regression at exercise date 5\n",
      "475/475 [==============================] - 2s 4ms/step - loss: 5.7389e-04 - mae: 0.0142\n",
      "3795/3795 [==============================] - 3s 664us/step\n",
      "Regression at exercise date 4\n",
      "485/485 [==============================] - 2s 4ms/step - loss: 3.3781e-04 - mae: 0.0120\n",
      "3877/3877 [==============================] - 3s 643us/step\n",
      "Regression at exercise date 3\n",
      "495/495 [==============================] - 2s 4ms/step - loss: 6.3937e-04 - mae: 0.0175\n",
      "3956/3956 [==============================] - 3s 663us/step\n",
      "Regression at exercise date 2\n",
      "504/504 [==============================] - 2s 4ms/step - loss: 6.0845e-04 - mae: 0.0180\n",
      "4026/4026 [==============================] - 3s 651us/step\n",
      "Regression at exercise date 1\n",
      "510/510 [==============================] - 2s 4ms/step - loss: 3.3073e-04 - mae: 0.0123\n",
      "4076/4076 [==============================] - 3s 653us/step\n",
      "4096/4096 [==============================] - 3s 656us/step\n",
      "4096/4096 [==============================] - 3s 648us/step\n",
      "4096/4096 [==============================] - 3s 678us/step\n",
      "4096/4096 [==============================] - 3s 644us/step\n",
      "4096/4096 [==============================] - 3s 646us/step\n",
      "4096/4096 [==============================] - 3s 746us/step\n",
      "4096/4096 [==============================] - 3s 651us/step\n",
      "4096/4096 [==============================] - 3s 662us/step\n",
      "4096/4096 [==============================] - 3s 647us/step\n",
      "4096/4096 [==============================] - 3s 657us/step\n",
      "4096/4096 [==============================] - 3s 654us/step\n",
      "4096/4096 [==============================] - 3s 654us/step\n",
      "4096/4096 [==============================] - 3s 659us/step\n",
      "Signature data shape: (131072, 15, 8)\n",
      "Payoff data shape: (131072, 15)\n",
      "dW data shape: (131072, 14)\n",
      "Using 14 time steps instead of 252 for model building\n",
      "Using indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] for 14 exercise dates\n",
      "Using indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] for 14 exercise dates\n",
      "Model expects Payoff shape: (batch_size, 14)\n",
      "Trimmed Payoff shape: (131072, 14)\n",
      "Epoch 1/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0827 - val_loss: 0.0823\n",
      "Epoch 2/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0823 - val_loss: 0.0822\n",
      "Epoch 3/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0823 - val_loss: 0.0822\n",
      "Epoch 4/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0822 - val_loss: 0.0821\n",
      "Epoch 5/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0822 - val_loss: 0.0822\n",
      "Epoch 6/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0822 - val_loss: 0.0821\n",
      "Epoch 7/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0821 - val_loss: 0.0823\n",
      "Epoch 8/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0821 - val_loss: 0.0820\n",
      "Epoch 9/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0821 - val_loss: 0.0820\n",
      "Epoch 10/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0821 - val_loss: 0.0821\n",
      "Epoch 11/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0821 - val_loss: 0.0823\n",
      "Epoch 12/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0821 - val_loss: 0.0821\n",
      "Epoch 13/15\n",
      "461/461 [==============================] - 3s 7ms/step - loss: 0.0821 - val_loss: 0.0820\n",
      "Epoch 14/15\n",
      "461/461 [==============================] - 3s 8ms/step - loss: 0.0821 - val_loss: 0.0822\n",
      "Epoch 15/15\n",
      "461/461 [==============================] - 4s 8ms/step - loss: 0.0821 - val_loss: 0.0821\n",
      "4096/4096 [==============================] - 8s 2ms/step\n",
      "4096/4096 [==============================] - 7s 2ms/step\n",
      "MG shape: (131072, 14)\n",
      "MG with zeros shape: (131072, 15)\n"
     ]
    }
   ],
   "source": [
    "ls_pricer = DeepLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='tanh',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # This is correct as float\n",
    "    dropout=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "\n",
    "dual_pricer = DeepDualPricer(\n",
    "    N1=N1,\n",
    "    N=N,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # ERROR: should be float, not boolean\n",
    "    dropout=False,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "# LS pricer call is correct\n",
    "lower_bound_deep, lower_bound_deep_std, ls_regression_models = ls_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    M_val=0,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Dual pricer call is correct\n",
    "y0, upper_bound_deep, upper_bound_deep_std, dual_model, dual_rule_model = dual_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    dW_training[:,:,0],  # use only first component of Brownian increments\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    dW_testing[:,:,0],  # use only first component of Brownian increments\n",
    "    M_val=int(0.9*M),\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jX2tyIWchsd",
   "metadata": {
    "id": "9jX2tyIWchsd"
   },
   "source": [
    "Similarly for the dual problem, we consider the same network but use the $relu(x)$ activation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "186917ea",
   "metadata": {
    "id": "186917ea"
   },
   "outputs": [],
   "source": [
    "# Consistent parameter usage for validation set size\n",
    "M_val_percentage = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cDQUUvN_c6wl",
   "metadata": {
    "id": "cDQUUvN_c6wl"
   },
   "source": [
    "The Deep Longstaff Schwartz uses $15$ epochs for at the last exercise date, and then one epochs at the remainding ones by initiliazing smartly. The learning rate for the Stochastic Gradient Descent is choosen as $0.001$, and we use batch sizes of $2^8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d04e9ff6",
   "metadata": {
    "id": "d04e9ff6",
    "outputId": "8fd83460-c4c2-40ca-8718-2b02f3227db5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Longstaff-Schwartz lower bound: 0.05005361095340205  7.127670084829288e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Deep Longstaff-Schwartz lower bound: {lower_bound_deep}  {lower_bound_deep_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "RsfNfQyIebJD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsfNfQyIebJD",
    "outputId": "7fa5e436-d332-4232-c5e7-1a2784f5d8f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Dual upper bound: 0.0715030415031311  6.832770578573121e-05\n",
      "Pricing interval: (0.05005361095340205, 0.0715030415031311) 7.127670084829288e-05 \n"
     ]
    }
   ],
   "source": [
    "print(f\"Deep Dual upper bound: {upper_bound_deep}  {upper_bound_deep_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(lower_bound_deep,upper_bound_deep)} {np.maximum(upper_bound_deep_std,lower_bound_deep_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd49d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_kernel_signature_optimal_stopping import DeepKernelLongstaffSchwartzPricer, DeepKernelDualPricer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "96187943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Define the RFF feature computation functions with corrected implementation\n",
    "\n",
    "def compute_rff_kernel_features(signatures, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for lower bound pricer (list format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of tensors with shape [M, rff_dim*2, 1] for each exercise date\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices based on actual data dimensions\n",
    "    actual_steps = T_steps - 1\n",
    "    subindex = [min(int((j+1)*actual_steps/N1), actual_steps) for j in range(N1)]\n",
    "    \n",
    "    print(f\"Signature data has {T_steps} time points\")\n",
    "    print(f\"Using exercise indices: {subindex}\")\n",
    "    \n",
    "    # Create list to hold RFF features for each exercise date\n",
    "    rff_features_list = []\n",
    "    \n",
    "    # For each exercise date\n",
    "    for t in range(len(subindex)):\n",
    "        idx = min(subindex[t], T_steps-1)\n",
    "        X_t = signatures[:, idx, :]\n",
    "        \n",
    "        # Generate random projection matrix for RBF kernel approximation\n",
    "        np.random.seed(42 + t)  # Different seed for each exercise date\n",
    "        W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "        \n",
    "        # Compute RFF: [cos(Wx), sin(Wx)]\n",
    "        projection = X_t @ W\n",
    "        rff_features = np.column_stack([\n",
    "            np.cos(projection),\n",
    "            np.sin(projection)\n",
    "        ]) * np.sqrt(1/rff_dim)\n",
    "        \n",
    "        # Reshape to match expected format: [M, rff_dim*2, 1]\n",
    "        rff_features = rff_features.reshape(M, rff_dim*2, 1)\n",
    "        \n",
    "        rff_features_list.append(rff_features)\n",
    "    \n",
    "    return rff_features_list\n",
    "\n",
    "def compute_rff_kernel_features_dual(signatures, N, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for dual pricer (3D tensor format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N: Number of time steps in the discretization\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with shape [M, features, time] for all time points\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices proportional to exercise dates\n",
    "    # We need to map our exercise indices to the full discretization grid\n",
    "    actual_steps = min(T_steps - 1, N)\n",
    "    all_indices = np.minimum(np.array([int(t * T_steps / (N+1)) for t in range(N+1)]), T_steps-1)\n",
    "    \n",
    "    print(f\"Using exercise indices for dual pricer: {all_indices[:5]}...{all_indices[-5:]}\")\n",
    "    \n",
    "    # Generate random projection matrix once\n",
    "    np.random.seed(42)\n",
    "    W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "    \n",
    "    # Extract all required signature data at once\n",
    "    X_all = signatures[:, all_indices, :]  # Shape: [M, N+1, feature_dim]\n",
    "    \n",
    "    # Reshape for batch matrix multiplication\n",
    "    X_reshaped = X_all.reshape(-1, feature_dim)  # Shape: [M*(N+1), feature_dim]\n",
    "    \n",
    "    # Compute all projections at once\n",
    "    projections = X_reshaped @ W  # Shape: [M*(N+1), rff_dim]\n",
    "    \n",
    "    # Compute RFF features\n",
    "    cos_features = np.cos(projections)\n",
    "    sin_features = np.sin(projections)\n",
    "    rff_features = np.column_stack([cos_features, sin_features]) * np.sqrt(1/rff_dim)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    full_rff = rff_features.reshape(M, N+1, rff_dim*2)\n",
    "    \n",
    "    # Transpose to match expected format: [M, features, time]\n",
    "    return np.transpose(full_rff, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "565afbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing kernel features for lower bound...\n",
      "Signature data has 15 time points\n",
      "Using exercise indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Signature data has 15 time points\n",
      "Using exercise indices: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "Computing kernel features for upper bound...\n",
      "Using exercise indices for dual pricer: [0 1 2 3 4]...[10 11 12 13 14]\n",
      "Using exercise indices for dual pricer: [0 1 2 3 4]...[10 11 12 13 14]\n",
      "Computing kernel-based lower bound...\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (Batch  (None, 128)               512       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " layer_normalization (Layer  (None, 128)               256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_1 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_2 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_3 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_4 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_5 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 13\n",
      "Epoch 1/15\n",
      "402/402 [==============================] - 6s 14ms/step - loss: 0.1403 - r2_score: -6.9413\n",
      "Epoch 2/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.1032 - r2_score: 0.2170\n",
      "Epoch 3/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0891 - r2_score: 0.5293\n",
      "Epoch 4/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0754 - r2_score: 0.6532\n",
      "Epoch 5/15\n",
      "402/402 [==============================] - 6s 14ms/step - loss: 0.0626 - r2_score: 0.7189\n",
      "Epoch 6/15\n",
      "402/402 [==============================] - 5s 13ms/step - loss: 0.0510 - r2_score: 0.7600\n",
      "Epoch 7/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0408 - r2_score: 0.7929\n",
      "Epoch 8/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0321 - r2_score: 0.8001\n",
      "Epoch 9/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0249 - r2_score: 0.8251\n",
      "Epoch 10/15\n",
      "402/402 [==============================] - 6s 14ms/step - loss: 0.0191 - r2_score: 0.8363\n",
      "Epoch 11/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0144 - r2_score: 0.8543\n",
      "Epoch 12/15\n",
      "402/402 [==============================] - 5s 13ms/step - loss: 0.0108 - r2_score: 0.8606\n",
      "Epoch 13/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0080 - r2_score: 0.8657\n",
      "Epoch 14/15\n",
      "402/402 [==============================] - 5s 12ms/step - loss: 0.0060 - r2_score: 0.8726\n",
      "Epoch 15/15\n",
      "402/402 [==============================] - 5s 13ms/step - loss: 0.0045 - r2_score: 0.8691\n",
      "3215/3215 [==============================] - 9s 3ms/step\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_1 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_6 (Lay  (None, 128)               256       \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_7 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_8 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_9 (Lay  (None, 32)                64        \n",
      " erNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_10 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_11 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 12\n",
      "410/410 [==============================] - 5s 12ms/step - loss: 0.0045 - r2_score: 0.6614\n",
      "3274/3274 [==============================] - 9s 3ms/step\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_12 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_13 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_14 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_15 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_16 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_17 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 11\n",
      "418/418 [==============================] - 5s 13ms/step - loss: 0.0033 - r2_score: 0.7865\n",
      "3339/3339 [==============================] - 9s 3ms/step\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_18 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_19 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_20 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_21 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_22 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_23 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 10\n",
      "426/426 [==============================] - 5s 12ms/step - loss: 0.0026 - r2_score: 0.8050\n",
      "3402/3402 [==============================] - 9s 3ms/step\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_4 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_24 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_25 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_26 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_27 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_28 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_29 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 9\n",
      "434/434 [==============================] - 5s 13ms/step - loss: 0.0024 - r2_score: 0.7097\n",
      "3472/3472 [==============================] - 9s 3ms/step\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_5 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_30 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_31 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_32 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_33 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_34 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_35 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 8\n",
      "445/445 [==============================] - 6s 13ms/step - loss: 0.0019 - r2_score: 0.7613\n",
      "3553/3553 [==============================] - 11s 3ms/step\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_6 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_36 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_37 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_38 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_39 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_40 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_41 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 7\n",
      "454/454 [==============================] - 6s 13ms/step - loss: 0.0019 - r2_score: 0.6657\n",
      "3630/3630 [==============================] - 10s 3ms/step\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_7 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_42 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_43 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_44 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_45 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_46 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_47 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 6\n",
      "464/464 [==============================] - 6s 12ms/step - loss: 0.0014 - r2_score: 0.7112\n",
      "3707/3707 [==============================] - 10s 3ms/step\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_8 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_48 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_49 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_50 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_51 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_52 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_53 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 5\n",
      "475/475 [==============================] - 6s 13ms/step - loss: 0.0012 - r2_score: 0.7224\n",
      "3795/3795 [==============================] - 10s 3ms/step\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " layer_normalization_54 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_55 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_56 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_57 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_58 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_59 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 4\n",
      "485/485 [==============================] - 6s 12ms/step - loss: 0.0014 - r2_score: 0.5702\n",
      "3877/3877 [==============================] - 10s 3ms/step\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_10 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_60 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_61 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_62 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_63 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_64 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_65 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 3\n",
      "495/495 [==============================] - 6s 13ms/step - loss: 0.0016 - r2_score: 0.4147\n",
      "3956/3956 [==============================] - 11s 3ms/step\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_11 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_66 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_67 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_68 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_69 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_70 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_71 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 2\n",
      "504/504 [==============================] - 6s 12ms/step - loss: 0.0011 - r2_score: 0.4338\n",
      "4026/4026 [==============================] - 11s 3ms/step\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_12 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_72 (La  (None, 128)               256       \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 32)                4128      \n",
      "                                                                 \n",
      " layer_normalization_73 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_74 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_75 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " layer_normalization_76 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " layer_normalization_77 (La  (None, 32)                64        \n",
      " yerNormalization)                                               \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7361 (28.75 KB)\n",
      "Trainable params: 7105 (27.75 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "_________________________________________________________________\n",
      "Regression at exercise date 1\n",
      "510/510 [==============================] - 6s 13ms/step - loss: 0.0013 - r2_score: 0.1929\n",
      "4076/4076 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 12s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 12s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "4096/4096 [==============================] - 11s 3ms/step\n",
      "Computing kernel-based upper bound...\n",
      "Building network with parameters: I=4, q=160, d=128, activation=relu\n",
      "Building network with parameters: I=4, q=160, d=128, activation=relu\n",
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " sig (InputLayer)            [(None, 15, 128)]            0         []                            \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 15, 128)              512       ['sig[0][0]']                 \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_117 (Dense)           (None, 15, 160)              20640     ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_84 (La  (None, 15, 160)              320       ['dense_117[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_118 (Dense)           (None, 15, 160)              25760     ['layer_normalization_84[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_85 (La  (None, 15, 160)              320       ['dense_118[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 15, 160)              0         ['layer_normalization_85[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_86 (La  (None, 15, 160)              320       ['dropout_3[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_119 (Dense)           (None, 15, 160)              25760     ['layer_normalization_86[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_87 (La  (None, 15, 160)              320       ['dense_119[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 15, 160)              0         ['layer_normalization_87[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_88 (La  (None, 15, 160)              320       ['dropout_4[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dense_120 (Dense)           (None, 15, 160)              25760     ['layer_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_89 (La  (None, 15, 160)              320       ['dense_120[0][0]']           \n",
      " yerNormalization)                                                                                \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 15, 160)              0         ['layer_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_121 (Dense)           (None, 15, 1)                161       ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)         (None, 15)                   0         ['dense_121[0][0]']           \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5  (None, 14)                   0         ['flatten_3[0][0]']           \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " dW (InputLayer)             [(None, 14)]                 0         []                            \n",
      "                                                                                                  \n",
      " deep_martingales_3 (DeepMa  (None, 14)                   0         ['tf.__operators__.getitem_5[0\n",
      " rtingales)                                                         ][0]',                        \n",
      "                                                                     'dW[0][0]']                  \n",
      "                                                                                                  \n",
      " Y (InputLayer)              [(None, 14)]                 0         []                            \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_6  (None, 14)                   0         ['deep_martingales_3[0][0]']  \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7  (None, 14)                   0         ['Y[0][0]']                   \n",
      "  (SlicingOpLambda)                                                                               \n",
      "                                                                                                  \n",
      " dual_stopping_loss_3 (Dual  ()                           0         ['tf.__operators__.getitem_6[0\n",
      " StoppingLoss)                                                      ][0]',                        \n",
      "                                                                     'tf.__operators__.getitem_7[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 100513 (392.63 KB)\n",
      "Trainable params: 100257 (391.63 KB)\n",
      "Non-trainable params: 256 (1.00 KB)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/15\n",
      "461/461 [==============================] - 39s 84ms/step - loss: 0.5695 - val_loss: 0.3362\n",
      "Epoch 2/15\n",
      "461/461 [==============================] - 39s 84ms/step - loss: 0.2211 - val_loss: 0.1378\n",
      "Epoch 3/15\n",
      "461/461 [==============================] - 40s 86ms/step - loss: 0.1050 - val_loss: 0.0822\n",
      "Epoch 4/15\n",
      "461/461 [==============================] - 38s 83ms/step - loss: 0.0757 - val_loss: 0.0707\n",
      "Epoch 5/15\n",
      "461/461 [==============================] - 34s 75ms/step - loss: 0.0697 - val_loss: 0.0684\n",
      "Epoch 6/15\n",
      "461/461 [==============================] - 37s 79ms/step - loss: 0.0687 - val_loss: 0.0678\n",
      "Epoch 7/15\n",
      "461/461 [==============================] - 35s 75ms/step - loss: 0.0684 - val_loss: 0.0679\n",
      "Epoch 8/15\n",
      "461/461 [==============================] - 34s 74ms/step - loss: 0.0684 - val_loss: 0.0680\n",
      "Epoch 9/15\n",
      "461/461 [==============================] - 34s 74ms/step - loss: 0.0683 - val_loss: 0.0678\n",
      "Epoch 10/15\n",
      "461/461 [==============================] - 36s 77ms/step - loss: 0.0683 - val_loss: 0.0676\n",
      "Epoch 11/15\n",
      "461/461 [==============================] - 35s 76ms/step - loss: 0.0682 - val_loss: 0.0677\n",
      "Epoch 12/15\n",
      "461/461 [==============================] - 35s 76ms/step - loss: 0.0683 - val_loss: 0.0677\n",
      "Epoch 13/15\n",
      "461/461 [==============================] - 34s 74ms/step - loss: 0.0682 - val_loss: 0.0676\n",
      "Epoch 14/15\n",
      "461/461 [==============================] - 33s 71ms/step - loss: 0.0681 - val_loss: 0.0677\n",
      "Epoch 15/15\n",
      "461/461 [==============================] - 33s 71ms/step - loss: 0.0682 - val_loss: 0.0677\n",
      "4096/4096 [==============================] - 27s 7ms/step\n",
      "4096/4096 [==============================] - 25s 6ms/step\n",
      "Deep Kernel Longstaff-Schwartz lower bound: 0.05085179186152318  5.9909287707761646e-05\n",
      "Deep Kernel Dual upper bound: 0.0670540232458464  5.1701906437363074e-05\n",
      "Kernel-based pricing interval: [0.05085179186152318, 0.0670540232458464]  5.9909287707761646e-05\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Calculate and use RFF features for pricing with corrected implementation\n",
    "\n",
    "# Calculate both sets of features\n",
    "rff_dim = 64\n",
    "print(\"Computing kernel features for lower bound...\")\n",
    "kernel_training = compute_rff_kernel_features(log_signatures_training, N1, rff_dim=rff_dim)\n",
    "kernel_testing = compute_rff_kernel_features(log_signatures_testing, N1, rff_dim=rff_dim)\n",
    "\n",
    "# IMPORTANT: For the dual approach, we need to generate features for N1 steps\n",
    "print(\"Computing kernel features for upper bound...\")\n",
    "kernel_training_dual = compute_rff_kernel_features_dual(log_signatures_training, N1, N1, rff_dim=rff_dim)\n",
    "kernel_testing_dual = compute_rff_kernel_features_dual(log_signatures_testing, N1, N1, rff_dim=rff_dim)\n",
    "print(\"Initializing kernel-based lower bound...\")\n",
    "# 1. LOWER BOUND calculation - this works correctly\n",
    "kernel_pricer = DeepKernelLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    L=rff_dim*2,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=False,\n",
    "    layer_normalization=True\n",
    ")\n",
    "\n",
    "print(\"Computing kernel-based lower bound...\")\n",
    "lower_bound_kernel, lower_bound_kernel_std, kernel_models = kernel_pricer.price(\n",
    "    kernel_training,\n",
    "    kernel_testing,\n",
    "    Payoff_training,\n",
    "    Payoff_testing,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "# 2. UPPER BOUND calculation - use direct payoff, no need to expand\n",
    "print(\"Initializing kernel-based upper bound...\")\n",
    "kernel_dual_pricer = DeepKernelDualPricer(\n",
    "    N1=N1,\n",
    "    N=N1,  # Using N1 instead of N=252 here is the key change\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=4,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=True,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=True,\n",
    "    mode_dim=\"1-dim\"\n",
    ")\n",
    "print(\"Computing kernel-based upper bound...\")\n",
    "try:\n",
    "    y0_kernel, upper_bound_kernel, upper_bound_kernel_std, kernel_model, kernel_rule_model = kernel_dual_pricer.price(\n",
    "        kernel_training_dual,\n",
    "        Payoff_training,\n",
    "        dW_training[:,:,0],\n",
    "        kernel_testing_dual,\n",
    "        Payoff_testing,      \n",
    "        dW_testing[:,:,0],\n",
    "        M_val=int(0.9*M),\n",
    "        batch=2**8,\n",
    "        epochs=15,\n",
    "        learning_rate=0.0005\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel}  {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Deep Kernel Dual upper bound: {upper_bound_kernel}  {upper_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Kernel-based pricing interval: [{lower_bound_kernel}, {upper_bound_kernel}]  {np.maximum(upper_bound_kernel_std, lower_bound_kernel_std)/np.sqrt(M2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in dual pricer: {e}\")\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel}  {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(\"Upper bound calculation failed - using only lower bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2K5Yrl-ejcN",
   "metadata": {
    "id": "a2K5Yrl-ejcN"
   },
   "source": [
    "We once again stress that the parameters for the the discretization (here $J=120$), the sample size (here $M=10^{15}$), and the signature trunaction level (here $K=3$) are not choosen big enough to get narrow gaps, but we can still already observe an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b1b07",
   "metadata": {},
   "source": [
    "## Step 5: Contextualizing Theoretical Price in USD\n",
    "Convert the normalized model price bounds into USD per share and per contract, and print actionable trading recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1abfbfb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Method</th>\n",
       "      <th>Lower Bound (USD)</th>\n",
       "      <th>Upper Bound (USD)</th>\n",
       "      <th>Std Error (USD)</th>\n",
       "      <th>Price Gap (USD)</th>\n",
       "      <th>Gap (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Linear Signature</td>\n",
       "      <td>$5.36</td>\n",
       "      <td>$6.72</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$1.36</td>\n",
       "      <td>25.29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Extended Linear Signature</td>\n",
       "      <td>$5.36</td>\n",
       "      <td>$6.69</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$1.32</td>\n",
       "      <td>24.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Deep Log-Signature</td>\n",
       "      <td>$5.01</td>\n",
       "      <td>$7.15</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$2.14</td>\n",
       "      <td>42.85%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Deep Kernel Method</td>\n",
       "      <td>$5.09</td>\n",
       "      <td>$6.71</td>\n",
       "      <td>$0.01</td>\n",
       "      <td>$1.62</td>\n",
       "      <td>31.86%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the normalized price bounds to actual USD values\n",
    "actual_stock_price = X0 * 100  # USD per share\n",
    "actual_strike = strike * actual_stock_price  # USD per share\n",
    "\n",
    "# Include all four methods in the list\n",
    "methods = [\n",
    "    \"Linear Signature\", \n",
    "    \"Extended Linear Signature\", \n",
    "    \"Deep Log-Signature\",\n",
    "    \"Deep Kernel Method\"  # Added the kernel method\n",
    "]\n",
    "\n",
    "# Collect all price bounds\n",
    "lower_bounds = [lower_bound, lower_bound_extended, lower_bound_deep, lower_bound_kernel]\n",
    "upper_bounds = [upper_bound, upper_bound_extended, upper_bound_deep, upper_bound_kernel]\n",
    "stds = [lower_bound_std, lower_bound_extended_std, lower_bound_deep_std, lower_bound_kernel_std]\n",
    "\n",
    "# Create a table of results\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "results = []\n",
    "for i, method in enumerate(methods):\n",
    "    usd_lower = float(lower_bounds[i]) * actual_stock_price\n",
    "    \n",
    "    # Handle the case where upper bound might not be available for kernel method\n",
    "    if i == 3 and 'upper_bound_kernel' not in locals():\n",
    "        usd_upper = float('nan')  # Use NaN if upper bound isn't available\n",
    "    else:\n",
    "        usd_upper = float(upper_bounds[i]) * actual_stock_price\n",
    "    \n",
    "    usd_std = float(stds[i]) * actual_stock_price / np.sqrt(M2)\n",
    "    \n",
    "    # Calculate gap only if upper bound exists\n",
    "    if not np.isnan(usd_upper):\n",
    "        gap = usd_upper - usd_lower\n",
    "        gap_percent = gap / usd_lower * 100\n",
    "    else:\n",
    "        gap = float('nan')\n",
    "        gap_percent = float('nan')\n",
    "    \n",
    "    results.append({\n",
    "        \"Method\": method,\n",
    "        \"Lower Bound (USD)\": f\"${usd_lower:.2f}\",\n",
    "        \"Upper Bound (USD)\": f\"${usd_upper:.2f}\" if not np.isnan(usd_upper) else \"N/A\",\n",
    "        \"Std Error (USD)\": f\"${usd_std:.2f}\",\n",
    "        \"Price Gap (USD)\": f\"${gap:.2f}\" if not np.isnan(gap) else \"N/A\",\n",
    "        \"Gap (%)\": f\"{gap_percent:.2f}%\" if not np.isnan(gap_percent) else \"N/A\"\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(HTML(results_df.to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b75c6",
   "metadata": {},
   "source": [
    "## Option Trading Interpretation\n",
    "\n",
    "Now let's interpret these results from a trading perspective. We'll evaluate the fair price range for an American put option contract (which typically represents 100 shares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "403373a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American Put Option Contract Analysis (for 100 shares)\n",
      "=====================================================================\n",
      "Stock Price: $100.00\n",
      "Strike Price: $105.00\n",
      "Time to Maturity: 14 years\n",
      "Interest Rate: 5.00%\n",
      "Rough Volatility Parameters: H=0.07, =1.9, =-0.9, =0.09\n",
      "=====================================================================\n",
      "Fair Price Range: $5.01 to $7.15 per contract\n",
      "Midpoint Price: $6.08\n",
      "=====================================================================\n",
      "Trading Recommendations:\n",
      "\n",
      "If market price is $4.00 (Below Fair Value):\n",
      " BUY: Market price is below fair value range\n",
      " Expected edge: $1.00 to $3.15 per contract\n",
      " Consider buying puts for protection or speculative profit\n",
      "\n",
      "If market price is $6.08 (At Fair Value):\n",
      " NEUTRAL: Market price is within fair value range\n",
      " Price is positioned 50% through the fair value range\n",
      " No clear edge for buying or selling\n",
      "\n",
      "If market price is $8.58 (Above Fair Value):\n",
      " SELL: Market price is above fair value range\n",
      " Expected edge: $1.43 to $3.58 per contract\n",
      " Consider writing puts, potentially as part of a spread strategy to limit risk\n"
     ]
    }
   ],
   "source": [
    "# Now we can calculate the fair price range for a standard options contract\n",
    "# Cell under # For a standard options contract (100 shares)\n",
    "shares_per_contract = 100\n",
    "contract_lower   = float(lower_bound_deep) * actual_stock_price\n",
    "contract_upper   = float(upper_bound_deep) * actual_stock_price\n",
    "contract_midpoint = (contract_lower + contract_upper) / 2\n",
    "\n",
    "print(f\"American Put Option Contract Analysis (for {shares_per_contract} shares)\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Stock Price: ${actual_stock_price:.2f}\")\n",
    "print(f\"Strike Price: ${actual_strike:.2f}\")\n",
    "print(f\"Time to Maturity: {T} days\")\n",
    "print(f\"Interest Rate: {r*100:.2f}%\")\n",
    "print(f\"Rough Volatility Parameters: H={H}, ={eta}, ={rho}, ={xi}\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Fair Price Range: ${contract_lower:.2f} to ${contract_upper:.2f} per contract\")\n",
    "print(f\"Midpoint Price: ${contract_midpoint:.2f}\")\n",
    "print(f\"=====================================================================\")\n",
    "\n",
    "# Trading recommendations based on market prices\n",
    "hypothetical_market_prices = [contract_lower * 0.8, contract_midpoint, contract_upper * 1.2]\n",
    "labels = [\"Below Fair Value\", \"At Fair Value\", \"Above Fair Value\"]\n",
    "\n",
    "print(\"Trading Recommendations:\")\n",
    "for price, label in zip(hypothetical_market_prices, labels):\n",
    "    print(f\"\\nIf market price is ${price:.2f} ({label}):\")\n",
    "    \n",
    "    if price < contract_lower:\n",
    "        print(\" BUY: Market price is below fair value range\")\n",
    "        print(f\" Expected edge: ${(contract_lower - price):.2f} to ${(contract_upper - price):.2f} per contract\")\n",
    "        print(\" Consider buying puts for protection or speculative profit\")\n",
    "    elif price > contract_upper:\n",
    "        print(\" SELL: Market price is above fair value range\")\n",
    "        print(f\" Expected edge: ${price - contract_upper:.2f} to ${price - contract_lower:.2f} per contract\")\n",
    "        print(\" Consider writing puts, potentially as part of a spread strategy to limit risk\")\n",
    "    else:\n",
    "        print(\" NEUTRAL: Market price is within fair value range\")\n",
    "        position = (price - contract_lower) / (contract_upper - contract_lower)\n",
    "        print(f\" Price is positioned {position:.0%} through the fair value range\")\n",
    "        if position < 0.4:\n",
    "            print(\" Slight bias toward buying\")\n",
    "        elif position > 0.6:\n",
    "            print(\" Slight bias toward selling\")\n",
    "        else:\n",
    "            print(\" No clear edge for buying or selling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30e089",
   "metadata": {},
   "source": [
    "## Risk Management Considerations\n",
    "\n",
    "When trading American put options in a rough volatility environment, several risk management considerations are important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab5bf077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Management Considerations:\n",
      "=====================================================================\n",
      "Moneyness: 0.95 (5% in-the-money)\n",
      "Time Value: $0.01 per share\n",
      "Uncertainty Range: $2.14 per contract\n",
      "\n",
      "Recommended Risk Management Strategies:\n",
      "---------------------------------------------------------------------\n",
      "1. Position Sizing: Limit exposure to <5% of portfolio per trade\n",
      "2. Early Exercise Consideration: Monitor optimal stopping boundaries\n",
      "3. Hedging: Consider delta and vega hedging for larger positions\n",
      "4. Model Risk: Be aware model assumes H=0.07, may differ from market\n",
      "\n",
      "Practical Implementation:\n",
      "---------------------------------------------------------------------\n",
      " ATM option: Maximum gamma/vega exposure\n",
      " Most sensitive to changes in volatility and rough volatility parameters\n",
      " Actively monitor for optimal early exercise conditions near expiration\n",
      "\n",
      "Note: This model incorporates rough volatility effects (H=0.07) which\n",
      "traditional models like Black-Scholes miss. This can be particularly\n",
      "important for managing risk in volatile market conditions.\n"
     ]
    }
   ],
   "source": [
    "# Calculate additional risk metrics\n",
    "# In the # Calculate additional risk metrics cell\n",
    "percent_itm = max(0, (actual_strike - actual_stock_price) / actual_strike * 100)\n",
    "\n",
    "moneyness = actual_stock_price / actual_strike\n",
    "time_value = float(lower_bound_deep) * actual_stock_price - max(0, actual_strike - actual_stock_price)\n",
    "model_implied_volatility = 0.3  # This would typically be backed out from the model price\n",
    "\n",
    "print(\"Risk Management Considerations:\")\n",
    "print(\"=====================================================================\")\n",
    "print(f\"Moneyness: {moneyness:.2f} ({percent_itm:.0f}% in-the-money)\")\n",
    "print(f\"Time Value: ${time_value:.2f} per share\")\n",
    "print(f\"Uncertainty Range: ${(contract_upper - contract_lower):.2f} per contract\")\n",
    "print(\"\\nRecommended Risk Management Strategies:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"1. Position Sizing: Limit exposure to <5% of portfolio per trade\")\n",
    "print(\"2. Early Exercise Consideration: Monitor optimal stopping boundaries\")\n",
    "print(\"3. Hedging: Consider delta and vega hedging for larger positions\")\n",
    "print(\"4. Model Risk: Be aware model assumes H={:.2f}, may differ from market\".format(H))\n",
    "\n",
    "# Additional practical advice\n",
    "print(\"\\nPractical Implementation:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "if moneyness < 0.95:\n",
    "    print(\" Deep ITM option: Consider early exercise if dividend yield > interest rate\")\n",
    "    print(\" Watch for significant changes in volatility that could shift optimal exercise boundary\")\n",
    "elif moneyness > 1.05:\n",
    "    print(\" OTM option: Early exercise unlikely, trade like European option\")\n",
    "    print(\" Primary value is in insurance against downside moves\")\n",
    "else:\n",
    "    print(\" ATM option: Maximum gamma/vega exposure\")\n",
    "    print(\" Most sensitive to changes in volatility and rough volatility parameters\")\n",
    "    print(\" Actively monitor for optimal early exercise conditions near expiration\")\n",
    "\n",
    "print(\"\\nNote: This model incorporates rough volatility effects (H={:.2f}) which\".format(H))\n",
    "print(\"traditional models like Black-Scholes miss. This can be particularly\")\n",
    "print(\"important for managing risk in volatile market conditions.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
