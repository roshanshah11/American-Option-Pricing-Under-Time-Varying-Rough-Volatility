{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eccc9cab",
   "metadata": {
    "id": "eccc9cab"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right, #6a11cb, #2575fc); padding: 20px; border-radius: 10px; color: white; text-align: center; margin-bottom: 30px; box-shadow: 0 4px 6px rgba(0,0,0,0.1);\">\n",
    "<h1 style=\"margin: 0; text-shadow: 2px 2px 4px rgba(0,0,0,0.2);\">Pricing American Options in Rough Bergomi</h1>\n",
    "<p style=\"margin-top: 10px; font-size: 18px; opacity: 0.9;\">Using Linear and Deep Signature Methods</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ckOdm_OAGE",
   "metadata": {
    "id": "c9ckOdm_OAGE"
   },
   "source": [
    "<div style=\"background-color: #f8f9fa; padding: 15px; border-left: 5px solid #2575fc; margin: 10px 0;\">\n",
    "<p>In this notebook we show how to use the code from <a href=\"https://github.com/lucapelizzari/Optimal_Stopping_with_signatures/tree/main\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">this GitHub repository</a>, to compute lower and upper bounds for American options in the rough Bergomi model using different signature methods, see for example Section 4.2 of <a href=\"https://arxiv.org/abs/2312.03444\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">this paper</a> for the linear approach, whereas the deep neural network approaches will be discussed in a forthcoming paper.</p>\n",
    "\n",
    "<p>The repository consists of:</p>\n",
    "\n",
    "<ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> Simulation packages for fractional Brownian motion, rough Bergomi and rough Heston models</li>\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> A modul for signature related computations <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">Signature_computer.py</code>, which can compute the signature and log-signature of various lifts related to volatility modelling, with the additional option of adding polynomials of the state-process and/or volatility.</li>\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> The main module for the linear signature approaches <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">Linear_signature_optimal_stopping.py</code>, which can be used to derive lower and upper bounds to the optimal stopping problem applying the approaches described in <a href=\"https://arxiv.org/abs/2312.03444\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">the paper</a>.</li>\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> The main module for deep log-signature approaches <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">Deep_signature_optimal_stopping.py</code>, which extends the linear approaches by applying deep neural networks on the log-signature. This code is accompanying a working paper on \"American option pricing using signatures\".</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78183f8f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #4472C4; color: white; padding: 20px; border-radius: 10px;\">\n",
    "<h1 style=\"text-align: center; color: white;\"> Parameter Estimation From Options Data</h1>\n",
    "<p style=\"text-align: center; font-style: italic; font-size: 16px;\">A step-by-step guide to estimate rough volatility model parameters from options data</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<p style=\"text-align: center; margin-top: 15px;\">Let's dive in! </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38f842",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "<div style=\"border-left: 5px solid #4472C4; padding-left: 10px;\">\n",
    "First, we'll import the necessary Python libraries for data handling, numerical computations, and file operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f371da7",
   "metadata": {},
   "source": [
    "## 2. Initializing Global Parameters\n",
    "<div style=\"background-color: #F2F9FF; padding: 15px; border-radius: 5px; border-left: 5px solid #5B9BD5;\">\n",
    "Here we set up placeholders for our model parameters. These will be populated later in the notebook based on our analysis.\n",
    "\n",
    "<table style=\"width:100%; border-collapse: collapse; margin-top:10px;\">\n",
    "  <tr style=\"background-color: #5B9BD5; color: white;\">\n",
    "    <th style=\"padding: 8px; text-align: left;\">Parameter</th>\n",
    "    <th style=\"padding: 8px; text-align: left;\">Description</th>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #EAF2FA;\">\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">N1, N, T</td>\n",
    "    <td style=\"padding: 8px;\">Time discretization parameters</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">M, M2</td>\n",
    "    <td style=\"padding: 8px;\">Monte Carlo simulation parameters</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #EAF2FA;\">\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">eta</td>\n",
    "    <td style=\"padding: 8px;\">Volatility of volatility parameter</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">X0</td>\n",
    "    <td style=\"padding: 8px;\">Initial asset price (forward price)</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #EAF2FA;\">\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">r</td>\n",
    "    <td style=\"padding: 8px;\">Risk-free interest rate</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">rho</td>\n",
    "    <td style=\"padding: 8px;\">Correlation between asset returns and volatility</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #EAF2FA;\">\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">xi</td>\n",
    "    <td style=\"padding: 8px;\">Initial variance value</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">strike</td>\n",
    "    <td style=\"padding: 8px;\">Option strike price</td>\n",
    "  </tr>\n",
    "  <tr style=\"background-color: #EAF2FA;\">\n",
    "    <td style=\"padding: 8px; font-family: monospace;\">K</td>\n",
    "    <td style=\"padding: 8px;\">Model calibration parameter</td>\n",
    "  </tr>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "### 2.1 The Payoff Function\n",
    "<div style=\"background-color: #F8F8F8; padding: 12px; border-left: 4px solid #70AD47; margin: 10px 0;\">\n",
    "For put options, the payoff function determines the final value of the option at expiration. It returns the maximum of (strike price - asset price) or zero.\n",
    "\n",
    "<div style=\"text-align: center; margin-top: 15px;\">\n",
    "$\\text{Payoff} = \\max(K - S_T, 0)$\n",
    "</div>\n",
    "\n",
    "<p style=\"text-align: center; font-style: italic; margin-top: 5px; color: #555;\">where $K$ is the strike price and $S_T$ is the asset price at expiration</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global option model parameters (pre-initialized)\n",
    "N1 = 0\n",
    "N = 0\n",
    "T = 0\n",
    "T_years = 0.0\n",
    "M = 0\n",
    "M2 = 0\n",
    "eta = 0.0\n",
    "X0 = 0.0\n",
    "r = 0.0\n",
    "rho = 0.0\n",
    "xi = 0.0\n",
    "strike = 0.0\n",
    "K = 0\n",
    "Hurst_list = []\n",
    "\n",
    "def phi(x):\n",
    "    return np.maximum(strike - x, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b91dee",
   "metadata": {},
   "source": [
    "Paths/ Loading_Cleaning the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2de602",
   "metadata": {},
   "source": [
    "## 3. Setting Up File Paths / Data Loading Function\n",
    "<div style=\"background-color: #FFF4E1; padding: 10px; border-radius: 5px; border-left: 5px solid #FF9800;\">\n",
    "Now we define the file paths for our data. We use Path from the pathlib library to ensure cross-platform compatibility.\n",
    "</div>\n",
    "<div style=\"background-color:#F3E5F5; width:95%; padding:15px; border-radius:10px; border-left:5px solid #7B1FA2\">\n",
    "This function handles loading the price dataset. It checks if a cached version exists (for faster loading) and otherwise loads from CSV.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60aad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "repo_root = os.path.abspath('..')\n",
    "notebook_dir = Path(repo_root) / 'XGboost_Roshan' / 'src'\n",
    "DATA_PICKLE = notebook_dir / 'data' / 'dataset1.pkl'\n",
    "RETURNS_PICKLE = notebook_dir / 'data' / 'dataset3.pkl'\n",
    "\n",
    "def load_and_clean_data(pickle_path):\n",
    "    if os.path.exists(pickle_path):\n",
    "        logger.info(f\"Loading data from {pickle_path}\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "    else:\n",
    "        logger.warning(f\"Pickle file not found at {pickle_path}\")\n",
    "        os.makedirs(pickle_path.parent, exist_ok=True)\n",
    "        csv_path = pickle_path.with_suffix('.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            logger.info(f\"Found CSV at {csv_path}. Converting to pickle.\")\n",
    "            df = pd.read_csv(csv_path, parse_dates=['date'], dayfirst=False)\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Converted and saved as pickle: {pickle_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"CSV file also not found at {csv_path}\")\n",
    "            return None\n",
    "\n",
    "    if 'secid' in df.columns or 'index_flag' in df.columns:\n",
    "        df = df.drop(columns=['secid', 'index_flag'], errors='ignore')\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    logger.info(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns from {pickle_path.name}\")\n",
    "    return df\n",
    "\n",
    "def select_data_by_date_and_days(df, target_date=None, days_length=None):\n",
    "    filtered_df = df.copy()\n",
    "    if target_date:\n",
    "        filtered_df = filtered_df[filtered_df['date'] == pd.to_datetime(target_date)]\n",
    "    if days_length is not None:\n",
    "        filtered_df = filtered_df[filtered_df['days'] == days_length]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def extract_model_inputs(option_df):\n",
    "    put_df = option_df[option_df['cp_flag'] == 'P']\n",
    "    if put_df.empty:\n",
    "        return None\n",
    "    row = put_df.iloc[0]\n",
    "    return {\n",
    "        \"ticker\": row['ticker'],\n",
    "        \"date\": row['date'],\n",
    "        \"T_years\": row['days'] / 252,\n",
    "        \"strike\": row['strike_price'],\n",
    "        \"X0 (forward)\": row['forward_price'],\n",
    "        \"market_premium\": row['premium']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b86736",
   "metadata": {},
   "source": [
    "## 4. Parameter Estimation Functions\n",
    "\n",
    "> Now we'll define functions to estimate two critical model parameters for rough volatility models:\n",
    "\n",
    "---\n",
    "\n",
    "### ρ (rho)\n",
    "- **Meaning**: Correlation between returns and volatility  \n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\rho = \\text{Corr}(r_t, \\Delta \\sigma_t)\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### η (eta)\n",
    "- **Meaning**: Volatility of volatility  \n",
    "- **Formula**:  \n",
    "  $$\n",
    "  \\eta = \\frac{\\text{std}(\\Delta \\log \\sigma_t)}{(\\Delta t)^H}\n",
    "  $$\n",
    "\n",
    "These parameters are crucial for capturing the dynamics of financial markets accurately.  \n",
    "We also use a **rolling Hurst exponent \\( H \\)** in the denominator of η to reflect local path roughness. Which we got from <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">train_hurst.py</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87378d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rho(returns_df, options_df, target_ticker, target_date, days_length):\n",
    "    ret_df = returns_df[(returns_df['ticker'] == target_ticker) & (returns_df['date'] <= target_date)].copy()\n",
    "    opt_df = options_df[(options_df['ticker'] == target_ticker) & \n",
    "                        (options_df['cp_flag'] == 'P') & \n",
    "                        (options_df['days'] == days_length) & \n",
    "                        (options_df['date'] <= target_date)].copy()\n",
    "\n",
    "    ret_df = ret_df.sort_values('date')\n",
    "    opt_df = opt_df.sort_values('date').drop_duplicates(subset=['date'])\n",
    "\n",
    "    merged = pd.merge(ret_df, opt_df[['date', 'impl_volatility']], on='date', how='inner')\n",
    "    merged['vol_change'] = merged['impl_volatility'].diff()\n",
    "    merged = merged.dropna(subset=['return', 'vol_change'])\n",
    "\n",
    "    if len(merged) < 2:\n",
    "        return None\n",
    "    return merged['return'].corr(merged['vol_change'])\n",
    "\n",
    "def compute_rolling_hurst(returns: np.ndarray, power: int) -> np.ndarray:\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        returns = np.array(returns)\n",
    "    n = 2**power\n",
    "    if len(returns) < n:\n",
    "        raise ValueError(f\"Need at least {n} data points for power={power}\")\n",
    "    hursts = []\n",
    "    exponents = np.arange(2, power+1)\n",
    "    for t in range(n, len(returns) + 1):\n",
    "        window = returns[t-n:t]\n",
    "        rs_log = []\n",
    "        for exp in exponents:\n",
    "            m = 2**exp\n",
    "            s = n // m\n",
    "            segments = window.reshape(s, m)\n",
    "            dev = np.cumsum(segments - segments.mean(axis=1, keepdims=True), axis=1)\n",
    "            R = dev.max(axis=1) - dev.min(axis=1)\n",
    "            S = segments.std(axis=1)\n",
    "            rs = np.where(S != 0, R/S, 0)\n",
    "            rs_log.append(np.log2(rs.mean()))\n",
    "        hursts.append(np.polyfit(exponents, rs_log, 1)[0])\n",
    "    return np.array(hursts)\n",
    "\n",
    "def estimate_eta_and_xi(option_df, hurst):\n",
    "    option_df = option_df.sort_values('date').drop_duplicates(subset=['date'])\n",
    "    option_df['log_vol'] = np.log(option_df['impl_volatility'])\n",
    "    option_df['log_vol_diff'] = option_df['log_vol'].diff()\n",
    "    std_log_vol_diff = option_df['log_vol_diff'].dropna().std()\n",
    "    delta_t = 1 / 252\n",
    "    eta = std_log_vol_diff / (delta_t ** hurst)\n",
    "    xi = option_df['impl_volatility'].iloc[0] ** 2 if not option_df['impl_volatility'].isnull().all() else 0.0\n",
    "    return eta, xi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efcbfa",
   "metadata": {},
   "source": [
    "## 7. Main Analysis Process\n",
    "<div style=\"background-color: #E8EAF6; padding: 15px; border-radius: 8px; border-left: 5px solid #3F51B5;\">\n",
    "Now let's define our main analysis function that brings everything together. This function will:\n",
    "\n",
    "<ol style=\"color: #333; margin-top: 10px;\">\n",
    "  <li>Filter the option data by ticker, date, and days to maturity</li>\n",
    "  <li>Extract model inputs from the filtered put options</li>\n",
    "  <li>Estimate the correlation (ρ) between returns and volatility</li>\n",
    "  <li>Compute the Hurst exponent (H) for the roughness of volatility</li>\n",
    "  <li>Estimate volatility parameters (η and ξ)</li>\n",
    "  <li>Set up all required model configuration parameters</li>\n",
    "</ol>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = load_and_clean_data(DATA_PICKLE)\n",
    "    returns_df = load_and_clean_data(RETURNS_PICKLE)\n",
    "\n",
    "    if df is not None and returns_df is not None:\n",
    "        date_input = input(\"Enter date (YYYY-MM-DD) [default: 2023-08-31]: \").strip()\n",
    "        target_date = pd.to_datetime(date_input if date_input else \"2023-08-31\")\n",
    "\n",
    "        days_input = input(\"Enter option length in days [default: 10]: \").strip()\n",
    "        try:\n",
    "            days_length = int(days_input) if days_input else 10\n",
    "        except ValueError:\n",
    "            logger.warning(\"Invalid input for days. Defaulting to 10.\")\n",
    "            days_length = 10\n",
    "\n",
    "        ticker_input = input(\"Enter ticker [default: AAPL]: \").strip().upper()\n",
    "        ticker = ticker_input if ticker_input else \"AAPL\"\n",
    "\n",
    "        filtered_data = select_data_by_date_and_days(df, target_date, days_length)\n",
    "        ticker_data = filtered_data[filtered_data['ticker'] == ticker]\n",
    "\n",
    "        if len(ticker_data) > 0:\n",
    "            print(f\"\\nFiltered data for {ticker} on {target_date.date()} with {days_length} days:\")\n",
    "            print(ticker_data)\n",
    "            model_inputs = extract_model_inputs(ticker_data)\n",
    "            if model_inputs:\n",
    "                print(\"\\n Extracted Model Inputs (PUTS):\")\n",
    "                for k, v in model_inputs.items():\n",
    "                    print(f\"{k}: {v}\")\n",
    "\n",
    "                rho_est = round(estimate_rho(returns_df, df, ticker, target_date, days_length), 2)\n",
    "                print(f\"Estimated rho: {rho_est}\" if rho_est is not None else \"Rho estimation failed.\")\n",
    "                rho = rho_est if rho_est is not None else -0.9\n",
    "\n",
    "                rets = returns_df[(returns_df['ticker'] == ticker) & (returns_df['date'] <= target_date)]\n",
    "                rets = rets.sort_values('date')['return'].values\n",
    "                hurst_values = compute_rolling_hurst(rets, power=5)\n",
    "                hurst_dates = returns_df[(returns_df['ticker'] == ticker) & (returns_df['date'] <= target_date)].sort_values('date').iloc[2**5 - 1:]['date'].values\n",
    "                hurst_df = pd.DataFrame({'date': hurst_dates, 'H': hurst_values})\n",
    "                H_df = hurst_df[hurst_df['date'] <= target_date]\n",
    "                latest_H = H_df.iloc[-1]['H'] if not H_df.empty else 0.1\n",
    "                print(f\"Using Hurst: {round(latest_H, 3)}\")\n",
    "\n",
    "                iv_path_df = df[\n",
    "                    (df['ticker'] == ticker) &\n",
    "                    (df['cp_flag'] == 'P') &\n",
    "                    (df['days'] == days_length) &\n",
    "                    (df['date'] <= target_date)\n",
    "                ].sort_values('date')\n",
    "\n",
    "                eta, xi = estimate_eta_and_xi(iv_path_df, hurst=latest_H)\n",
    "                eta = round(eta, 3)\n",
    "                xi = round(xi, 5)\n",
    "\n",
    "                N = 252\n",
    "                M = 2**13\n",
    "                M2 = 2**13\n",
    "                \n",
    "                r = 0.05\n",
    "                K = 2\n",
    "\n",
    "                T = int(model_inputs[\"T_years\"] * 252)\n",
    "                N1 = T\n",
    "                premium = model_inputs[\"market_premium\"]\n",
    "                T_years = model_inputs[\"T_years\"]\n",
    "                X0 = model_inputs[\"X0 (forwa\" \\\n",
    "                \"rd)\"]\n",
    "                strike = model_inputs[\"strike\"]\n",
    "\n",
    "                print(\"\\n✅ Global parameters set:\")\n",
    "                print(f\"N1 = {N1}, N = {N}, T = {T}, T_years = {T_years:.5f}\")\n",
    "                print(f\"M = {M}, M2 = {M2}, eta = {eta}, X0 = {X0}, strike = {strike}\")\n",
    "                print(f\"r = {r}, rho = {rho}, xi = {xi}, K = {K}\")\n",
    "            else:\n",
    "                logger.warning(\"No put option found for this filter.\")\n",
    "        else:\n",
    "            logger.warning(\"No data matches your filter criteria.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e039383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n1 = {N1}, n = {N}, T = {T}, T_years = {T_years:.5f}, M = {M}, M2 = {M2}, eta = {eta:}, X0 = {X0:.5f}, strike = {strike:.5f}, r = {r:.5f}, rho = {rho:.5f}, xi = {xi:.5f}, K = {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for TensorFlow import issues and environment compatibility\n",
    "# Set correct Python path to find modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory and subdirectories to Python path\n",
    "repo_root = os.path.abspath('..')\n",
    "sys.path.append(repo_root)\n",
    "sys.path.append(os.path.join(repo_root, \"Linear signature optimal stopping\"))\n",
    "sys.path.append(os.path.join(repo_root, \"Non linear signature optimal stopping\"))\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ccd27",
   "metadata": {},
   "source": [
    "## Now I will Estimating my Hurst Parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925868f8",
   "metadata": {},
   "source": [
    "# <span style=\"color:#1E88E5\">XGBoost for Hurst Exponent Forecasting (over multiple days) </span> - Step by Step Tutorial\n",
    "\n",
    "This notebook provides a detailed walkthrough of `train_hurst.py`, a script that computes the rolling Hurst exponent of a time series and trains/evaluates an XGBoost model to forecast its future values.\n",
    "\n",
    "## <span style=\"color:#43A047\">What is the Hurst Exponent?</span>\n",
    "\n",
    "<div style=\"background-color:#EFF8FB; max-width: 700px; padding:15px; border-radius:10px; border-left:5px solid #4682B4\">\n",
    "The Hurst exponent is a measure used in time series analysis that quantifies the long-term memory of a series. It helps determine if a time series is:\n",
    "<ul>\n",
    "  <li><span style=\"color:#F44336\"><b>H < 0.5</b></span>: Anti-persistent (mean-reverting)</li>\n",
    "  <li><span style=\"color:#9E9E9E\"><b>H = 0.5</b></span>: Random walk (no memory)</li>\n",
    "  <li><span style=\"color:#4CAF50\"><b>H > 0.5</b></span>: Trend-reinforcing (persistent)</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "## <span style=\"color:#43A047\">What This Notebook Covers</span>\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 10px; margin-top: 10px\">\n",
    "  <div style=\"flex: 1; min-width: 100px; background-color:#E3F2FD; padding:10px; border-radius:5px\">📚 <b>1.</b> Importing libraries</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#E8F5E9; padding:10px; border-radius:5px\">📈 <b>2.</b> Computing rolling Hurst</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#FFF8E1; padding:10px; border-radius:5px\">💾 <b>3.</b> Loading financial data</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#F3E5F5; padding:10px; border-radius:5px\">⚙️ <b>4.</b> Creating lagged features</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#E0F7FA; padding:10px; border-radius:5px\">🧠 <b>5.</b> Training XGBoost model</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#FFEBEE; padding:10px; border-radius:5px\">🔍 <b>6.</b> Evaluating performance</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#F1F8E9; padding:10px; border-radius:5px\">🔮 <b>7.</b> Forecasting future values</div>\n",
    "  <div style=\"flex: 1; min-width: 150px; background-color:#E8EAF6; padding:10px; border-radius:5px\">🖥️ <b>8.</b> Interactive menu system</div>\n",
    "</div>\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for numerical operations, data manipulation, and visualization\n",
    "import numpy as np                # For numerical computations (arrays, math functions)\n",
    "import pandas as pd               # For data manipulation with DataFrames\n",
    "import matplotlib.pyplot as plt   # For creating plots and visualizations\n",
    "from pathlib import Path          # For handling file paths in a platform-independent way\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # For model evaluation metrics\n",
    "import logging                    # For logging status and debug messages\n",
    "import joblib                     # For saving/loading Python objects (like our trained models)\n",
    "from xgboost import XGBRegressor  # XGBoost regression model implementation\n",
    "\n",
    "# Set up basic configuration\n",
    "# How many days ahead to predict (forecast horizon)\n",
    "HORIZON = T\n",
    "\n",
    "# Configure logging to show informational messages with timestamps\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Show INFO level messages and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Include timestamp and message level\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directory for saving trained models\n",
    "MODELS_DIR = notebook_dir / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "print(f\"Models will be stored in: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97766b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#FF8F00\">2. Computing the Rolling Hurst Exponent</span>\n",
    "<hr style=\"height:1px;border:none;background-color:#FF8F00\">\n",
    "\n",
    "This function calculates the Hurst exponent using Rescaled Range (R/S) analysis over a rolling window. The Hurst exponent measures the long-term memory or persistence of a time series.\n",
    "\n",
    "<div style=\"background-color:#FFF8E1; max-width: 700px; padding:15px; border-radius:10px; border-left:5px solid #FFB300\">\n",
    "<span style=\"font-size:1.1em\">🔑 <b>Key Concepts:</b></span>\n",
    "<ul>\n",
    "  <li><b>Window size</b>: 2<sup>power</sup> determines how many data points we use for each calculation</li>\n",
    "  <li><b>R/S Analysis</b>: Calculates the ratio of the range to the standard deviation at different time scales</li>\n",
    "  <li><b>Slope of log-log plot</b>: The slope of the line relating log(R/S) to log(time scale) gives us the Hurst exponent</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_hurst(returns: np.ndarray, power: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Hurst exponent using R/S analysis over a rolling window of size 2^power.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    returns : np.ndarray\n",
    "        Time series data (e.g., asset returns) to analyze\n",
    "    power : int\n",
    "        Power of 2 to determine window size (window size = 2^power)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Array of Hurst exponents calculated over rolling windows\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it's not already\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        returns = np.array(returns)\n",
    "        \n",
    "    # Validate the power parameter\n",
    "    if not isinstance(power, int) or power < 1:\n",
    "        raise ValueError(\"power must be a positive integer\")\n",
    "        \n",
    "    # Calculate window length as 2^power\n",
    "    n = 2**power\n",
    "    \n",
    "    # Check if we have enough data\n",
    "    if len(returns) < n:\n",
    "        raise ValueError(f\"Need at least {n} data points for power={power}\")\n",
    "        \n",
    "    # Initialize list to store Hurst exponents\n",
    "    hursts = []\n",
    "    \n",
    "    # Define the range of exponents for multi-scale analysis\n",
    "    # (from 2 to power, these will be our different time scales)\n",
    "    exponents = np.arange(2, power+1)\n",
    "    \n",
    "    # Roll the window through the data\n",
    "    for t in range(n, len(returns) + 1):\n",
    "        # Get current window of data\n",
    "        window = returns[t-n:t]\n",
    "        \n",
    "        # Store log values of R/S at different scales\n",
    "        rs_log = []\n",
    "        \n",
    "        # Calculate R/S at different time scales\n",
    "        for exp in exponents:\n",
    "            # Size of each segment at this scale\n",
    "            m = 2**exp\n",
    "            \n",
    "            # Number of segments\n",
    "            s = n // m\n",
    "            \n",
    "            # Reshape data into segments\n",
    "            segments = window.reshape(s, m)\n",
    "            \n",
    "            # Calculate cumulative deviation from mean for each segment\n",
    "            dev = np.cumsum(\n",
    "                segments - segments.mean(axis=1, keepdims=True),\n",
    "                axis=1\n",
    "            )\n",
    "            \n",
    "            # Range (R) is max deviation minus min deviation\n",
    "            R = dev.max(axis=1) - dev.min(axis=1)\n",
    "            \n",
    "            # Standard deviation (S) of each segment\n",
    "            S = segments.std(axis=1)\n",
    "            \n",
    "            # Calculate R/S ratio (avoid division by zero)\n",
    "            rs = np.where(S != 0, R/S, 0)\n",
    "            \n",
    "            # Store log2 of mean R/S value\n",
    "            rs_log.append(np.log2(rs.mean()))\n",
    "        \n",
    "        # Fit a line to log(R/S) vs log(time scale)\n",
    "        # The slope of this line is the Hurst exponent\n",
    "        hursts.append(np.polyfit(exponents, rs_log, 1)[0])\n",
    "        \n",
    "    return np.array(hursts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the price dataset from CSV or cache, sort by date,\n",
    "    and return a pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing price data sorted by date\n",
    "    \"\"\"\n",
    "    # First, check if a cached (pickled) version of the data exists\n",
    "    if RETURNS_PICKLE.exists():\n",
    "        # Load from the pickle file (much faster than CSV)\n",
    "        df = pd.read_pickle(RETURNS_PICKLE)\n",
    "        logger.info(f\"Loaded data from cache: {RETURNS_PICKLE}\")\n",
    "    else:\n",
    "        # Sort the data by date\n",
    "        df.sort_values('date', inplace=True)\n",
    "        \n",
    "        # Cache the data for faster future loading\n",
    "        df.to_pickle(RETURNS_PICKLE)\n",
    "        logger.info(f\"Saved data to cache: {RETURNS_PICKLE}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea4ace",
   "metadata": {},
   "source": [
    "## <span style=\"color:#26A69A\">4. Creating Lagged Features for Time Series Forecasting</span>\n",
    "<hr style=\"height:1px;border:none;background-color:#26A69A\">\n",
    "\n",
    "This function creates lagged features for time series forecasting. Lagged features are simply past values of the time series that are used to predict future values.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin: 20px 0;\">\n",
    "  <div style=\"background-color:#E0F2F1; padding:15px; border-radius:10px; width:90%\">\n",
    "    <span style=\"color:#00695C; font-weight:bold\">Key Concepts:</span>\n",
    "    <table style=\"width:100%; border-collapse: collapse; margin-top: 10px\">\n",
    "      <tr style=\"background-color:#B2DFDB\">\n",
    "        <th style=\"padding:8px; text-align:left\">Concept</th>\n",
    "        <th style=\"padding:8px; text-align:left\">Description</th>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"padding:8px; border-bottom:1px solid #ddd\"><b>Lag</b></td>\n",
    "        <td style=\"padding:8px; border-bottom:1px solid #ddd\">Using past values (t-1, t-2, ..., t-k) to predict the current value (t)</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"padding:8px; border-bottom:1px solid #ddd\"><b>Feature Matrix X</b></td>\n",
    "        <td style=\"padding:8px; border-bottom:1px solid #ddd\">Each row contains k consecutive past values</td>\n",
    "      </tr>\n",
    "      <tr>\n",
    "        <td style=\"padding:8px\"><b>Target Vector y</b></td>\n",
    "        <td style=\"padding:8px\">Contains the value we want to predict (the value at time t)</td>\n",
    "      </tr>\n",
    "    </table>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagged_features(series: pd.Series, k: int):\n",
    "    \"\"\"\n",
    "    Generate lagged features matrix X and target vector y from a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series data to create lagged features from\n",
    "    k : int\n",
    "        Number of lagged values to use as features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    (np.ndarray, np.ndarray)\n",
    "        X: Feature matrix where each row contains k consecutive past values\n",
    "        y: Target vector containing the values to predict\n",
    "    \"\"\"\n",
    "    # Convert series to a numpy array for faster processing\n",
    "    vals = series.values\n",
    "    \n",
    "    # Initialize empty lists for features and targets\n",
    "    X, y = [], []\n",
    "    \n",
    "    # For each point in the time series (starting from position k)\n",
    "    for i in range(k, len(vals)):\n",
    "        # Add the k previous values as features\n",
    "        X.append(vals[i-k:i])\n",
    "        # Add the current value as the target\n",
    "        y.append(vals[i])\n",
    "    \n",
    "    # Convert lists to numpy arrays and return\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61de46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgb_only(hurst_series: pd.Series, train_frac: float, k: int, xgb_params: dict):\n",
    "    \"\"\"\n",
    "    Train a pure XGBoost model on lagged Hurst series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurst_series : pd.Series\n",
    "        Series of Hurst exponent values\n",
    "    train_frac : float\n",
    "        Fraction of data to use for training (0 to 1)\n",
    "    k : int\n",
    "        Number of lagged values to use as features\n",
    "    xgb_params : dict\n",
    "        Parameters for the XGBoost model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    XGBRegressor\n",
    "        Trained XGBoost model\n",
    "    \"\"\"\n",
    "    # Split data into training and test sets based on time\n",
    "    # (first train_frac portion for training)\n",
    "    split = int(len(hurst_series) * train_frac)\n",
    "    train = hurst_series.iloc[:split]\n",
    "    \n",
    "    # Create lagged features for training data\n",
    "    X_tr, y_tr = make_lagged_features(train, k)\n",
    "    \n",
    "    # Initialize and train the XGBoost model\n",
    "    xgb_only = XGBRegressor(**xgb_params)\n",
    "    xgb_only.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Save the trained model to disk for later use\n",
    "    joblib.dump(xgb_only, MODELS_DIR / 'xgb_only.pkl')\n",
    "    logger.info(\"Pure XGBoost model saved to %s\", MODELS_DIR)\n",
    "    \n",
    "    return xgb_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad985374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_xgb_only(hurst_series: pd.Series, xgb_only, k: int, test_frac: float):\n",
    "    \"\"\"\n",
    "    Evaluate pure XGBoost model on test split and plot results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurst_series : pd.Series\n",
    "        Series of Hurst exponent values\n",
    "    xgb_only : XGBRegressor\n",
    "        Trained XGBoost model\n",
    "    k : int\n",
    "        Number of lagged values used as features\n",
    "    test_frac : float\n",
    "        Fraction of data to use for testing (0 to 1)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    (float, float)\n",
    "        MSE and MAE on the test set\n",
    "    \"\"\"\n",
    "    # Split data into training and test sets based on time\n",
    "    # (last test_frac portion for testing)\n",
    "    split = int(len(hurst_series) * (1 - test_frac))\n",
    "    test = hurst_series.iloc[split:]\n",
    "    \n",
    "    # Create lagged features for test data\n",
    "    X_te, y_te = make_lagged_features(test, k)\n",
    "    \n",
    "    # Use the model to make predictions on test data\n",
    "    y_pred = xgb_only.predict(X_te)\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    mse = mean_squared_error(y_te, y_pred)\n",
    "    mae = mean_absolute_error(y_te, y_pred)\n",
    "    \n",
    "    # Log the results\n",
    "    logger.info(\"XGBoost-only results – MSE: %.4f, MAE: %.4f\", mse, mae)\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(12,6))\n",
    "    \n",
    "    # Get dates for the test set (need to skip the first k points because of lagging)\n",
    "    dates_te = test.index[k:]\n",
    "    \n",
    "    # Plot actual values\n",
    "    plt.plot(dates_te, y_te, 'k-', label='Actual Hurst')\n",
    "    \n",
    "    # Plot predicted values\n",
    "    plt.plot(dates_te, y_pred, 'g--', label='XGB-only Forecast')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('XGBoost-only: Actual vs Forecast Hurst Exponent')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Hurst Exponent')\n",
    "    \n",
    "    # Add legend, grid, and format plot\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_xgb_only(hurst_series: pd.Series, xgb_only, periods: int, k: int):\n",
    "    \"\"\"\n",
    "    Generate future forecasts using pure XGBoost model recursively.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    hurst_series : pd.Series\n",
    "        Series of Hurst exponent values\n",
    "    xgb_only : XGBRegressor\n",
    "        Trained XGBoost model\n",
    "    periods : int\n",
    "        Number of future periods to forecast\n",
    "    k : int\n",
    "        Number of lagged values used as features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with dates and forecasted values\n",
    "    \"\"\"\n",
    "    # Create a copy of historical values to avoid modifying the original\n",
    "    hist = list(hurst_series.values)\n",
    "    \n",
    "    # Initialize list to store predictions\n",
    "    preds = []\n",
    "    \n",
    "    # Generate predictions for each future period\n",
    "    for _ in range(periods):\n",
    "        # If we don't have enough historical values yet, default to 0\n",
    "        if len(hist) < k:\n",
    "            p = 0\n",
    "        else:\n",
    "            # Use the model to predict the next value\n",
    "            p = xgb_only.predict(np.array(hist[-k:]).reshape(1, -1))[0]\n",
    "        \n",
    "        # Store the prediction\n",
    "        preds.append(p)\n",
    "        \n",
    "        # Add the prediction to historical values for recursive forecasting\n",
    "        hist.append(p)\n",
    "    \n",
    "    # Generate future dates starting from the day after the last historical date\n",
    "    # 'B' frequency means business days (Monday to Friday)\n",
    "    dates = pd.date_range(\n",
    "        hurst_series.index[-1] + pd.Timedelta(days=1), \n",
    "        periods=periods, \n",
    "        freq='B'\n",
    "    )\n",
    "    \n",
    "    # Return DataFrame with dates and forecasts\n",
    "    return pd.DataFrame({'ds': dates, 'yhat': np.array(preds)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hurst():\n",
    "    \"\"\"\n",
    "    Main entry point: show menu, get user choice, compute Hurst series,\n",
    "    and train/evaluate/forecast the XGBoost model as requested.\n",
    "    \"\"\"\n",
    "\n",
    "    # Log start of execution\n",
    "    logger.info(\"Starting main execution...\")\n",
    "    # for the sake of this example, we will use a fixed mode\n",
    "    power = 5\n",
    "    train_frac, test_frac = 0.8, 0.2\n",
    "    k = 5\n",
    "    xgb_params = {'n_estimators': 100, 'learning_rate': 0.1}\n",
    "    periods = HORIZON\n",
    "\n",
    "    # Load data and filter by ticker symbol\n",
    "    df_all = load_data()\n",
    "    df_t = df_all[df_all['ticker'] == ticker].sort_values('date')\n",
    "    returns = df_t['return'].values\n",
    "\n",
    "    # Compute rolling Hurst exponent series\n",
    "    hurst_vals = compute_rolling_hurst(returns, power)\n",
    "    dates = df_t['date']\n",
    "    start_index = 2**power - 1\n",
    "    hurst_series = pd.Series(hurst_vals, index=dates[start_index:])\n",
    "\n",
    "    #it will first train the model, then evaluate, and finally forcase\n",
    "    train_xgb_only(hurst_series, train_frac, k, xgb_params)\n",
    "    xgb_only = joblib.load(MODELS_DIR / 'xgb_only.pkl')\n",
    "    evaluate_xgb_only(hurst_series, xgb_only, k, test_frac)\n",
    "    xgb_only = joblib.load(MODELS_DIR / 'xgb_only.pkl')\n",
    "    df_fc = forecast_xgb_only(hurst_series, xgb_only, periods, k)\n",
    "    print(\"List of hurst parameters:\", list(df_fc['yhat']))\n",
    "    logger.info(\"XGBoost-only forecast:\\n%s\", df_fc)\n",
    "        \n",
    "        \n",
    "    # Plot historical and forecasted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(hurst_series.index, hurst_series.values, 'k-', label='Historical Hurst')\n",
    "    plt.plot(df_fc['ds'], df_fc['yhat'], 'g--', label='Forecast')\n",
    "    plt.title(f'{periods}-Day Hurst Forecast for {ticker}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Hurst Exponent')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return list(list(df_fc['yhat']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hurst_list = main_hurst()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e568a2",
   "metadata": {
    "id": "57e568a2"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right, #2575fc, #6a11cb); padding: 10px; border-radius: 8px; margin: 15px 0;\">\n",
    "<h2 style=\"color: white; margin: 0; padding: 5px;\">American Put options in the rough Bergomi model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572b688",
   "metadata": {
    "id": "6572b688"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px; border-left: 5px solid #2575fc;\">\n",
    "Recall the price and volatility dynamics of the latter are given by \n",
    "<div style=\"text-align: center; margin: 10px 0;\">\n",
    "\\begin{align*}\n",
    "dX_t &= rX_tdt+X_tv_t \\left (\\rho dW_r+\\sqrt{1-\\rho^2}dB_t\\right ), \\\\ \n",
    "v_t & =\\xi_0\\mathcal{E}\\left (\\eta \\int_0^t(t-s)^{H-\\frac{1}{2}}dW_s \\right )\n",
    "\\end{align*}\n",
    "</div>\n",
    "and pricing an American Put-option can be formulated as optimal stopping problem \n",
    "<div style=\"text-align: center; margin: 10px 0;\">\n",
    "$$y_0=\\sup_{\\tau \\in \\mathcal{S}_0}\\mathbb{E}[e^{-r\\tau}\\left (K-X_{\\tau}\\right )^{+}]$$\n",
    "</div>\n",
    "for some strike $K$. In this notebook we consider the following choice of paramteres \n",
    "<div style=\"text-align: center; margin: 10px 0; font-weight: bold; color: #2575fc;\">\n",
    "$$ H=0.07,X_0 = 100, r=0.05, \\eta = 1.9, \\rho = -0.9, \\xi_0= 0.09, K = 110.$$\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xlp80cKeRW9z",
   "metadata": {
    "id": "Xlp80cKeRW9z"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right, #2575fc, #6a11cb); padding: 10px; border-radius: 8px; margin: 15px 0;\">\n",
    "<h2 style=\"color: white; margin: 0; padding: 5px;\">Step 1: Simulation rough Bergomi model</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8badb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust path to include repository root\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rBergomi_simulation import SimulationofrBergomi\n",
    "from dynamic_hurst_rbergomi import SimulationWithDynamicHurst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_dynamic_hurst(M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r):\n",
    "    \"\"\"\n",
    "    Generate simulation data with time-varying Hurst parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Similar to SimulationWithDynamicHurst, but returns formatted data ready for model training.\n",
    "    \"\"\"\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationWithDynamicHurst(\n",
    "        M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r\n",
    "    )\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r):\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationofrBergomi(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining hurst parameter... last official step to figure out\n",
    "if Hurst_list is not None:\n",
    "    H = Hurst_list\n",
    "    print(f\"Using Hurst: {H}\")\n",
    "    S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data_dynamic_hurst(\n",
    "    M, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "    )\n",
    "\n",
    "    S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data_dynamic_hurst(\n",
    "        M2, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "    )\n",
    "\n",
    "else:\n",
    "    H = 0.07  # Hurst parameter\n",
    "    # Use K in your function call, not strike\n",
    "    S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "    S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data(M2, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "\n",
    "print(f\"Hurst parameter: {H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29669ebe",
   "metadata": {
    "id": "29669ebe"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px; border-left: 5px solid #2575fc;\">\n",
    "Next we define a function generating rough Bergomi prices, volatilies and the corresponding Brownian motion and payoff process. Then we generate training and testing data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show Head of the training data\")\n",
    "print(\"S_training\", S_training[:5])\n",
    "print(\"V_training\", V_training[:5])\n",
    "print(\"Payoff_training\", Payoff_training[:5])\n",
    "print(\"dW_training\", dW_training[:5])\n",
    "print(\"I_training\", I_training[:5])\n",
    "print(\"MM_training\", MM_training[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68169f97",
   "metadata": {
    "id": "68169f97"
   },
   "outputs": [],
   "source": [
    "#compute the volatility processes\n",
    "vol_training = np.sqrt(V_training)\n",
    "vol_testing = np.sqrt(V_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LW4SLN7bQy4R",
   "metadata": {
    "id": "LW4SLN7bQy4R"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right, #2575fc, #6a11cb); padding: 10px; border-radius: 8px; margin: 15px 0;\">\n",
    "<h2 style=\"color: white; margin: 0; padding: 5px;\">Step 2: Signature computations</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NUEKCKWsRPy4",
   "metadata": {
    "id": "NUEKCKWsRPy4"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px;\">\n",
    "We will make us uf the <a href=\"https://pypi.org/project/iisignature/\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">iisignature package</a> to compute the signature, and it can be installed using pip:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14790b8",
   "metadata": {
    "id": "d14790b8"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px;\">\n",
    "We import our signature computation module, which can compute various signature and log signature lift related to the generated data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa867a0",
   "metadata": {
    "id": "1fa867a0"
   },
   "outputs": [],
   "source": [
    "from Signature_computer import SignatureComputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eb781",
   "metadata": {
    "id": "fe2eb781"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px; border-left: 5px solid #2575fc;\">\n",
    "<p>Next we initialize the <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">SignatureComputer</code>, which allows to choose from the linear and the log signature, and various choices of signature lifts. Here are some examples:</p>\n",
    "<div style=\"text-align: center; background-color: #f8f9fa; padding: 10px; margin: 10px 0; border-radius: 5px;\">\n",
    "$$ t\\mapsto \\mathrm{Sig}(A_t,X_t),t\\mapsto \\mathrm{Sig}(A_t,\\phi(X)_t),t\\mapsto \\mathrm{Sig}(A_t,X_t,X_{t-\\epsilon}),t\\mapsto \\mathrm{Sig}(A_t,X_t,\\phi(X_t)),t\\mapsto \\mathrm{Sig}(A_t,v_t),$$\n",
    "</div>\n",
    "<p>where $t\\mapsto A_t$ is a monoton path and in our examples we choose between:</p>\n",
    "<div style=\"text-align: center; font-weight: bold; color: #2575fc; margin: 10px 0;\">\n",
    "$$A_t=t, \\quad  A_t = \\langle X\\rangle_t.$$\n",
    "</div>\n",
    "<p>Additonally we can add Laguerre polynomials of $X$ or $(X,v)$ to the signature, see the module for all the details.</p>\n",
    "<p>In this example we choose the basis $(\\mathrm{Sig}(t,v_t),p_i(X_t))$, which proves to be a solid choice for rough volatility models. We choose both the polynomial and signature degree to be $3$ for this example. (To improve result one should higher truncations levels ($4-5$) for the signature, but to keep the complexity reasonable here we choose level $3$ signatures.)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a377c",
   "metadata": {
    "id": "8c4a377c"
   },
   "outputs": [],
   "source": [
    "#initialize signature computer\n",
    "sig_computer = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c365145",
    "outputId": "d26148d9-199d-4ebd-bd3f-60524818666b"
   },
   "outputs": [],
   "source": [
    "#Compute the signature for training and test data\n",
    "tt = np.linspace(0,T,N+1)\n",
    "A_training = np.zeros((M, N+1)) #time-augmentation\n",
    "A_testing = np.zeros((M2, N+1))\n",
    "A_training[:, 1:] = A_testing[:, 1:] = tt[1:]\n",
    "signatures_training = sig_computer.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_testing = sig_computer.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19711264",
   "metadata": {
    "id": "19711264"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right, #2575fc, #6a11cb); padding: 10px; border-radius: 8px; margin: 15px 0;\">\n",
    "<h2 style=\"color: white; margin: 0; padding: 5px;\">Step 3: Compute pricing intervals with linear signatures</h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180c7788",
   "metadata": {
    "id": "180c7788"
   },
   "source": [
    "<div style=\"background-color: #f0f4f8; padding: 15px; border-radius: 5px; border-left: 5px solid #2575fc;\">\n",
    "<p>We can now import the linear primal and dual pricers, which compute true lower and upper bounds.</p>\n",
    "<ul style=\"list-style-type: none; padding-left: 20px;\">\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> The <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">LinearLongstaffSchwartzPricer</code> uses the signature of the training data to recursively approximate continuation values in the spirit of the Longstaff-Schwartz algorithm (descibed in detail in Section 3.1 of <a href=\"https://arxiv.org/abs/2312.03444\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">this paper</a>). The resulting regression coefficients at each exercise date provide a stopping rule, which can be applied to the testing data to get true lower-bounds</li>\n",
    "    <li><span style=\"color: #2575fc; font-weight: bold;\">→</span> The <code style=\"background-color: #eef; padding: 2px 4px; border-radius: 3px;\">LinearDualPricer</code> uses the signature of the training data to minimize over the familiy of linear signature martingales, by solving a corresponding linear program (described in Detail in Section 3.2 of <a href=\"https://arxiv.org/abs/2312.03444\" style=\"color: #2575fc; text-decoration: none; font-weight: bold;\">this paper</a>). The resulting coefficients yield a Doob martingale approximation, which for the testing data yields a true upper bound.</li>\n",
    "</ul>\n",
    "<p>By combining the two values, we receive confidence intervals for the true option price.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eTaJQW5V8q6",
   "metadata": {
    "id": "1eTaJQW5V8q6"
   },
   "source": [
    "<div style=\"background-color: #fff4e6; padding: 12px; border-radius: 5px; border-left: 4px solid #fd7e14; margin: 10px 0;\">\n",
    "<p>To solve the linear programm, one can optionally choose to use <a href=\"https://www.gurobi.com\" style=\"color: #fd7e14; text-decoration: none; font-weight: bold;\">Gurobi</a>, which requires a free licence, which is recommended especially for high-dimensional LPs, which occur when choosing large sample-sizes and/or high signature truncations levels. Alternatively, we use the free LP solvers from CVXPY</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc73ec",
   "metadata": {
    "id": "edbc73ec"
   },
   "outputs": [],
   "source": [
    "# ────────────────────────────────────────────────────────────────────────────────\n",
    "# add root of repo and the “Linear signature optimal stopping” folder to PYTHONPATH\n",
    "import sys, os\n",
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "\n",
    "# now the module can be imported\n",
    "from Linear_signature_optimal_stopping import LinearLongstaffSchwartzPricer, LinearDualPricer\n",
    "# ────────────────────────────────────────────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a888326",
   "metadata": {
    "id": "5a888326"
   },
   "outputs": [],
   "source": [
    "\n",
    "#initialze the models\n",
    "ls_pricer = LinearLongstaffSchwartzPricer(\n",
    "        N1=N1,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        mode=\"American Option\",\n",
    "        ridge=10**(-9)\n",
    "    )\n",
    "\n",
    "dual_pricer = LinearDualPricer(\n",
    "        N1=N1,\n",
    "        N=N,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        LP_solver=\"CVXPY\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4f4d",
   "metadata": {
    "id": "28db4f4d"
   },
   "source": [
    "The choice mode=\"American Option\" indicates that the Longstaff-Schwartz recursion will only consider \"in-the-money\" paths, which was originally suggested by Longstaff & Schwartz, and is reasonable for non-negative payoffs. For general payoffs we can use mode = \"Standard\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889e9a8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0889e9a8",
    "outputId": "8afb4b10-9708-46a7-f241-da85ac65ee99"
   },
   "outputs": [],
   "source": [
    "#compute true lower bounds\n",
    "lower_bound, lower_bound_std, ls_regression_models = ls_pricer.price(\n",
    "        signatures_training,\n",
    "        Payoff_training,\n",
    "        signatures_testing,\n",
    "        Payoff_testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a34fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e64a34fa",
    "outputId": "b116657f-4c30-4379-dbfb-5aa71549133d"
   },
   "outputs": [],
   "source": [
    "print(f\"Linear Longstaff-Schwartz lower bound: {lower_bound} ± {lower_bound_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a51c2",
   "metadata": {
    "id": "676a51c2"
   },
   "source": [
    "Similarly let us derive the upper bounds, but we will train the model only for $M= 5000$ paths to reduce computation time, and then compute true prices for all testing samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba8b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08fba8b2",
    "outputId": "710b6932-eb7b-4111-c511-20de6dd42eb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_dual = 5000\n",
    "upper_bound, upper_bound_std, MG = dual_pricer.price(\n",
    "        signatures_training[:M_dual],\n",
    "        Payoff_training[:M_dual],\n",
    "        dW_training[:M_dual,:,0],  # Select only the first component of the Brownian increments\n",
    "        signatures_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # Select only the first component of the Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d591c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d591c8",
    "outputId": "e078ca47-ef36-4133-d227-c6d2b88c391d"
   },
   "outputs": [],
   "source": [
    "print(f\"Linear Dual upper bound: {upper_bound} ± {upper_bound_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(float(lower_bound),float(upper_bound))}± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab12b0",
   "metadata": {
    "id": "06ab12b0"
   },
   "source": [
    "# Improving the duality gap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hir98wJJXKE8",
   "metadata": {
    "id": "hir98wJJXKE8"
   },
   "source": [
    "Especially in rough regimes (here $H=0.1$), we observe a significant gap between lower and upper bounds, and in this section we present two ways to improve it. The first one still relies on linear signatures, but extends the basis as explained in in Section 4 of https://arxiv.org/abs/2312.03444."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ea7df",
   "metadata": {
    "id": "4d7ea7df"
   },
   "source": [
    "## Part 1: Extending the linear basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beffb042",
   "metadata": {
    "id": "beffb042"
   },
   "source": [
    "We consider a more involved basis by choosing the extended signature lift of $(t,X_t,\\phi(X_t))$, and additionally add Laguerre polynomials of $(X_t,v_t)$. We can again use the SignatureComputer to compute this extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb54f12",
   "metadata": {
    "id": "1eb54f12"
   },
   "outputs": [],
   "source": [
    "sig_computer_extended = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"payoff-and-polynomial-extended\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6249a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d6249a",
    "outputId": "53179ca8-2f48-4b44-b1eb-99f3548c0900"
   },
   "outputs": [],
   "source": [
    "signatures_extended_training = sig_computer_extended.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_extended_testing = sig_computer_extended.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zFWMoC-X-kM",
   "metadata": {
    "id": "5zFWMoC-X-kM"
   },
   "source": [
    "Now we repeat the procedure for the extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d3960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1c2d3960",
    "outputId": "009708e1-bce5-42b9-861f-3f437c5136e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#compute true lower bounds for the new basis\n",
    "lower_bound_extended, lower_bound_extended_std, ls_regression_models_extended = ls_pricer.price(\n",
    "        signatures_extended_training,\n",
    "        Payoff_training,\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing\n",
    "    )\n",
    "#Repeating the dual procedure for the new basis\n",
    "upper_bound_extended, upper_bound_extended_std, MG_extended = dual_pricer.price(\n",
    "        signatures_extended_training[:M_dual,:,:],\n",
    "        Payoff_training[:M_dual,:],\n",
    "        dW_training[:M_dual,:,0],  # select first component of Brownian increments\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # select first component of Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "692d7161",
    "outputId": "5811cd15-9104-4ec6-fb2a-31da9a47991c"
   },
   "outputs": [],
   "source": [
    "print(f\"Improve pricing interval: {(float(lower_bound_extended),float(upper_bound_extended))}± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5c7df",
   "metadata": {
    "id": "17b5c7df"
   },
   "source": [
    "## Part 2: Deep log-signature optimal stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5f596",
   "metadata": {
    "id": "b7e5f596"
   },
   "source": [
    " In forthcoming work about \"American options in rough volatility models\", we will focus on more non-linear apporaches to price American options. More precisely, we extend the primal and dual procecdure by replacing linear functionals of the signature by deep neural networks on the log-signature $\\mathbb{L}=\\mathrm{log}^\\otimes(\\mathbb{X})$. This transformed version of the signature still captures the relevant information about the past of the underlying process, but grows much slower as the signature it self with respect to the truncation. Then, in order to learn highly non-linear functionals, such as the integrand of the Doob martingale (\"derivative of the Snell-envelope\"), we apply deep feedforward neural networks $\\theta$ on the log-signature. Of course, in both methods a optimization of the hyperparameters is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTyO0IKtbYse",
   "metadata": {
    "id": "kTyO0IKtbYse"
   },
   "source": [
    "We proceed as before, but replace the linear signature by the log-signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54699ea8",
   "metadata": {
    "id": "54699ea8"
   },
   "outputs": [],
   "source": [
    "sig_computer_log = SignatureComputer(T, N, 3, \"log\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998aed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e998aed9",
    "outputId": "195d4128-74a3-4f42-a198-6310039f6c19"
   },
   "outputs": [],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use I_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ff201",
   "metadata": {
    "id": "bc7ff201"
   },
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_signatures_optimal_stopping import DeepLongstaffSchwartzPricer, DeepDualPricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use correct I_testing\n",
    ")\n",
    "print(\"shape of dW_training\", dW_training.shape)\n",
    "print(\"shape of dW_testing\", dW_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d0eff",
   "metadata": {
    "id": "9a1d0eff"
   },
   "source": [
    "The DeepLongstaffSchwartzPricer generalizes the LinearLongstaffSchwartzPrices, where the Ridge Regression at each exercise date is replace by learning the conditional expectations via neural networks. In the following initialization we build a network with $3$ hidden layers and $16$ neurons each, between each hidden layer we apply the activation function $\\mathrm{tanh}(x)$. The remainding parameters are set to 'False'. (One can run the 'Hyperparameter_optimization_primal.py' file to optimize the choice of hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da432f",
   "metadata": {
    "id": "b2da432f"
   },
   "outputs": [],
   "source": [
    "ls_pricer = DeepLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='tanh',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # This is correct as float\n",
    "    dropout=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "\n",
    "dual_pricer = DeepDualPricer(\n",
    "    N1=N1,\n",
    "    N=N,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # ERROR: should be float, not boolean\n",
    "    dropout=False,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "# LS pricer call is correct\n",
    "lower_bound_deep, lower_bound_deep_std, ls_regression_models = ls_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    M_val=0,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Dual pricer call is correct\n",
    "y0, upper_bound_deep, upper_bound_deep_std, dual_model, dual_rule_model = dual_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    dW_training[:,:,0],  # use only first component of Brownian increments\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    dW_testing[:,:,0],  # use only first component of Brownian increments\n",
    "    M_val=int(0.9*M),\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jX2tyIWchsd",
   "metadata": {
    "id": "9jX2tyIWchsd"
   },
   "source": [
    "Similarly for the dual problem, we consider the same network but use the $relu(x)$ activation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186917ea",
   "metadata": {
    "id": "186917ea"
   },
   "outputs": [],
   "source": [
    "# Consistent parameter usage for validation set size\n",
    "M_val_percentage = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cDQUUvN_c6wl",
   "metadata": {
    "id": "cDQUUvN_c6wl"
   },
   "source": [
    "The Deep Longstaff Schwartz uses $15$ epochs for at the last exercise date, and then one epochs at the remainding ones by initiliazing smartly. The learning rate for the Stochastic Gradient Descent is choosen as $0.001$, and we use batch sizes of $2^8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e9ff6",
   "metadata": {
    "id": "d04e9ff6",
    "outputId": "8fd83460-c4c2-40ca-8718-2b02f3227db5"
   },
   "outputs": [],
   "source": [
    "print(f\"Deep Longstaff-Schwartz lower bound: {lower_bound_deep} ± {lower_bound_deep_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RsfNfQyIebJD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsfNfQyIebJD",
    "outputId": "7fa5e436-d332-4232-c5e7-1a2784f5d8f4"
   },
   "outputs": [],
   "source": [
    "print(f\"Deep Dual upper bound: {upper_bound_deep} ± {upper_bound_deep_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(lower_bound_deep,upper_bound_deep)}± {np.maximum(upper_bound_deep_std,lower_bound_deep_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_kernel_signature_optimal_stopping import DeepKernelLongstaffSchwartzPricer, DeepKernelDualPricer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96187943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Define the RFF feature computation functions with corrected implementation\n",
    "\n",
    "def compute_rff_kernel_features(signatures, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for lower bound pricer (list format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of tensors with shape [M, rff_dim*2, 1] for each exercise date\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices based on actual data dimensions\n",
    "    actual_steps = T_steps - 1\n",
    "    subindex = [min(int((j+1)*actual_steps/N1), actual_steps) for j in range(N1)]\n",
    "    \n",
    "    print(f\"Signature data has {T_steps} time points\")\n",
    "    print(f\"Using exercise indices: {subindex}\")\n",
    "    \n",
    "    # Create list to hold RFF features for each exercise date\n",
    "    rff_features_list = []\n",
    "    \n",
    "    # For each exercise date\n",
    "    for t in range(len(subindex)):\n",
    "        idx = min(subindex[t], T_steps-1)\n",
    "        X_t = signatures[:, idx, :]\n",
    "        \n",
    "        # Generate random projection matrix for RBF kernel approximation\n",
    "        np.random.seed(42 + t)  # Different seed for each exercise date\n",
    "        W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "        \n",
    "        # Compute RFF: [cos(Wx), sin(Wx)]\n",
    "        projection = X_t @ W\n",
    "        rff_features = np.column_stack([\n",
    "            np.cos(projection),\n",
    "            np.sin(projection)\n",
    "        ]) * np.sqrt(1/rff_dim)\n",
    "        \n",
    "        # Reshape to match expected format: [M, rff_dim*2, 1]\n",
    "        rff_features = rff_features.reshape(M, rff_dim*2, 1)\n",
    "        \n",
    "        rff_features_list.append(rff_features)\n",
    "    \n",
    "    return rff_features_list\n",
    "\n",
    "def compute_rff_kernel_features_dual(signatures, N, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for dual pricer (3D tensor format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N: Number of time steps in the discretization\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with shape [M, features, time] for all time points\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices proportional to exercise dates\n",
    "    # We need to map our exercise indices to the full discretization grid\n",
    "    actual_steps = min(T_steps - 1, N)\n",
    "    all_indices = np.minimum(np.array([int(t * T_steps / (N+1)) for t in range(N+1)]), T_steps-1)\n",
    "    \n",
    "    print(f\"Using exercise indices for dual pricer: {all_indices[:5]}...{all_indices[-5:]}\")\n",
    "    \n",
    "    # Generate random projection matrix once\n",
    "    np.random.seed(42)\n",
    "    W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "    \n",
    "    # Extract all required signature data at once\n",
    "    X_all = signatures[:, all_indices, :]  # Shape: [M, N+1, feature_dim]\n",
    "    \n",
    "    # Reshape for batch matrix multiplication\n",
    "    X_reshaped = X_all.reshape(-1, feature_dim)  # Shape: [M*(N+1), feature_dim]\n",
    "    \n",
    "    # Compute all projections at once\n",
    "    projections = X_reshaped @ W  # Shape: [M*(N+1), rff_dim]\n",
    "    \n",
    "    # Compute RFF features\n",
    "    cos_features = np.cos(projections)\n",
    "    sin_features = np.sin(projections)\n",
    "    rff_features = np.column_stack([cos_features, sin_features]) * np.sqrt(1/rff_dim)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    full_rff = rff_features.reshape(M, N+1, rff_dim*2)\n",
    "    \n",
    "    # Transpose to match expected format: [M, features, time]\n",
    "    return np.transpose(full_rff, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565afbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Calculate and use RFF features for pricing with corrected implementation\n",
    "\n",
    "# Calculate both sets of features\n",
    "rff_dim = 64\n",
    "print(\"Computing kernel features for lower bound...\")\n",
    "kernel_training = compute_rff_kernel_features(log_signatures_training, N1, rff_dim=rff_dim)\n",
    "kernel_testing = compute_rff_kernel_features(log_signatures_testing, N1, rff_dim=rff_dim)\n",
    "\n",
    "# IMPORTANT: For the dual approach, we need to generate features for N1 steps\n",
    "print(\"Computing kernel features for upper bound...\")\n",
    "kernel_training_dual = compute_rff_kernel_features_dual(log_signatures_training, N1, N1, rff_dim=rff_dim)\n",
    "kernel_testing_dual = compute_rff_kernel_features_dual(log_signatures_testing, N1, N1, rff_dim=rff_dim)\n",
    "print(\"Initializing kernel-based lower bound...\")\n",
    "# 1. LOWER BOUND calculation - this works correctly\n",
    "kernel_pricer = DeepKernelLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    L=rff_dim*2,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=False,\n",
    "    layer_normalization=True\n",
    ")\n",
    "\n",
    "print(\"Computing kernel-based lower bound...\")\n",
    "lower_bound_kernel, lower_bound_kernel_std, kernel_models = kernel_pricer.price(\n",
    "    kernel_training,\n",
    "    kernel_testing,\n",
    "    Payoff_training,\n",
    "    Payoff_testing,\n",
    "    batch=2**8,\n",
    "    epochs=15,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "# 2. UPPER BOUND calculation - use direct payoff, no need to expand\n",
    "print(\"Initializing kernel-based upper bound...\")\n",
    "kernel_dual_pricer = DeepKernelDualPricer(\n",
    "    N1=N1,\n",
    "    N=N1,  # Using N1 instead of N=252 here is the key change\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=4,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=True,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=True,\n",
    "    mode_dim=\"1-dim\"\n",
    ")\n",
    "print(\"Computing kernel-based upper bound...\")\n",
    "try:\n",
    "    y0_kernel, upper_bound_kernel, upper_bound_kernel_std, kernel_model, kernel_rule_model = kernel_dual_pricer.price(\n",
    "        kernel_training_dual,\n",
    "        Payoff_training,\n",
    "        dW_training[:,:,0],\n",
    "        kernel_testing_dual,\n",
    "        Payoff_testing,      \n",
    "        dW_testing[:,:,0],\n",
    "        M_val=int(0.9*M),\n",
    "        batch=2**8,\n",
    "        epochs=15,\n",
    "        learning_rate=0.0005\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} ± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Deep Kernel Dual upper bound: {upper_bound_kernel} ± {upper_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Kernel-based pricing interval: [{lower_bound_kernel}, {upper_bound_kernel}] ± {np.maximum(upper_bound_kernel_std, lower_bound_kernel_std)/np.sqrt(M2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in dual pricer: {e}\")\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} ± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(\"Upper bound calculation failed - using only lower bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2K5Yrl-ejcN",
   "metadata": {
    "id": "a2K5Yrl-ejcN"
   },
   "source": [
    "We once again stress that the parameters for the the discretization (here $J=120$), the sample size (here $M=10^{15}$), and the signature trunaction level (here $K=3$) are not choosen big enough to get narrow gaps, but we can still already observe an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b1b07",
   "metadata": {},
   "source": [
    "## Step 5: Contextualizing Theoretical Price in USD\n",
    "Convert the normalized model price bounds into USD per share and per contract, and print actionable trading recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the normalized price bounds to actual USD values\n",
    "actual_stock_price = X0  # USD per share\n",
    "actual_strike = strike  # USD per share\n",
    "\n",
    "# Include all four methods in the list\n",
    "methods = [\n",
    "    \"Linear Signature\", \n",
    "    \"Extended Linear Signature\", \n",
    "    \"Deep Log-Signature\",\n",
    "    \"Deep Kernel Method\"\n",
    "]\n",
    "print(lower_bound_kernel, upper_bound_kernel, lower_bound_kernel_std, upper_bound_kernel_std)\n",
    "\n",
    "# Actual Bond Premium stored as variable premium\n",
    "actual_premium = model_inputs[\"market_premium\"]\n",
    "\n",
    "# Collect all price bounds\n",
    "lower_bounds = [lower_bound, lower_bound_extended, lower_bound_deep, lower_bound_kernel]\n",
    "upper_bounds = [upper_bound, upper_bound_extended, upper_bound_deep, upper_bound_kernel]\n",
    "stds = [lower_bound_std, lower_bound_extended_std, lower_bound_deep_std, lower_bound_kernel_std]\n",
    "\n",
    "# Create a table of results with duality gap\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "results = []\n",
    "for i, method in enumerate(methods):\n",
    "    usd_lower = float(lower_bounds[i])\n",
    "    \n",
    "    # Handle the case where upper bound might not be available for kernel method\n",
    "    if i == 3 and 'upper_bound_kernel' not in locals():\n",
    "        usd_upper = float('nan')  # Use NaN if upper bound isn't available\n",
    "    else:\n",
    "        usd_upper = float(upper_bounds[i]) \n",
    "    \n",
    "    usd_std = float(stds[i]) / np.sqrt(M2)\n",
    "    \n",
    "    # Calculate duality gap only if upper bound exists\n",
    "    if not np.isnan(usd_upper):\n",
    "        duality_gap = usd_upper - usd_lower\n",
    "        gap_percent = duality_gap / usd_lower * 100\n",
    "    else:\n",
    "        duality_gap = float('nan')\n",
    "        gap_percent = float('nan')\n",
    "    \n",
    "    # Determine if premium is within bounds\n",
    "    if not np.isnan(usd_upper):\n",
    "        premium_status = \"Within bounds\" if usd_lower <= actual_premium <= usd_upper else \"Outside bounds\"\n",
    "    else:\n",
    "        premium_status = \"Compared to lower bound only\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Method\": method,\n",
    "        \"Lower Bound (USD)\": f\"${usd_lower:.2f}\",\n",
    "        \"Upper Bound (USD)\": f\"${usd_upper:.2f}\" if not np.isnan(usd_upper) else \"N/A\",\n",
    "        \"Std Error (USD)\": f\"${usd_std:.2f}\",\n",
    "        \"Duality Gap (USD)\": f\"${duality_gap:.2f}\" if not np.isnan(duality_gap) else \"N/A\",\n",
    "        \"Gap (%)\": f\"{gap_percent:.2f}%\" if not np.isnan(gap_percent) else \"N/A\",\n",
    "        \"Market Premium\": f\"${actual_premium:.2f}\",\n",
    "        \"Premium Status\": premium_status\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(HTML(results_df.to_html(index=False)))\n",
    "\n",
    "# Print text comparison as well\n",
    "print(f\"\\nMarket Premium: ${actual_premium:.2f}\")\n",
    "print(\"\\nComparison with calculated bounds:\")\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    lower = float(lower_bounds[i])\n",
    "    upper = float(upper_bounds[i]) if i < 3 or 'upper_bound_kernel' in locals() else float('nan')\n",
    "    \n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Lower Bound: ${lower:.2f}  {'(underprices)' if lower < actual_premium else '(overprices)'}\")\n",
    "    \n",
    "    if not np.isnan(upper):\n",
    "        print(f\"  Upper Bound: ${upper:.2f}  {'(underprices)' if upper < actual_premium else '(overprices)'}\")\n",
    "        print(f\"  Duality Gap: ${upper - lower:.2f} ({(upper - lower)/lower*100:.2f}%)\")\n",
    "        print(f\"  Premium within bounds: {'Yes' if lower <= actual_premium <= upper else 'No'}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d9864a",
   "metadata": {},
   "source": [
    "## Problem with Rough Volatility Simulation\n",
    "\n",
    "# This highlights a limitation of our current rough volatility approach. Because the Hurst exponent we estimated is around 0.7, the model assumes a relatively smooth (less rough) path and ends up over-pricing option premia. Rough volatility frameworks are designed to capture very irregular (highly rough) volatility dynamics typically corresponding to more volatile stocks so when applied to smoother data (H = 0.7) they tend to inflate theoretical premiums compared to actual market prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617b75c6",
   "metadata": {},
   "source": [
    "## Option Trading Interpretation\n",
    "\n",
    "Now let's interpret these results from a trading perspective. We'll evaluate the fair price range for an American put option contract (which typically represents 100 shares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can calculate the fair price range for a standard options contract\n",
    "# Cell under “# For a standard options contract (100 shares)”\n",
    "shares_per_contract = 100\n",
    "contract_lower   = float(lower_bound_deep) * actual_stock_price\n",
    "contract_upper   = float(upper_bound_deep) * actual_stock_price\n",
    "contract_midpoint = (contract_lower + contract_upper) / 2\n",
    "\n",
    "print(f\"American Put Option Contract Analysis (for {shares_per_contract} shares)\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Stock Price: ${actual_stock_price:.2f}\")\n",
    "print(f\"Strike Price: ${actual_strike:.2f}\")\n",
    "print(f\"Time to Maturity: {T} days\")\n",
    "print(f\"Interest Rate: {r*100:.2f}%\")\n",
    "print(f\"Rough Volatility Parameters: H={H}, η={eta}, ρ={rho}, ξ₀={xi}\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Fair Price Range: ${contract_lower:.2f} to ${contract_upper:.2f} per contract\")\n",
    "print(f\"Midpoint Price: ${contract_midpoint:.2f}\")\n",
    "print(f\"=====================================================================\")\n",
    "\n",
    "# Trading recommendations based on market prices\n",
    "hypothetical_market_prices = [contract_lower * 0.8, contract_midpoint, contract_upper * 1.2]\n",
    "labels = [\"Below Fair Value\", \"At Fair Value\", \"Above Fair Value\"]\n",
    "\n",
    "print(\"Trading Recommendations:\")\n",
    "for price, label in zip(hypothetical_market_prices, labels):\n",
    "    print(f\"\\nIf market price is ${price:.2f} ({label}):\")\n",
    "    \n",
    "    if price < contract_lower:\n",
    "        print(\"→ BUY: Market price is below fair value range\")\n",
    "        print(f\"→ Expected edge: ${(contract_lower - price):.2f} to ${(contract_upper - price):.2f} per contract\")\n",
    "        print(\"→ Consider buying puts for protection or speculative profit\")\n",
    "    elif price > contract_upper:\n",
    "        print(\"→ SELL: Market price is above fair value range\")\n",
    "        print(f\"→ Expected edge: ${price - contract_upper:.2f} to ${price - contract_lower:.2f} per contract\")\n",
    "        print(\"→ Consider writing puts, potentially as part of a spread strategy to limit risk\")\n",
    "    else:\n",
    "        print(\"→ NEUTRAL: Market price is within fair value range\")\n",
    "        position = (price - contract_lower) / (contract_upper - contract_lower)\n",
    "        print(f\"→ Price is positioned {position:.0%} through the fair value range\")\n",
    "        if position < 0.4:\n",
    "            print(\"→ Slight bias toward buying\")\n",
    "        elif position > 0.6:\n",
    "            print(\"→ Slight bias toward selling\")\n",
    "        else:\n",
    "            print(\"→ No clear edge for buying or selling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a30e089",
   "metadata": {},
   "source": [
    "## Risk Management Considerations\n",
    "\n",
    "When trading American put options in a rough volatility environment, several risk management considerations are important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual market premium from the data\n",
    "market_premium = actual_premium \n",
    "percent_itm = max(0, (actual_strike - actual_stock_price) / actual_strike)\n",
    "\n",
    "moneyness = actual_stock_price / actual_strike\n",
    "# Corrected time value calculation for a put option\n",
    "intrinsic_value = max(0, actual_strike - actual_stock_price)\n",
    "time_value = market_premium - intrinsic_value\n",
    "\n",
    "rounded_hurst = [round(item, 2) for item in Hurst_list]\n",
    "\n",
    "print(\"Risk Management Considerations:\")\n",
    "print(\"=====================================================================\")\n",
    "print(f\"Moneyness: {moneyness:.2f} ({percent_itm:.0%} in-the-money)\")\n",
    "print(f\"Intrinsic Value: ${intrinsic_value:.2f} per share\")\n",
    "print(f\"Time Value: ${time_value:.2f} per share\")\n",
    "print(f\"Total Premium: ${market_premium:.2f} per share\")\n",
    "print(f\"Uncertainty Range: ${(contract_upper - contract_lower):.2f} per contract\")\n",
    "print(\"\\nRecommended Risk Management Strategies:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"1. Position Sizing: Limit exposure to <5% of portfolio per trade\")\n",
    "print(\"2. Early Exercise Consideration: Monitor optimal stopping boundaries\")\n",
    "print(\"3. Hedging: Consider delta and vega hedging for larger positions\")\n",
    "print(\"4. Model Risk: Be aware model assumes H={rounded_hurst}, may differ from market\")\n",
    "\n",
    "# Additional practical advice\n",
    "print(\"\\nPractical Implementation:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "if moneyness < 0.95:\n",
    "    print(\"→ Deep ITM option: Consider early exercise if dividend yield > interest rate\")\n",
    "    print(\"→ Watch for significant changes in volatility that could shift optimal exercise boundary\")\n",
    "elif moneyness > 1.05:\n",
    "    print(\"→ OTM option: Early exercise unlikely, trade like European option\")\n",
    "    print(\"→ Primary value is in insurance against downside moves\")\n",
    "else:\n",
    "    print(\"→ ATM option: Maximum gamma/vega exposure\")\n",
    "    print(\"→ Most sensitive to changes in volatility and rough volatility parameters\")\n",
    "    print(\"→ Actively monitor for optimal early exercise conditions near expiration\")\n",
    "\n",
    "print(\"\\nNote: This model incorporates rough volatility effects H=({rounded_hurst}) which\\n\")\n",
    "print(\"traditional models like Black-Scholes miss. This can be particularly\")\n",
    "print(\"important for managing risk in volatile market conditions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b9769",
   "metadata": {},
   "source": [
    "## Environment Configuration for TensorFlow\n",
    "<div style=\"background-color: #E8F4FD; padding: 12px; border-radius: 5px; border-left: 5px solid #2196F3; margin: 10px 0;\">\n",
    "This section handles Python path configuration and environment setup for TensorFlow compatibility. We need to:\n",
    "\n",
    "1. Add the necessary directories to the Python path to make modules available for import\n",
    "2. Suppress TensorFlow warnings which can be distracting in a notebook environment\n",
    "3. Set the TensorFlow CPP log level to minimize non-essential messages\n",
    "\n",
    "This is particularly important for the deep learning pricing methods we'll use later in the notebook.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
