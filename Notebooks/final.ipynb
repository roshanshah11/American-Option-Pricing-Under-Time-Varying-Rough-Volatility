{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e1598c",
   "metadata": {},
   "source": [
    "# Enhancing Signature-Based American Option Pricing Through Lightweight, Practical Modifications\n",
    "\n",
    " Signatures  \n",
    "\n",
    "**Student**: Roshan Shah  \n",
    "**Instructor**: Ms. Patel  \n",
    "**Date**: 4 June 2025  \n",
    "\n",
    "This notebook develops an efficient signature-based framework for valuing American options under rough volatility. Key features include dynamic Hurst estimation and fast signature kernel approximation, combined with both primal and dual optimal-stopping techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58ec8c",
   "metadata": {},
   "source": [
    "## Project Objective\n",
    "Enhance Americanâ€option pricing under rough volatility by extending the signatureâ€based framework of Bayer et al. (2025).  \n",
    "Our two practical contributions are:\n",
    "\n",
    "1. **Dynamic Hurst Estimation** â€“ Predict a time-varying sequence \\(H(t)\\) from rolling volatility-path signatures, replacing the fixed roughness assumption.  \n",
    "2. **Fast Signature Kernels** â€“ Use Random Fourier Features to approximate signature kernels, reducing runtime for high-dimensional paths.\n",
    "\n",
    "We combine a **primal Longstaffâ€“Schwartz approach** with a **dual martingale formulation**, quantify the duality gap, and benchmark computational efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- Parameter Estimation from Options Data  \n",
    "- Signature Feature Extraction  \n",
    "- Hurst Estimation Model  \n",
    "- Dynamic Roughness Integration  \n",
    "- Volatility Path Simulation  \n",
    "- American Option Pricing (Primal)  \n",
    "- American Option Pricing (Dual)  \n",
    "- Evaluation and Results  \n",
    "- Future Work  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc9cab",
   "metadata": {
    "id": "eccc9cab"
   },
   "source": [
    "# Pricing American Options in Rough Bergomi\n",
    "\n",
    "## Using Linear and Deep Signature Methods\n",
    "\n",
    "---\n",
    "\n",
    "*Part of the [Optimal Stopping with Signatures](https://github.com/roshanshah11/Optimal_Stopping_with_signatures) project*\n",
    "\n",
    "---\n",
    "\n",
    "> **Project Overview:**  \n",
    "> This notebook applies **signature-based methods** to solve **optimal stopping problems** in **non-Markovian settings**, with a focus on pricing American options under **rough volatility**.  \n",
    "> \n",
    "> **Primary Reference:**  \n",
    "> [Optimal Stopping with Signatures (arXiv:2312.03444)](https://arxiv.org/abs/2312.03444) by Bayer, Pelizzari, and Schoenmakers (2025)\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation Summary\n",
    "\n",
    "This implementation improves the framework proposed by Bayer et al. by introducing two key practical enhancements:\n",
    "\n",
    "- **Dynamic Hurst Estimation**  \n",
    "  A gradient-boosted regression model predicts a sequence of time-varying Hurst parameters \\( H(t) \\) from rolling volatility path signatures.\n",
    "\n",
    "- **Fast Signature Kernel Approximation**  \n",
    "  Signature-kernel computations are accelerated using **Random Fourier Features**, making high-dimensional paths (N > 100) tractable.\n",
    "\n",
    "We implement both:\n",
    "\n",
    "- **Primal pricing** using Longstaff-Schwartz regression over signature features  \n",
    "- **Dual pricing** using martingale representations parameterized by the path signature\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ckOdm_OAGE",
   "metadata": {
    "id": "c9ckOdm_OAGE"
   },
   "source": [
    "# Repository Overview\n",
    "\n",
    "> This notebook demonstrates the code from [the Optimal Stopping with Signatures repository](https://github.com/lucapelizzari/Optimal_Stopping_with_signatures/tree/main) to compute lower and upper bounds for American options in the rough Bergomi model using signature methods. The linear approach is described in Section 4.2 of [the accompanying paper](https://arxiv.org/abs/2312.03444), while the deep neural network approaches will be discussed in a forthcoming paper.\n",
    "\n",
    "## Repository Contents\n",
    "\n",
    "The codebase includes:\n",
    "\n",
    "* **Simulation packages** for fractional Brownian motion, rough Bergomi and rough Heston models\n",
    "  \n",
    "* **Signature computation module** (`Signature_computer.py`) - Computes signature and log-signature of various lifts related to volatility modeling, with options to add polynomials of the state-process and/or volatility\n",
    "\n",
    "* **Linear signature module** (`Linear_signature_optimal_stopping.py`) - Derives lower and upper bounds to the optimal stopping problem using the approaches described in [the paper](https://arxiv.org/abs/2312.03444)\n",
    "\n",
    "* **Deep log-signature module** (`Deep_signature_optimal_stopping.py`) - Extends the linear approaches by applying deep neural networks on the log-signature, accompanying a working paper on \"American option pricing using signatures\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78183f8f",
   "metadata": {},
   "source": [
    "# Section 1: Parameter Estimation From Options Data\n",
    "\n",
    "---\n",
    "\n",
    "## *A step-by-step guide to estimate rough volatility model parameters from options data*\n",
    "\n",
    "---\n",
    "\n",
    "Let's dive in!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a38f842",
   "metadata": {},
   "source": [
    "## 1.1 Importing Required Libraries\n",
    "<div style=\"border-left: 5px solid #4472C4; padding-left: 10px;\">\n",
    "First, we'll import the necessary Python libraries for data handling, numerical computations, and file operations.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f371da7",
   "metadata": {},
   "source": [
    "## 1.2 Initializing Global Parameters\n",
    "\n",
    "> **Model Parameters**\n",
    ">\n",
    "> Here we set up placeholders for our model parameters. These will be populated later in the notebook based on our analysis.\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `N1`, `N`, `T` | Time discretization parameters |\n",
    "| `M`, `M2` | Monte Carlo simulation parameters |\n",
    "| `eta` | Volatility of volatility parameter |\n",
    "| `X0` | Initial asset price (forward price) |\n",
    "| `r` | Risk-free interest rate |\n",
    "| `rho` | Correlation between asset returns and volatility |\n",
    "| `xi` | Initial variance value |\n",
    "| `strike` | Option strike price |\n",
    "| `K` | Model calibration parameter |\n",
    "\n",
    "### 1.3 The Payoff Function\n",
    "\n",
    "> For put options, the payoff function determines the final value of the option at expiration. It returns the maximum of (strike price - asset price) or zero.\n",
    ">\n",
    "> $\\text{Payoff} = \\max(K - S_T, 0)$\n",
    ">\n",
    "> where $K$ is the strike price and $S_T$ is the asset price at expiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb829a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global option model parameters (pre-initialized)\n",
    "N1 = 0\n",
    "N = 0\n",
    "T = 0\n",
    "T_years = 0.0\n",
    "M = 0\n",
    "M2 = 0\n",
    "eta = 0.0\n",
    "X0 = 0.0\n",
    "r = 0.0\n",
    "rho = 0.0\n",
    "xi = 0.0\n",
    "strike = 0.0\n",
    "K = 0\n",
    "Hurst_list = []\n",
    "\n",
    "def phi(x):\n",
    "    return np.maximum(strike - x, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b91dee",
   "metadata": {},
   "source": [
    "Paths/ Loading_Cleaning the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2de602",
   "metadata": {},
   "source": [
    "## 1.4 Setting Up File Paths / Data Loading Function\n",
    "\n",
    "> **File Path Configuration**  \n",
    "> Now we define the file paths for our data. We use Path from the pathlib library to ensure cross-platform compatibility.\n",
    "\n",
    "> **Data Loading Function**  \n",
    "> This function handles loading the price dataset. It checks if a cached version exists (for faster loading) and otherwise loads from CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60aad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging setup\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Paths\n",
    "repo_root = os.path.abspath('..')\n",
    "notebook_dir = Path(repo_root) / 'XGboost_Roshan' / 'src'\n",
    "DATA_PICKLE = notebook_dir / 'data' / 'dataset1.pkl'\n",
    "RETURNS_PICKLE = notebook_dir / 'data' / 'dataset3.pkl'\n",
    "\n",
    "def load_and_clean_data(pickle_path):\n",
    "    if os.path.exists(pickle_path):\n",
    "        logger.info(f\"Loading data from {pickle_path}\")\n",
    "        df = pd.read_pickle(pickle_path)\n",
    "    else:\n",
    "        logger.warning(f\"Pickle file not found at {pickle_path}\")\n",
    "        os.makedirs(pickle_path.parent, exist_ok=True)\n",
    "        csv_path = pickle_path.with_suffix('.csv')\n",
    "        if os.path.exists(csv_path):\n",
    "            logger.info(f\"Found CSV at {csv_path}. Converting to pickle.\")\n",
    "            df = pd.read_csv(csv_path, parse_dates=['date'], dayfirst=False)\n",
    "            df.to_pickle(pickle_path)\n",
    "            logger.info(f\"Converted and saved as pickle: {pickle_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"CSV file also not found at {csv_path}\")\n",
    "            return None\n",
    "\n",
    "    if 'secid' in df.columns or 'index_flag' in df.columns:\n",
    "        df = df.drop(columns=['secid', 'index_flag'], errors='ignore')\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    logger.info(f\"Loaded {df.shape[0]} rows and {df.shape[1]} columns from {pickle_path.name}\")\n",
    "    return df\n",
    "\n",
    "def select_data_by_date_and_days(df, target_date=None, days_length=None):\n",
    "    filtered_df = df.copy()\n",
    "    if target_date:\n",
    "        filtered_df = filtered_df[filtered_df['date'] == pd.to_datetime(target_date)]\n",
    "    if days_length is not None:\n",
    "        filtered_df = filtered_df[filtered_df['days'] == days_length]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def extract_model_inputs(option_df):\n",
    "    put_df = option_df[option_df['cp_flag'] == 'P']\n",
    "    if put_df.empty:\n",
    "        return None\n",
    "    row = put_df.iloc[0]\n",
    "    return {\n",
    "        \"ticker\": row['ticker'],\n",
    "        \"date\": row['date'],\n",
    "        \"T_years\": row['days'] / 252,\n",
    "        \"strike\": row['strike_price'],\n",
    "        \"X0 (forward)\": row['forward_price'],\n",
    "        \"market_premium\": row['premium']\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b86736",
   "metadata": {},
   "source": [
    "## 1.5 Parameter Estimation Functions\n",
    "\n",
    "> Now we'll define functions to estimate two critical model parameters for rough volatility models:\n",
    "\n",
    "### Parameter: Ï (rho)\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Meaning** | Correlation between returns and volatility |\n",
    "| **Formula** | $\\rho = \\text{Corr}(r_t, \\Delta \\sigma_t)$ |\n",
    "\n",
    "### Parameter: Î· (eta)\n",
    "\n",
    "| Aspect | Description |\n",
    "|--------|-------------|\n",
    "| **Meaning** | Volatility of volatility |\n",
    "| **Formula** | $\\eta = \\frac{\\text{std}(\\Delta \\log \\sigma_t)}{(\\Delta t)^H}$ |\n",
    "\n",
    "> **Note on Hurst Exponent**: We use a rolling Hurst exponent $(H)$ in the denominator of Î· to reflect local path roughness, which we calculated using [`train_hurst.py`](../../Optimal_Stopping_with_signatures-main/XGboost_Roshan/src/train_hurst.py)\n",
    "\n",
    "---\n",
    "\n",
    "These parameters are crucial for capturing the dynamics of financial markets accurately.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87378d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_rho(returns_df, options_df, target_ticker, target_date, days_length):\n",
    "    ret_df = returns_df[(returns_df['ticker'] == target_ticker) & (returns_df['date'] <= target_date)].copy()\n",
    "    opt_df = options_df[(options_df['ticker'] == target_ticker) & \n",
    "                        (options_df['cp_flag'] == 'P') & \n",
    "                        (options_df['days'] == days_length) & \n",
    "                        (options_df['date'] <= target_date)].copy()\n",
    "\n",
    "    ret_df = ret_df.sort_values('date')\n",
    "    opt_df = opt_df.sort_values('date').drop_duplicates(subset=['date'])\n",
    "\n",
    "    merged = pd.merge(ret_df, opt_df[['date', 'impl_volatility']], on='date', how='inner')\n",
    "    merged['vol_change'] = merged['impl_volatility'].diff()\n",
    "    merged = merged.dropna(subset=['return', 'vol_change'])\n",
    "\n",
    "    if len(merged) < 2:\n",
    "        return None\n",
    "    return merged['return'].corr(merged['vol_change'])\n",
    "\n",
    "def compute_rolling_hurst(returns: np.ndarray, power: int) -> np.ndarray:\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        returns = np.array(returns)\n",
    "    n = 2**power\n",
    "    if len(returns) < n:\n",
    "        raise ValueError(f\"Need at least {n} data points for power={power}\")\n",
    "    hursts = []\n",
    "    exponents = np.arange(2, power+1)\n",
    "    for t in range(n, len(returns) + 1):\n",
    "        window = returns[t-n:t]\n",
    "        rs_log = []\n",
    "        for exp in exponents:\n",
    "            m = 2**exp\n",
    "            s = n // m\n",
    "            segments = window.reshape(s, m)\n",
    "            dev = np.cumsum(segments - segments.mean(axis=1, keepdims=True), axis=1)\n",
    "            R = dev.max(axis=1) - dev.min(axis=1)\n",
    "            S = segments.std(axis=1)\n",
    "            rs = np.where(S != 0, R/S, 0)\n",
    "            rs_log.append(np.log2(rs.mean()))\n",
    "        hursts.append(np.polyfit(exponents, rs_log, 1)[0])\n",
    "    return np.array(hursts)\n",
    "\n",
    "def estimate_eta_and_xi(option_df, hurst):\n",
    "    option_df = option_df.sort_values('date').drop_duplicates(subset=['date'])\n",
    "    option_df['log_vol'] = np.log(option_df['impl_volatility'])\n",
    "    option_df['log_vol_diff'] = option_df['log_vol'].diff()\n",
    "    std_log_vol_diff = option_df['log_vol_diff'].dropna().std()\n",
    "    delta_t = 1 / 252\n",
    "    eta = std_log_vol_diff / (delta_t ** hurst + 0.5)\n",
    "    xi = option_df['impl_volatility'].iloc[0] ** 2 if not option_df['impl_volatility'].isnull().all() else 0.0\n",
    "    return eta, xi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efcbfa",
   "metadata": {},
   "source": [
    "## 1.6 Main Analysis Process for Determining the Parameters\n",
    "\n",
    "> This section defines our main analysis function that brings everything together. This function performs the following steps:\n",
    "\n",
    "1. **Filter the option data** by ticker, date, and days to maturity\n",
    "2. **Extract model inputs** from the filtered put options\n",
    "3. **Estimate the correlation (Ï)** between returns and volatility\n",
    "4. **Compute the latest Hurst exponent (H)** for the roughness of volatility\n",
    "5. **Estimate volatility parameters** (Î· and Î¾)\n",
    "6. **Set up all required** model configuration parameters\n",
    "\n",
    "---\n",
    "\n",
    "*The analysis process creates a comprehensive pipeline that transforms raw option data into calibrated model parameters for the rough volatility model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = load_and_clean_data(DATA_PICKLE)\n",
    "    returns_df = load_and_clean_data(RETURNS_PICKLE)\n",
    "\n",
    "    if df is not None and returns_df is not None:\n",
    "        date_input = input(\"Enter date (YYYY-MM-DD) from 2022-08-31 to 2023-08-31 [default: 2023-08-31]: \").strip()\n",
    "        target_date = pd.to_datetime(date_input if date_input else \"2023-08-31\")\n",
    "\n",
    "        days_input = input(\"Enter option length in days(Either 10,30,60, 91, 122,152,182,273,365,547,730) [default: 10]: \").strip()\n",
    "        try:\n",
    "            days_length = int(days_input) if days_input else 10\n",
    "        except ValueError:\n",
    "            logger.warning(\"Invalid input for days. Defaulting to 10.\")\n",
    "            days_length = 10\n",
    "\n",
    "        ticker_input = input(\"Enter ticker (Either: AAPL, TSLA, JPM, AMD, META) [default: AAPL]: \").strip().upper()\n",
    "        ticker = ticker_input if ticker_input else \"AAPL\"\n",
    "\n",
    "        filtered_data = select_data_by_date_and_days(df, target_date, days_length)\n",
    "        ticker_data = filtered_data[filtered_data['ticker'] == ticker]\n",
    "\n",
    "        if len(ticker_data) > 0:\n",
    "            print(f\"\\nFiltered data for {ticker} on {target_date.date()} with {days_length} days:\")\n",
    "            print(ticker_data)\n",
    "            model_inputs = extract_model_inputs(ticker_data)\n",
    "            if model_inputs:\n",
    "                print(\"\\n Extracted Model Inputs (PUTS):\")\n",
    "                for k, v in model_inputs.items():\n",
    "                    print(f\"{k}: {v}\")\n",
    "\n",
    "                rho_est = round(estimate_rho(returns_df, df, ticker, target_date, days_length), 2)\n",
    "                print(f\"Estimated rho: {rho_est}\" if rho_est is not None else \"Rho estimation failed.\")\n",
    "                rho = rho_est if rho_est is not None else -0.9\n",
    "\n",
    "                rets = returns_df[(returns_df['ticker'] == ticker) & (returns_df['date'] <= target_date)]\n",
    "                rets = rets.sort_values('date')['return'].values\n",
    "                hurst_values = compute_rolling_hurst(rets, power=5)\n",
    "                hurst_dates = returns_df[(returns_df['ticker'] == ticker) & (returns_df['date'] <= target_date)].sort_values('date').iloc[2**5 - 1:]['date'].values\n",
    "                hurst_df = pd.DataFrame({'date': hurst_dates, 'H': hurst_values})\n",
    "                H_df = hurst_df[hurst_df['date'] <= target_date]\n",
    "                latest_H = H_df.iloc[-1]['H'] if not H_df.empty else 0.1\n",
    "                print(f\"Using Hurst: {round(latest_H, 3)}\")\n",
    "\n",
    "                iv_path_df = df[\n",
    "                    (df['ticker'] == ticker) &\n",
    "                    (df['cp_flag'] == 'P') &\n",
    "                    (df['days'] == days_length) &\n",
    "                    (df['date'] <= target_date)\n",
    "                ].sort_values('date')\n",
    "\n",
    "                eta, xi = estimate_eta_and_xi(iv_path_df, hurst=latest_H)\n",
    "                eta = round(eta, 3)\n",
    "                xi = round(xi, 5)\n",
    "\n",
    "                N = 252\n",
    "                M = 2**15\n",
    "                M2 = 2**15\n",
    "                \n",
    "                \n",
    "                r = 0.05\n",
    "                K = 3\n",
    "\n",
    "                T = int(model_inputs[\"T_years\"] * 252)\n",
    "                N1 = T\n",
    "                premium = model_inputs[\"market_premium\"]\n",
    "                T_years = model_inputs[\"T_years\"]\n",
    "                X0 = model_inputs[\"X0 (forwa\" \\\n",
    "                \"rd)\"]\n",
    "                strike = model_inputs[\"strike\"]\n",
    "\n",
    "                print(\"\\nâœ… Global parameters set:\")\n",
    "                print(f\"N1 = {N1}, N = {N}, T = {T}, T_years = {T_years:.5f}\")\n",
    "                print(f\"M = {M}, M2 = {M2}, eta = {eta}, X0 = {X0}, strike = {strike}\")\n",
    "                print(f\"r = {r}, rho = {rho}, xi = {xi}, K = {K}\")\n",
    "            else:\n",
    "                logger.warning(\"No put option found for this filter.\")\n",
    "        else:\n",
    "            logger.warning(\"No data matches your filter criteria.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17633a0d",
   "metadata": {},
   "source": [
    "> ## ðŸ“‹ Parameter Verification Note\n",
    "> \n",
    "> This cell displays a comprehensive summary of all global parameters after initialization:\n",
    ">\n",
    "> ### Time Parameters\n",
    "> - **N1, N**: Discretization points\n",
    "> - **T, T_years**: Time horizon in units and years\n",
    ">\n",
    "> ### Simulation Parameters\n",
    "> - **M**: Monte Carlo sample size (training)\n",
    "> - **M2**: Monte Carlo sample size (testing)\n",
    ">\n",
    "> ### Volatility Model Parameters\n",
    "> - **eta**: Volatility of volatility\n",
    "> - **xi**: Initial variance\n",
    "> - **rho**: Correlation between price and volatility\n",
    ">\n",
    "> ### Option Parameters\n",
    "> - **X0**: Initial forward price\n",
    "> - **strike**: Option strike price\n",
    "> - **r**: Risk-free interest rate\n",
    ">\n",
    "> ### Model Parameters\n",
    "> - **K**: Calibration parameter\n",
    ">\n",
    "> *Always verify these values before proceeding with calculations to ensure correct model behavior.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e039383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"n1 = {N1}, n = {N}, T = {T}, T_years = {T_years:.5f}, M = {M}, M2 = {M2}, eta = {eta:}, X0 = {X0:.5f}, strike = {strike:.5f}, r = {r:.5f}, rho = {rho:.5f}, xi = {xi:.5f}, K = {K}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895274e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for TensorFlow import issues and environment compatibility\n",
    "# Set correct Python path to find modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the root directory and subdirectories to Python path\n",
    "repo_root = os.path.abspath('..')\n",
    "sys.path.append(repo_root)\n",
    "sys.path.append(os.path.join(repo_root, \"Linear signature optimal stopping\"))\n",
    "sys.path.append(os.path.join(repo_root, \"Non linear signature optimal stopping\"))\n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ccd27",
   "metadata": {},
   "source": [
    "## Now I will Estimating my Hurst Parameter:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925868f8",
   "metadata": {},
   "source": [
    "# Section 2: XGBoost for Hurst Exponent Forecasting (over multiple days)\n",
    "\n",
    "This notebook provides a detailed walkthrough of [`train_hurst.py`](../../Optimal_Stopping_with_signatures-main/XGboost_Roshan/src/train_hurst.py), a script that computes the rolling Hurst exponent of a time series and trains/evaluates an XGBoost model to forecast its future values.\n",
    "\n",
    "## What is the Hurst Exponent?\n",
    "\n",
    "> The Hurst exponent is a measure used in time series analysis that quantifies the long-term memory of a series. It helps determine if a time series is:\n",
    "> - **H < 0.5**: Anti-persistent (mean-reverting)\n",
    "> - **H = 0.5**: Random walk (no memory)\n",
    "> - **H > 0.5**: Trend-reinforcing (persistent)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [2.1 Library Imports and Configuration Setup](#section-2-1)\n",
    "- [2.2 Computing the Rolling Hurst Exponent](#section-2-2)\n",
    "- [2.3 Loading and Preparing Financial Data](#section-2-3)\n",
    "- [2.4 Creating Lagged Features for Time Series Forecasting](#section-2-4)\n",
    "- [2.5 Training the XGBoost Model for Hurst Prediction](#section-2-5)\n",
    "- [2.6 Evaluating the XGBoost Model Performance](#section-2-6)\n",
    "- [2.7 Forecasting Future Hurst Exponents](#section-2-7)\n",
    "- [2.8 Main Function for Hurst Analysis](#section-2-8)\n",
    "- [2.8 Run the Main Function](#section-2-9)\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "*Last updated: 2025-05-20 15:08:45 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d1645",
   "metadata": {},
   "source": [
    "<a id=\"section-2-1\"></a>\n",
    "\n",
    "## 2.1 Library Imports and Configuration Setup\n",
    "\n",
    "This section imports all necessary Python libraries required for the Hurst exponent calculation and XGBoost forecasting, including:\n",
    "\n",
    "- **Numerical & Data Processing**: NumPy, Pandas\n",
    "- **Visualization**: Matplotlib \n",
    "- **File System**: Pathlib\n",
    "- **Machine Learning**: XGBoost, Scikit-learn metrics\n",
    "- **Utilities**: Logging, Joblib\n",
    "\n",
    "The configuration section establishes:\n",
    "- Forecast horizon settings\n",
    "- Logging configuration\n",
    "- Model storage directories\n",
    "\n",
    "> The code below sets up the foundation for all subsequent analysis and model building steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for numerical operations, data manipulation, and visualization\n",
    "import numpy as np                # For numerical computations (arrays, math functions)\n",
    "import pandas as pd               # For data manipulation with DataFrames\n",
    "import matplotlib.pyplot as plt   # For creating plots and visualizations\n",
    "from pathlib import Path          # For handling file paths in a platform-independent way\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error  # For model evaluation metrics\n",
    "import logging                    # For logging status and debug messages\n",
    "import joblib                     # For saving/loading Python objects (like our trained models)\n",
    "from xgboost import XGBRegressor  # XGBoost regression model implementation\n",
    "\n",
    "# Set up basic configuration\n",
    "# How many days ahead to predict (forecast horizon)\n",
    "HORIZON = T\n",
    "\n",
    "# Configure logging to show informational messages with timestamps\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Show INFO level messages and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Include timestamp and message level\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Create directory for saving trained models\n",
    "MODELS_DIR = notebook_dir / 'models'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)  # Create directory if it doesn't exist\n",
    "\n",
    "print(f\"Models will be stored in: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a97766b",
   "metadata": {},
   "source": [
    "<a id=\"section-2-2\"></a>\n",
    "\n",
    "## 2.2 Computing the Rolling Hurst Exponent\n",
    "\n",
    "This function calculates the Hurst exponent using Rescaled Range (R/S) analysis over a rolling window. The Hurst exponent measures the long-term memory or persistence of a time series.\n",
    "\n",
    "> ðŸ”‘ **Key Concepts:**\n",
    "> - **Window size**: 2^power determines how many data points we use for each calculation\n",
    "> - **R/S Analysis**: Calculates the ratio of the range to the standard deviation at different time scales\n",
    "> - **Slope of log-log plot**: The slope of the line relating log(R/S) to log(time scale) gives us the Hurst exponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_hurst(returns: np.ndarray, power: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Hurst exponent using R/S analysis over\n",
    "    a rolling window of size 2^power.\n",
    "    \"\"\"\n",
    "    # here is a source i used: https://en.wikipedia.org/wiki/Hurst_exponent\n",
    "    # Ensure returns are in numpy array form\n",
    "    if not isinstance(returns, np.ndarray):\n",
    "        returns = np.array(returns)\n",
    "    # Validate 'power' parameter\n",
    "    if not isinstance(power, int) or power < 1:\n",
    "        raise ValueError(\"power must be a positive integer\")\n",
    "    # Window length = 2^power\n",
    "    n = 2**power\n",
    "    if len(returns) < n:\n",
    "        raise ValueError(f\"Need at least {n} data points for power={power}\")\n",
    "    hursts = []                          # list to collect Hurst exponents\n",
    "    exponents = np.arange(2, power+1)    # scales for R/S calculation\n",
    "    # Slide the window through the data\n",
    "    for t in range(n, len(returns) + 1):\n",
    "        window = returns[t-n:t]          # current data chunk\n",
    "        rs_log = []                      # log2 of rescaled range values\n",
    "        for exp in exponents:\n",
    "            # split window into segments and compute R/S per segment\n",
    "            m = 2**exp\n",
    "            s = n // m\n",
    "            segments = window.reshape(s, m)\n",
    "            dev = np.cumsum(\n",
    "                segments - segments.mean(axis=1, keepdims=True),\n",
    "                axis=1\n",
    "            )\n",
    "            R = dev.max(axis=1) - dev.min(axis=1)\n",
    "            S = segments.std(axis=1)\n",
    "            rs = np.where(S != 0, R/S, 0)\n",
    "            rs_log.append(np.log2(rs.mean()))\n",
    "        # linear fit: slope â‰ˆ Hurst exponent\n",
    "        hursts.append(np.polyfit(exponents, rs_log, 1)[0])\n",
    "    return np.array(hursts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a66ee6",
   "metadata": {},
   "source": [
    "<a id=\"section-2-3\"></a>\n",
    "\n",
    "## 2.3 Loading and Preparing Financial Data\n",
    "\n",
    "This section implements the data loading functionality that retrieves price data for our Hurst exponent calculations. The function loads data efficiently by:\n",
    "\n",
    "1. First checking for a cached version of the dataset\n",
    "2. Loading from the faster pickle format when available\n",
    "3. Otherwise processing and sorting the data by date\n",
    "4. Creating a cache for future use\n",
    "\n",
    "> The `load_data()` function returns a pandas DataFrame containing all necessary price data for our time series analysis, properly sorted chronologically for accurate Hurst exponent calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the price dataset from CSV or cache, sort by date,\n",
    "    and return a pandas DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing price data sorted by date\n",
    "    \"\"\"\n",
    "    # First, check if a cached (pickled) version of the data exists\n",
    "    if RETURNS_PICKLE.exists():\n",
    "        # Load from the pickle file (much faster than CSV)\n",
    "        df = pd.read_pickle(RETURNS_PICKLE)\n",
    "        logger.info(f\"Loaded data from cache: {RETURNS_PICKLE}\")\n",
    "    else:\n",
    "        # Sort the data by date\n",
    "        df.sort_values('date', inplace=True)\n",
    "        \n",
    "        # Cache the data for faster future loading\n",
    "        df.to_pickle(RETURNS_PICKLE)\n",
    "        logger.info(f\"Saved data to cache: {RETURNS_PICKLE}\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea4ace",
   "metadata": {},
   "source": [
    "<a id=\"section-2-4\"></a>\n",
    "\n",
    "## 2.4 Creating Lagged Features for Time Series Forecasting\n",
    "\n",
    "This function creates lagged features for time series forecasting. Lagged features are simply past values of the time series that are used to predict future values.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "| Concept | Description |\n",
    "|---------|-------------|\n",
    "| **Lag** | Using past values (t-1, t-2, ..., t-k) to predict the current value (t) |\n",
    "| **Feature Matrix X** | Each row contains k consecutive past values |\n",
    "| **Target Vector y** | Contains the value we want to predict (the value at time t) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lagged_features(series: pd.Series, k: int):\n",
    "    \"\"\"\n",
    "    Generate lagged features matrix X and target vector y from a time series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Time series data to create lagged features from\n",
    "    k : int\n",
    "        Number of lagged values to use as features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    (np.ndarray, np.ndarray)\n",
    "        X: Feature matrix where each row contains k consecutive past values\n",
    "        y: Target vector containing the values to predict\n",
    "    \"\"\"\n",
    "    # Convert series to a numpy array for faster processing\n",
    "    vals = series.values\n",
    "    \n",
    "    # Initialize empty lists for features and targets\n",
    "    X, y = [], []\n",
    "    \n",
    "    # For each point in the time series (starting from position k)\n",
    "    for i in range(k, len(vals)):\n",
    "        # Add the k previous values as features\n",
    "        X.append(vals[i-k:i])\n",
    "        # Add the current value as the target\n",
    "        y.append(vals[i])\n",
    "    \n",
    "    # Convert lists to numpy arrays and return\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293adf7b",
   "metadata": {},
   "source": [
    "<a id=\"section-2-5\"></a>\n",
    "\n",
    "## 2.5 Training the XGBoost Model for Hurst Prediction\n",
    "\n",
    "This section defines the function for training a standalone XGBoost regression model on the time series of Hurst exponents. The training process follows these key steps:\n",
    "\n",
    "1. **Temporal Data Splitting** - Divides the Hurst exponent time series into training and testing sets based on chronological order\n",
    "2. **Feature Engineering** - Creates lagged features from the training data using the previously defined function\n",
    "3. **Model Training** - Initializes and fits an XGBoost regressor with the provided hyperparameters\n",
    "4. **Model Persistence** - Saves the trained model to disk for later prediction use\n",
    "\n",
    "> The XGBoost model learns patterns in the Hurst exponent's temporal evolution, allowing us to forecast how market roughness will change in future periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61de46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_step_xgb(hurst_series: pd.Series, train_frac: float, k: int, horizon: int, xgb_params: dict):\n",
    "    \"\"\"Train multiple XGBoost models to predict all steps in the forecast horizon at once.\"\"\"\n",
    "    split = int(len(hurst_series) * train_frac)\n",
    "    train = hurst_series.iloc[:split]\n",
    "    values = train.values\n",
    "    \n",
    "    # Create separate models for each step in the forecast horizon\n",
    "    models = []\n",
    "    \n",
    "    for step in range(horizon):\n",
    "        X, y = [], []\n",
    "        # For each forecast step, create appropriate training examples\n",
    "        for i in range(k, len(values) - step):\n",
    "            X.append(values[i-k:i])\n",
    "            y.append(values[i+step])  # Target is 'step' days ahead\n",
    "            \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        # Train model for this specific forecast step\n",
    "        model = XGBRegressor(**xgb_params)\n",
    "        model.fit(X, y)\n",
    "        models.append(model)\n",
    "        logger.info(f\"Trained model for step {step+1}/{horizon}\")\n",
    "    \n",
    "    # Save the models\n",
    "    joblib.dump(models, MODELS_DIR / f'xgb_multi_step_{horizon}.pkl')\n",
    "    logger.info(f\"Multi-step XGBoost models saved to {MODELS_DIR}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd37ac",
   "metadata": {},
   "source": [
    "<a id=\"section-2-6\"></a>\n",
    "\n",
    "## 2.6 Evaluating the XGBoost Model Performance\n",
    "\n",
    "This section implements the evaluation framework for our trained XGBoost model. The function performs a comprehensive assessment through:\n",
    "\n",
    "1. **Time-based Test Split** - Uses the most recent portion of the Hurst exponent time series for out-of-sample testing\n",
    "2. **Feature Generation** - Creates the same lagged feature structure for test data as was used in training\n",
    "3. **Performance Metrics** - Calculates both Mean Squared Error (MSE) and Mean Absolute Error (MAE)\n",
    "4. **Visual Validation** - Plots actual vs. predicted Hurst exponent values over time\n",
    "\n",
    "> The evaluation provides quantitative metrics and visual confirmation of how well our model can forecast future Hurst exponent values, which is critical for assessing market roughness dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad985374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multi_step_xgb(hurst_series: pd.Series, models: list, k: int, test_frac: float, horizon: int):\n",
    "    \"\"\"Evaluate multi-step XGBoost models on test split and plot results.\"\"\"\n",
    "    split = int(len(hurst_series) * (1 - test_frac))\n",
    "    test = hurst_series.iloc[split:]\n",
    "    \n",
    "    # Calculate the actual values for each forecast step\n",
    "    actuals = []\n",
    "    for step in range(horizon):\n",
    "        if step < len(test) - k:\n",
    "            actual_vals = test.iloc[k+step:].values\n",
    "            actuals.append(actual_vals)\n",
    "        else:\n",
    "            actuals.append([])\n",
    "    \n",
    "    # Calculate predictions for each step\n",
    "    predictions = []\n",
    "    for step, model in enumerate(models):\n",
    "        if step < horizon:\n",
    "            X_test = []\n",
    "            for i in range(k, len(test) - step):\n",
    "                X_test.append(test.iloc[i-k:i].values)\n",
    "            \n",
    "            if X_test:\n",
    "                X_test = np.array(X_test)\n",
    "                preds = model.predict(X_test)\n",
    "                predictions.append(preds)\n",
    "            else:\n",
    "                predictions.append([])\n",
    "    \n",
    "    # Calculate error metrics for each step\n",
    "    mse_values = []\n",
    "    mae_values = []\n",
    "    for step in range(horizon):\n",
    "        if len(actuals[step]) > 0 and len(predictions[step]) > 0:\n",
    "            min_len = min(len(actuals[step]), len(predictions[step]))\n",
    "            mse = mean_squared_error(actuals[step][:min_len], predictions[step][:min_len])\n",
    "            mae = mean_absolute_error(actuals[step][:min_len], predictions[step][:min_len])\n",
    "            mse_values.append(mse)\n",
    "            mae_values.append(mae)\n",
    "            logger.info(f\"Step {step+1} - MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    \n",
    "    # Plot results for multiple steps\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    dates_test = test.index[k:]\n",
    "    plt.plot(dates_test, test.iloc[k:].values, 'k-', label='Actual Hurst')\n",
    "    \n",
    "    # Assuming horizon is defined earlier\n",
    "    colors = [tuple(np.random.random(3)) for _ in range(horizon)]  # Generate as tuples, not lists\n",
    "\n",
    "    for step in range(horizon):\n",
    "        if len(predictions[step]) > 0:\n",
    "            forecast_dates = dates_test[:len(predictions[step])]\n",
    "            plt.plot(forecast_dates, \n",
    "                    predictions[step], \n",
    "                    color=colors[step],  # Use color parameter instead of format string\n",
    "                    label=f'Step {step+1} Forecast')\n",
    "    \n",
    "    plt.title('Multi-Step XGBoost: Actual vs Forecast Hurst Exponent')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Hurst Exponent')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return np.mean(mse_values), np.mean(mae_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4b370",
   "metadata": {},
   "source": [
    "<a id=\"section-2-7\"></a>\n",
    "\n",
    "## 2.7 Forecasting Future Hurst Exponents\n",
    "\n",
    "This section implements the forecasting functionality that generates predictions for future Hurst exponent values. The function creates multi-step forecasts by:\n",
    "\n",
    "1. **Recursive Prediction** - Using each prediction as input for subsequent predictions\n",
    "2. **Historical Context** - Maintaining a sliding window of the most recent k values\n",
    "3. **Business Calendar** - Generating forecasts for business days only\n",
    "4. **Structured Output** - Returning results in a DataFrame with dates and forecasted values\n",
    "\n",
    "> The forecasting process enables us to project future market roughness characteristics, providing valuable insight for trading strategies and risk management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f00d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_multi_step_xgb(hurst_series: pd.Series, models: list, periods: int, k: int):\n",
    "    \"\"\"Generate all future forecasts at once using the multi-step models.\"\"\"\n",
    "    # Get the most recent window of data\n",
    "    last_window = hurst_series.values[-k:]\n",
    "    \n",
    "    # Generate predictions for each step using its own model\n",
    "    predictions = []\n",
    "    for step in range(min(periods, len(models))):\n",
    "        model = models[step]\n",
    "        pred = model.predict(last_window.reshape(1, -1))[0]\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    # If we need more predictions than we have models for\n",
    "    while len(predictions) < periods:\n",
    "        predictions.append(predictions[-1])  # Just repeat the last prediction\n",
    "    \n",
    "    dates = pd.date_range(hurst_series.index[-1] + pd.Timedelta(days=1), \n",
    "                         periods=periods, freq='B')\n",
    "    \n",
    "    return pd.DataFrame({'ds': dates, 'yhat': np.array(predictions)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23fa86",
   "metadata": {},
   "source": [
    "<a id=\"section-2-8\"></a>\n",
    "\n",
    "## 2.8 Main Execution Function for Hurst Analysis\n",
    "\n",
    "This section defines the main orchestration function that brings together all previous components into a complete Hurst exponent analysis pipeline. The function:\n",
    "\n",
    "1. **Initializes Parameters** - Sets key configuration values for window size, feature count, and model parameters\n",
    "2. **Processes Data** - Loads and filters price data for the target ticker\n",
    "3. **Computes Historical Hurst** - Calculates rolling Hurst exponent values from return data\n",
    "4. **Executes Complete Workflow** - Sequentially runs training, evaluation, and forecasting\n",
    "5. **Visualizes Results** - Plots historical and forecasted Hurst values for interpretation\n",
    "6. **Returns Forecast** - Provides the list of forecasted Hurst values for further use\n",
    "\n",
    "> This comprehensive function encapsulates the entire workflow from raw return data to visualized Hurst forecasts, enabling efficient execution of the roughness analysis process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4845a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_hurst():\n",
    "    \"\"\"\n",
    "    Main entry point: show menu, get user choice, compute Hurst series,\n",
    "    and train/evaluate/forecast the XGBoost model as requested.\n",
    "    \"\"\"\n",
    "\n",
    "    # Log start of execution\n",
    "    logger.info(\"Starting main execution...\")\n",
    "    # for the sake of this example, we will use a fixed mode\n",
    "    power = 5\n",
    "    train_frac, test_frac = 0.8, 0.2\n",
    "    k = 5\n",
    "    xgb_params = {'n_estimators': 100, 'learning_rate': 0.1}\n",
    "    periods = HORIZON\n",
    "\n",
    "    # Load data and filter by ticker symbol\n",
    "    df_all = load_data()\n",
    "    df_t = df_all[df_all['ticker'] == ticker].sort_values('date')\n",
    "    returns = df_t['return'].values\n",
    "\n",
    "    # Compute rolling Hurst exponent series\n",
    "    hurst_vals = compute_rolling_hurst(returns, power)\n",
    "    dates = df_t['date']\n",
    "    start_index = 2**power - 1\n",
    "    hurst_series = pd.Series(hurst_vals, index=dates[start_index:])\n",
    "\n",
    "    #it will first train the model, then evaluate, and finally forcase\n",
    "    multi_models = train_multi_step_xgb(hurst_series, train_frac, k, periods, xgb_params)\n",
    "        # Evaluate multi-step model\n",
    "    try:\n",
    "        multi_models = joblib.load(MODELS_DIR / f'xgb_multi_step_{periods}.pkl')\n",
    "        mse, mae = evaluate_multi_step_xgb(hurst_series, multi_models, k, test_frac, periods)\n",
    "        logger.info(f\"Multi-step XGBoost results - Average MSE: {mse:.4f}, Average MAE: {mae:.4f}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Multi-step model file not found. Please train the model first.\")\n",
    "    try:\n",
    "        multi_models = joblib.load(MODELS_DIR / f'xgb_multi_step_{periods}.pkl')\n",
    "        df_fc = forecast_multi_step_xgb(hurst_series, multi_models, periods, k)\n",
    "        logger.info(\"Multi-step XGBoost forecast:\\n%s\", df_fc)\n",
    "            \n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.plot(hurst_series.index, hurst_series.values, 'k-', label='Historical Hurst')\n",
    "        plt.plot(df_fc['ds'], df_fc['yhat'], 'r--', label='Multi-step Forecast')\n",
    "        plt.title(f'{periods}-Day Multi-step Hurst Forecast for {ticker}')\n",
    "        plt.xlabel('Date'); plt.ylabel('Hurst Exponent')\n",
    "        plt.legend(); plt.grid(True)\n",
    "        plt.xticks(rotation=45); plt.tight_layout(); plt.show()\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Multi-step model file not found. Please train the model first.\")\n",
    "    print(\"List of hurst parameters:\", list(df_fc['yhat']))\n",
    "    logger.info(\"XGBoost-only forecast:\\n%s\", df_fc)\n",
    "    return list(list(df_fc['yhat']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af8b88",
   "metadata": {},
   "source": [
    "<a id=\"section-2-9\"></a>\n",
    "\n",
    "## 2.9 Hurst Analysis Output\n",
    "\n",
    "When we execute the main_hurst() function, it produces the following results:\n",
    "\n",
    "1. **Forecasted Hurst Values** - A list of numerical values representing the predicted Hurst exponents for the next 10 business days is displayed in the console output\n",
    "   \n",
    "2. **Visualization** - A time series plot appears showing:\n",
    "   - Historical Hurst exponent values in black solid line\n",
    "   - Forecasted Hurst exponents in green dashed line\n",
    "   - Clear title indicating the forecast horizon and ticker symbol\n",
    "   - Properly labeled axes and legend\n",
    "\n",
    "3. **Console Log** - Detailed information about the forecast is recorded in the application logs\n",
    "\n",
    "4. **Return Value** - The Hurst_list variable now contains the array of forecasted values for use in subsequent trading strategy optimization\n",
    "\n",
    "> The output provides both visual and numerical representations of how market roughness is expected to evolve, allowing traders to adjust their strategies according to anticipated market conditions.\n",
    "\n",
    "*Generated: 2025-05-20 15:12:01 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hurst_list = main_hurst()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6140fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57e568a2",
   "metadata": {
    "id": "57e568a2"
   },
   "source": [
    "<a id=\"3.0\"></a>\n",
    "# Section 3: American Put Options in the Rough Bergomi Model (Main Part of the Study)\n",
    "\n",
    "> Recall the price and volatility dynamics are given by  \n",
    "> \n",
    "> $$\n",
    "> \\begin{aligned}\n",
    "> \\mathrm{d}X_t &= r\\,X_t\\,\\mathrm{d}t \\;+\\; X_t\\,v_t\\bigl(\\rho\\,\\mathrm{d}W_t \\;+\\;\\sqrt{1-\\rho^2}\\,\\mathrm{d}B_t\\bigr),\\\\\n",
    "> v_t &= \\xi_0\\,\\mathcal{E}\\!\\Bigl(\\eta\\!\\int_0^t (t - s)^{H - \\tfrac12}\\,\\mathrm{d}W_s\\Bigr),\n",
    "> \\end{aligned}\n",
    "> $$\n",
    ">\n",
    "> and pricing an American put option can be formulated as the optimal stopping problem  \n",
    "> \n",
    "> $$\n",
    "> y_0 \\;=\\; \\sup_{\\tau \\in \\mathcal{S}_0}\\mathbb{E}\\bigl[e^{-r\\tau}\\,(K - X_\\tau)^{+}\\bigr],\n",
    "> $$\n",
    ">\n",
    "> for some strike $K$. In this notebook we calculated all parameters ourselves (above).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [3.1 Simulation Setup with Dynamic Hurst Parameter](#3.1)\n",
    "- [3.2 Training Data Preview](#3.2)\n",
    "- [3.3 Signature Computations](#3.3)\n",
    "- [3.4 Signature Computation Process](#3.4)\n",
    "- [3.5 Computing Pricing Intervals with Linear Signatures](#3.5)\n",
    "- [3.6 Linear Longstaffâ€“Schwartz Pricer â€“ Lower/Upper Bound](#3.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Xlp80cKeRW9z",
   "metadata": {
    "id": "Xlp80cKeRW9z"
   },
   "source": [
    "<a id=\"3.1\"></a>\n",
    "## 3.1 Simulation Setup with Dynamic Hurst Parameter\n",
    "\n",
    "In this section, we implement the rough Bergomi model simulation framework with a critical innovation: **dynamic Hurst parameters** derived from our XGBoost forecasts in Section 2.\n",
    "\n",
    "The simulation engine supports two modes:\n",
    "1. **Fixed Hurst parameter** (traditional approach)\n",
    "2. **Time-varying Hurst parameter** (our novel approach)\n",
    "\n",
    "Our implementation creates training and testing datasets by simulating asset prices, volatilities, and option payoffs under the rough Bergomi dynamics. The key distinction is how we handle the Hurst parameter:\n",
    "\n",
    "- When forecasted Hurst values (`Hurst_list`) are available from our XGBoost model, we incorporate these time-varying roughness parameters into the simulation\n",
    "- Otherwise, we default to a constant Hurst parameter of 0.07, representing typical market conditions\n",
    "\n",
    "This approach allows us to capture the evolving nature of market roughness in our pricing and hedging strategies, potentially improving the accuracy of American put option valuations in real market conditions.\n",
    "\n",
    "> The simulation generates complete trajectory data including asset prices, volatilities, option payoffs, and Brownian increments â€“ all essential components for training our signature-based stopping policy in subsequent sections.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8badb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust path to include repository root\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from rBergomi_simulation import SimulationofrBergomi\n",
    "from rBergomi_simulation import SimulationofHurstHeston\n",
    "from dynamic_hurst_rbergomi import SimulationWithDynamicHurstHeston\n",
    "from dynamic_hurst_rbergomi import SimulationWithDynamicHurst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56cf399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_dynamic_hurst(M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r):\n",
    "    \"\"\"\n",
    "    Generate simulation data with time-varying Hurst parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Similar to SimulationWithDynamicHurst, but returns formatted data ready for model training.\n",
    "    \"\"\"\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationWithDynamicHurst(\n",
    "        M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r\n",
    "    )\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ba6457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_dynamic_hurst_heston(M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r):\n",
    "    \"\"\"\n",
    "    Generate simulation data with time-varying Hurst parameter for Heston model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Similar to SimulationWithDynamicHurstHeston, but returns formatted data ready for model training.\n",
    "    \"\"\"\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationWithDynamicHurstHeston(\n",
    "        M, N, T_years, phi, rho, K, X0, H_series, xi, eta, r\n",
    "    )\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca871ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_heston(M, N, T_years, phi, rho, K, X0, Hurst, xi, eta, r):\n",
    "    \"\"\"\n",
    "    Generate simulation data for Heston model with constant Hurst parameter.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Similar to SimulationofHurstHeston, but returns formatted data ready for model training.\n",
    "    \"\"\"\n",
    "    X, V, I, dI, dW1, dW2, dB = SimulationofHurstHeston(\n",
    "        M, N, T_years, phi, rho, K, X0, Hurst, xi, eta, r\n",
    "    )\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab6b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r):\n",
    "    X, V, I, dI, dW1, dW2, dB, Y = SimulationofrBergomi(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "    # Calculate Payoff\n",
    "    Payoff = phi(X)\n",
    "\n",
    "    # Stack state and volatility into features for signature\n",
    "    MM = np.stack([X, V], axis=-1)\n",
    "\n",
    "    return X, V, Payoff, dW1, I, MM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining hurst parameter... last official step to figure out\n",
    "if Hurst_list is not None:\n",
    "    H = Hurst_list\n",
    "    print(f\"Using Hurst Mean: {np.mean(H)}\")\n",
    "    if np.mean(H) < 0.6:\n",
    "        S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data_dynamic_hurst(\n",
    "        M, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "        )\n",
    "\n",
    "        S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data_dynamic_hurst(\n",
    "            M2, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "        )\n",
    "    else:\n",
    "        S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data_dynamic_hurst_heston(\n",
    "            M, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "        )\n",
    "\n",
    "        S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data_dynamic_hurst_heston(\n",
    "            M2, N, T_years, phi, rho, K, X0, H, xi, eta, r\n",
    "        )\n",
    "\n",
    "\n",
    "else:\n",
    "    H = 0.07  # Hurst parameter\n",
    "    # Use K in your function call, not strike\n",
    "    S_training, V_training, Payoff_training, dW_training, I_training, MM_training = generate_data(M, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "    S_testing, V_testing, Payoff_testing, dW_testing, I_testing, MM_testing = generate_data(M2, N, T_years, phi, rho, K, X0, H, xi, eta, r)\n",
    "\n",
    "print(f\"Using Hurst: {H}\")\n",
    "print(f\"Hurst parameter: {H}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec2cb58",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a>\n",
    "\n",
    "## 3.2 Training Data Preview\n",
    "\n",
    "For the sake of testing and verification, here is a preview of the first few rows of our training data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1fef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Show Head of the training data\")\n",
    "print(\"S_training\", S_training[:5])\n",
    "print(\"V_training\", V_training[:5])\n",
    "print(\"Payoff_training\", Payoff_training[:5])\n",
    "print(\"dW_training\", dW_training[:5])\n",
    "print(\"I_training\", I_training[:5])\n",
    "print(\"MM_training\", MM_training[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LW4SLN7bQy4R",
   "metadata": {
    "id": "LW4SLN7bQy4R"
   },
   "source": [
    "<a id=\"3.3\"></a>\n",
    "\n",
    "## 3.3 Signature Computations\n",
    "\n",
    "\n",
    "\n",
    "We will make use of the [iisignature package](https://pypi.org/project/iisignature/) to compute the signature, and it can be installed using pip.\n",
    "\n",
    "We import our signature computation module, which can compute various signature and log signature lift related to the generated data.\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542da7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the volatility processes\n",
    "vol_training = np.sqrt(V_training)\n",
    "vol_testing = np.sqrt(V_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa867a0",
   "metadata": {
    "id": "1fa867a0"
   },
   "outputs": [],
   "source": [
    "from Signature_computer import SignatureComputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efac91c",
   "metadata": {},
   "source": [
    "Next we initialize the `SignatureComputer`, which allows to choose from the linear and the log signature, and various choices of signature lifts. Here are some examples:\n",
    ">\n",
    "> $$\n",
    "> t\\mapsto \\mathrm{Sig}(A_t,X_t),t\\mapsto \\mathrm{Sig}(A_t,\\phi(X)_t),t\\mapsto \\mathrm{Sig}(A_t,X_t,X_{t-\\epsilon}),t\\mapsto \\mathrm{Sig}(A_t,X_t,\\phi(X_t)),t\\mapsto \\mathrm{Sig}(A_t,v_t),\n",
    "> $$\n",
    ">\n",
    "> where $t\\mapsto A_t$ is a monoton path and in our examples we choose between:\n",
    ">\n",
    "> $$\n",
    "> A_t=t, \\quad  A_t = \\langle X\\rangle_t.\n",
    "> $$\n",
    ">\n",
    "> Additonally we can add Laguerre polynomials of $X$ or $(X,v)$ to the signature, see the module for all the details.\n",
    ">\n",
    "> In this example we choose the basis $(\\mathrm{Sig}(t,v_t),p_i(X_t))$, which proves to be a solid choice for rough volatility models. We choose both the polynomial and signature degree to be $3$ for this example. (To improve result one should higher truncations levels ($4-5$) for the signature, but to keep the complexity reasonable here we choose level $3$ signatures.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a377c",
   "metadata": {
    "id": "8c4a377c"
   },
   "outputs": [],
   "source": [
    "#initialize signature computer\n",
    "sig_computer = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c1bad9",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a>\n",
    "## 3.4 Signature Computation Process\n",
    "\n",
    "After initializing our signature computer, we now compute the actual signatures for both our training and testing datasets:\n",
    "\n",
    "### Time Augmentation\n",
    "\n",
    "We first create a time grid spanning from 0 to T (our simulation horizon) with N+1 points. This grid is used to augment our path data with time information, which is crucial for signature-based methods:\n",
    "\n",
    "1. We initialize time-augmentation arrays for both training and testing data\n",
    "2. We populate these arrays with the monotonically increasing time values\n",
    "3. This time augmentation ensures the signatures capture the temporal structure of our financial data\n",
    "\n",
    "### Computing the Signatures\n",
    "\n",
    "The signature transformation maps our complex multi-dimensional paths into a set of algebraic features representing the essential characteristics of each path. This computation:\n",
    "\n",
    "1. Takes the simulated price paths, volatility processes, and option payoffs as inputs\n",
    "2. Incorporates the Brownian increments and integrated processes\n",
    "3. Constructs signature features that capture the path information in a way that's amenable to machine learning\n",
    "\n",
    "\n",
    "*Last updated: 2025-05-20 15:26:15 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c365145",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c365145",
    "outputId": "d26148d9-199d-4ebd-bd3f-60524818666b"
   },
   "outputs": [],
   "source": [
    "#Compute the signature for training and test data\n",
    "tt = np.linspace(0,T,N+1)\n",
    "A_training = np.zeros((M, N+1)) #time-augmentation\n",
    "A_testing = np.zeros((M2, N+1))\n",
    "A_training[:, 1:] = A_testing[:, 1:] = tt[1:]\n",
    "signatures_training = sig_computer.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_testing = sig_computer.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19711264",
   "metadata": {
    "id": "19711264"
   },
   "source": [
    "<a id=\"3.5\"></a>\n",
    "## 3.5 Computing Pricing Intervals with Linear Signatures\n",
    "\n",
    "> ### Step 3: Compute pricing intervals with linear signatures\n",
    "\n",
    "We can now import the linear primal and dual pricers, which compute true lower and upper bounds.\n",
    "\n",
    "* **â†’ The `LinearLongstaffSchwartzPricer`** uses the signature of the training data to recursively approximate continuation values in the spirit of the Longstaff-Schwartz algorithm (descibed in detail in Section 3.1 of [this paper](https://arxiv.org/abs/2312.03444)). The resulting regression coefficients at each exercise date provide a stopping rule, which can be applied to the testing data to get true lower-bounds\n",
    "\n",
    "* **â†’ The `LinearDualPricer`** uses the signature of the training data to minimize over the familiy of linear signature martingales, by solving a corresponding linear program (described in Detail in Section 3.2 of [this paper](https://arxiv.org/abs/2312.03444)). The resulting coefficients yield a Doob martingale approximation, which for the testing data yields a true upper bound.\n",
    "\n",
    "By combining the two values, we receive confidence intervals for the true option price.\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:** To solve the linear programm, one can optionally choose to use [Gurobi](https://www.gurobi.com), which requires a free licence, which is recommended especially for high-dimensional LPs, which occur when choosing large sample-sizes and/or high signature truncations levels. Alternatively, we use the free LP solvers from CVXPY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc73ec",
   "metadata": {
    "id": "edbc73ec"
   },
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import sys, os\n",
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "\n",
    "# now the module can be imported\n",
    "from Linear_signature_optimal_stopping import LinearLongstaffSchwartzPricer, LinearDualPricer\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a888326",
   "metadata": {
    "id": "5a888326"
   },
   "outputs": [],
   "source": [
    "\n",
    "#initialze the models\n",
    "ls_pricer = LinearLongstaffSchwartzPricer(\n",
    "        N1=N1,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        mode=\"American Option\",\n",
    "        ridge=10**(-9)\n",
    "    )\n",
    "\n",
    "dual_pricer = LinearDualPricer(\n",
    "        N1=N1,\n",
    "        N=N,\n",
    "        T=T_years,\n",
    "        r=r,\n",
    "        LP_solver=\"CVXPY\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db4f4d",
   "metadata": {
    "id": "28db4f4d"
   },
   "source": [
    "> ## Note on Longstaff-Schwartz Algorithm Mode Setting\n",
    "> \n",
    "> âš ï¸ **Mode Setting: `mode=\"Standard\"` Recommended for ATM Options**\n",
    "> \n",
    "> The setting `mode=\"American Option\"` tells the Longstaff-Schwartz algorithm to only use in-the-money paths when fitting continuation values. This follows the original Longstaff & Schwartz (2001) paper and can reduce variance when payoffs are strictly non-negative and clearly in-the-money (ITM).\n",
    "> \n",
    "> However, since we're pricing at-the-money (ATM) American put options, many paths are near the early-exercise boundary, and some may be just out-of-the-money. Excluding those paths could cause the model to learn poorly near the decision point.\n",
    "> \n",
    "> âœ… **To ensure accurate learning across the full range of possible exercise points, especially around ATM, we use:**\n",
    "> \n",
    "> ```\n",
    "> mode = \"Standard\"\n",
    "> ```\n",
    "> \n",
    "> This includes all paths, not just ITM ones, allowing for better estimation of continuation values at the boundary.\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3e022",
   "metadata": {},
   "source": [
    "<a id=\"3.6\"></a>\n",
    "## 3.6 Linear Longstaffâ€“Schwartz Pricer â€“ Lower/Upper Bound\n",
    "\n",
    "After fitting the signatureâ€based continuation rule on the training set, we apply it to the testing paths to compute a true lower bound for the American option price. Below we display the estimated lower bound and its standard error.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute true lower bounds\n",
    "lower_bound, lower_bound_std, ls_regression_models = ls_pricer.price(\n",
    "        signatures_training,\n",
    "        Payoff_training,\n",
    "        signatures_testing,\n",
    "        Payoff_testing\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a34fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e64a34fa",
    "outputId": "b116657f-4c30-4379-dbfb-5aa71549133d"
   },
   "outputs": [],
   "source": [
    "print(f\"Linear Longstaff-Schwartz lower bound: {lower_bound} Â± {lower_bound_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a51c2",
   "metadata": {
    "id": "676a51c2"
   },
   "source": [
    "Similarly let us derive the upper bounds, but we will train the model only for $M= 5000$ paths to reduce computation time, and then compute true prices for all testing samples. Again, better computations time will arise with a different number of paths.\n",
    "\n",
    "> The Longstaff-Schwartz algorithm provides a practical approach to American option pricing by recursively approximating the continuation value at each potential exercise date. Our signature-based enhancement improves the feature representation, capturing complex path dependencies that affect exercise decisions.\n",
    ">\n",
    "> The dual formulation gives us complementary upper bounds, creating a confidence interval that brackets the true option value. This is particularly valuable in rough volatility models where closed-form solutions are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fba8b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08fba8b2",
    "outputId": "710b6932-eb7b-4111-c511-20de6dd42eb6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_dual = 10000\n",
    "upper_bound, upper_bound_std, MG = dual_pricer.price(\n",
    "        signatures_training[:M_dual],\n",
    "        Payoff_training[:M_dual],\n",
    "        dW_training[:M_dual,:,0],  # Select only the first component of the Brownian increments\n",
    "        signatures_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # Select only the first component of the Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d591c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39d591c8",
    "outputId": "e078ca47-ef36-4133-d227-c6d2b88c391d"
   },
   "outputs": [],
   "source": [
    "print(f\"Linear Dual upper bound: {upper_bound} Â± {upper_bound_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(float(lower_bound),float(upper_bound))}Â± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab12b0",
   "metadata": {
    "id": "06ab12b0"
   },
   "source": [
    "<a id=\"4.0\"></a>\n",
    "# Section 4: Improving the Duality Gap\n",
    "\n",
    "In rough regimes (where $H=0.1$), we observe a significant gap between lower and upper bounds, and in this section we present two ways to improve it. The first one still relies on linear signatures, but extends the basis as explained in in Section 4 of [https://arxiv.org/abs/2312.03444](https://arxiv.org/abs/2312.03444).\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [4.1 Extended Basis Approach](#4.1)\n",
    "- [4.2 Nonlinear Signature Methods](#4.2)\n",
    "- [4.3 Kernel-Based Methods](#4.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ea7df",
   "metadata": {
    "id": "4d7ea7df"
   },
   "source": [
    "<a id=\"4.1\"></a>\n",
    "## 4.1 Extended Basis Approach\n",
    "\n",
    "In this section, we implement an enhanced signature-based approach that extends the linear basis to reduce the duality gap observed in rough regimes.\n",
    "\n",
    "### Extending the Linear Basis\n",
    "\n",
    "We consider a more involved basis by choosing the extended signature lift of $(t,X_t,\\phi(X_t))$, and additionally add Laguerre polynomials of $(X_t,v_t)$. This enriched feature representation improves our ability to capture complex path dependencies in rough volatility regimes.\n",
    "\n",
    "This extended basis approach maintains the computational efficiency of linear methods while improving approximation quality through:\n",
    "\n",
    "1. Adding payoff-dependent features directly into the signature lift\n",
    "2. Incorporating polynomial terms that capture nonlinear relationships\n",
    "3. Maintaining the interpretability of the linear framework\n",
    "\n",
    "> The key insight is that even within linear methods, the choice of basis functions can significantly impact pricing accuracy. By enriching the feature space, we can better approximate the optimal stopping rule and the martingale component in the dual formulation.\n",
    "\n",
    "After computing this extended basis, we recalculate both lower and upper bounds following the same procedure as in Section 3.6, but with our enhanced feature representation.\n",
    "\n",
    "The results demonstrate a measurable improvement in the duality gap compared to the standard linear signature approach, particularly in rough regimes where $H=0.1$.\n",
    "\n",
    "*Last updated: 2025-05-20 15:42:29 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb54f12",
   "metadata": {
    "id": "1eb54f12"
   },
   "outputs": [],
   "source": [
    "sig_computer_extended = SignatureComputer(T, N, 3, \"linear\", signature_lift=\"payoff-and-polynomial-extended\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6249a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33d6249a",
    "outputId": "53179ca8-2f48-4b44-b1eb-99f3548c0900"
   },
   "outputs": [],
   "source": [
    "signatures_extended_training = sig_computer_extended.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training, I_training, MM_training\n",
    ")\n",
    "signatures_extended_testing = sig_computer_extended.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing, I_testing, MM_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5zFWMoC-X-kM",
   "metadata": {
    "id": "5zFWMoC-X-kM"
   },
   "source": [
    "Now we repeat the procedure for the extended basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2d3960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1c2d3960",
    "outputId": "009708e1-bce5-42b9-861f-3f437c5136e6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#compute true lower bounds for the new basis\n",
    "lower_bound_extended, lower_bound_extended_std, ls_regression_models_extended = ls_pricer.price(\n",
    "        signatures_extended_training,\n",
    "        Payoff_training,\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing\n",
    "    )\n",
    "#Repeating the dual procedure for the new basis\n",
    "upper_bound_extended, upper_bound_extended_std, MG_extended = dual_pricer.price(\n",
    "        signatures_extended_training[:M_dual,:,:],\n",
    "        Payoff_training[:M_dual,:],\n",
    "        dW_training[:M_dual,:,0],  # select first component of Brownian increments\n",
    "        signatures_extended_testing,\n",
    "        Payoff_testing,\n",
    "        dW_testing[:,:,0]  # select first component of Brownian increments\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692d7161",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "692d7161",
    "outputId": "5811cd15-9104-4ec6-fb2a-31da9a47991c"
   },
   "outputs": [],
   "source": [
    "print(f\"Improve pricing interval: {(float(lower_bound_extended),float(upper_bound_extended))}Â± {np.maximum(upper_bound_std,lower_bound_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e5f596",
   "metadata": {
    "id": "b7e5f596"
   },
   "source": [
    "<a id=\"4.2\"></a>\n",
    "## 4.2 Nonlinear Signature Methods\n",
    "\n",
    "### Deep Log-Signature Optimal Stopping\n",
    "\n",
    "In forthcoming work about \"American options in rough volatility models\", we focus on more non-linear approaches to price American options. Specifically, we extend the primal and dual procedures by replacing linear functionals of the signature with deep neural networks applied to the log-signature $\\mathbb{L}=\\mathrm{log}^\\otimes(\\mathbb{X})$.\n",
    "\n",
    "This transformed version of the signature still captures the relevant information about the past of the underlying process, but grows much slower than the signature itself with respect to the truncation level. To learn highly non-linear functionals, such as the integrand of the Doob martingale (\"derivative of the Snell-envelope\"), we apply deep feedforward neural networks $\\theta$ on the log-signature.\n",
    "\n",
    "### Deep Neural Network Architecture\n",
    "\n",
    "For the primal (Longstaff-Schwartz) implementation, we use:\n",
    "- 3 hidden layers with 16 neurons each\n",
    "- Hyperbolic tangent (`tanh`) activation functions\n",
    "- No batch normalization or dropout regularization\n",
    "- Training over 15 epochs with learning rate 0.001\n",
    "\n",
    "For the dual approach, we use a similar architecture but with ReLU activation functions, which empirically perform better for the martingale approximation.\n",
    "\n",
    "> The neural network approach offers flexibility to capture highly nonlinear decision boundaries that determine optimal stopping rules, particularly important in rough volatility models where the exercise boundary can have complex geometry.\n",
    "\n",
    "This deep learning approach substantially improves pricing accuracy, offering tighter lower and upper bounds compared to linear methods, especially in challenging rough volatility regimes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTyO0IKtbYse",
   "metadata": {
    "id": "kTyO0IKtbYse"
   },
   "source": [
    "We proceed as before, but replace the linear signature by the log-signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54699ea8",
   "metadata": {
    "id": "54699ea8"
   },
   "outputs": [],
   "source": [
    "sig_computer_log = SignatureComputer(T, N, 3, \"log\", signature_lift=\"polynomial-vol\", poly_degree=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998aed9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e998aed9",
    "outputId": "195d4128-74a3-4f42-a198-6310039f6c19"
   },
   "outputs": [],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use I_testing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ff201",
   "metadata": {
    "id": "bc7ff201"
   },
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_signatures_optimal_stopping import DeepLongstaffSchwartzPricer, DeepDualPricer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747bdf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_signatures_training = sig_computer_log.compute_signature(\n",
    "    S_training, vol_training, A_training, Payoff_training,\n",
    "    dW_training[:,:,0], I_training, MM_training  # use first component\n",
    ")\n",
    "log_signatures_testing = sig_computer_log.compute_signature(\n",
    "    S_testing, vol_testing, A_testing, Payoff_testing,\n",
    "    dW_testing[:,:,0], I_testing, MM_testing  # use correct I_testing\n",
    ")\n",
    "print(\"shape of dW_training\", dW_training.shape)\n",
    "print(\"shape of dW_testing\", dW_testing.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d0eff",
   "metadata": {
    "id": "9a1d0eff"
   },
   "source": [
    "The DeepLongstaffSchwartzPricer generalizes the LinearLongstaffSchwartzPrices, where the Ridge Regression at each exercise date is replace by learning the conditional expectations via neural networks. In the following initialization we build a network with $3$ hidden layers and $16$ neurons each, between each hidden layer we apply the activation function $\\mathrm{tanh}(x)$. The remainding parameters are set to 'False'. (One can run the 'Hyperparameter_optimization_primal.py' file to optimize the choice of hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da432f",
   "metadata": {
    "id": "b2da432f"
   },
   "outputs": [],
   "source": [
    "ls_pricer = DeepLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='tanh',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,  # This is correct as float\n",
    "    dropout=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "\n",
    "dual_pricer = DeepDualPricer(\n",
    "    N1=N1,\n",
    "    N=N,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=3,\n",
    "    nodes=16,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=False,\n",
    "    regularizer=0.0,\n",
    "    dropout=False,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=False\n",
    ")\n",
    "# LS pricer call is correct\n",
    "lower_bound_deep, lower_bound_deep_std, ls_regression_models = ls_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    M_val=0,\n",
    "    batch=2**15,\n",
    "    epochs=15,\n",
    "    learning_rate=0.001\n",
    ")\n",
    "\n",
    "# Dual pricer call is correct\n",
    "y0, upper_bound_deep, upper_bound_deep_std, dual_model, dual_rule_model = dual_pricer.price(\n",
    "    log_signatures_training,\n",
    "    Payoff_training,\n",
    "    dW_training[:,:,0],  # use only first component of Brownian increments\n",
    "    log_signatures_testing,\n",
    "    Payoff_testing,\n",
    "    dW_testing[:,:,0],  # use only first component of Brownian increments\n",
    "    M_val=int(0.9*M),\n",
    "    batch=2**7,\n",
    "    epochs=15,\n",
    "    learning_rate=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9jX2tyIWchsd",
   "metadata": {
    "id": "9jX2tyIWchsd"
   },
   "source": [
    "Similarly for the dual problem, we consider the same network but use the $relu(x)$ activation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186917ea",
   "metadata": {
    "id": "186917ea"
   },
   "outputs": [],
   "source": [
    "# Consistent parameter usage for validation set size\n",
    "M_val_percentage = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cDQUUvN_c6wl",
   "metadata": {
    "id": "cDQUUvN_c6wl"
   },
   "source": [
    "The Deep Longstaff Schwartz uses $15$ epochs for at the last exercise date, and then one epochs at the remainding ones by initiliazing smartly. The learning rate for the Stochastic Gradient Descent is choosen as $0.001$, and we use batch sizes of $2^8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e9ff6",
   "metadata": {
    "id": "d04e9ff6",
    "outputId": "8fd83460-c4c2-40ca-8718-2b02f3227db5"
   },
   "outputs": [],
   "source": [
    "print(f\"Deep Longstaff-Schwartz lower bound: {lower_bound_deep} Â± {lower_bound_deep_std/np.sqrt(M2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RsfNfQyIebJD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsfNfQyIebJD",
    "outputId": "7fa5e436-d332-4232-c5e7-1a2784f5d8f4"
   },
   "outputs": [],
   "source": [
    "print(f\"Deep Dual upper bound: {upper_bound_deep} Â± {upper_bound_deep_std/np.sqrt(M2)}\")\n",
    "print(f\"Pricing interval: {(lower_bound_deep,upper_bound_deep)}Â± {np.maximum(upper_bound_deep_std,lower_bound_deep_std)/np.sqrt(M2)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a512142e",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a>\n",
    "## 4.3 Kernel-Based Methods\n",
    "\n",
    "We further enhance our approach by implementing kernel-based methods that leverage Random Fourier Features (RFF) to approximate Radial Basis Function (RBF) kernels. This approach combines the expressive power of kernel methods with the computational efficiency of neural networks.\n",
    "\n",
    "### Random Fourier Features (RFF)\n",
    "\n",
    "The key innovation in our kernel approach is the use of random Fourier features to approximate the RBF kernel:\n",
    "\n",
    "1. We generate random projection matrices with specific dimensions tailored to our exercise dates\n",
    "2. We transform our log-signature inputs through these projections using cosine and sine functions\n",
    "3. The resulting features provide a finite-dimensional approximation to an infinite-dimensional kernel space\n",
    "\n",
    "### Neural Network Architecture\n",
    "\n",
    "For the kernel-based methods, we use more complex networks:\n",
    "- 3-4 hidden layers with 32 neurons each\n",
    "- ReLU activation functions\n",
    "- Both batch normalization and layer normalization\n",
    "- L2 regularization (0.001)\n",
    "- Dropout for the dual approach to prevent overfitting\n",
    "\n",
    "These architectures are specifically designed to work with the RFF features, with training parameters carefully tuned:\n",
    "- Batch size of 256 (2^8)\n",
    "- 15 epochs of training\n",
    "- Learning rate of 0.0005 (lower than the standard neural network approach)\n",
    "\n",
    "> The kernel-based approach provides additional flexibility in capturing complex dependencies between price, volatility, and exercise decisions, while maintaining computational feasibility through the RFF approximation.\n",
    "\n",
    "This approach typically achieves the tightest bounds among all methods, particularly for high-dimensional or complex dynamics like those found in rough volatility models.\n",
    "\n",
    "*Last updated: 2025-05-20 15:42:29 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd49d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = os.path.abspath(\"..\")  # /Users/.../Optimal_Stopping_with_signatures\n",
    "ls_folder = os.path.join(repo_root, \"Non linear signature optimal stopping\")\n",
    "sys.path.extend([repo_root, ls_folder])\n",
    "from Deep_kernel_signature_optimal_stopping import DeepKernelLongstaffSchwartzPricer, DeepKernelDualPricer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96187943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Define the RFF feature computation functions with corrected implementation\n",
    "\n",
    "def compute_rff_kernel_features(signatures, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for lower bound pricer (list format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        List of tensors with shape [M, rff_dim*2, 1] for each exercise date\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices based on actual data dimensions\n",
    "    actual_steps = T_steps - 1\n",
    "    subindex = [min(int((j+1)*actual_steps/N1), actual_steps) for j in range(N1)]\n",
    "    \n",
    "    print(f\"Signature data has {T_steps} time points\")\n",
    "    print(f\"Using exercise indices: {subindex}\")\n",
    "    \n",
    "    # Create list to hold RFF features for each exercise date\n",
    "    rff_features_list = []\n",
    "    \n",
    "    # For each exercise date\n",
    "    for t in range(len(subindex)):\n",
    "        idx = min(subindex[t], T_steps-1)\n",
    "        X_t = signatures[:, idx, :]\n",
    "        \n",
    "        # Generate random projection matrix for RBF kernel approximation\n",
    "        np.random.seed(42 + t)  # Different seed for each exercise date\n",
    "        W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "        \n",
    "        # Compute RFF: [cos(Wx), sin(Wx)]\n",
    "        projection = X_t @ W\n",
    "        rff_features = np.column_stack([\n",
    "            np.cos(projection),\n",
    "            np.sin(projection)\n",
    "        ]) * np.sqrt(1/rff_dim)\n",
    "        \n",
    "        # Reshape to match expected format: [M, rff_dim*2, 1]\n",
    "        rff_features = rff_features.reshape(M, rff_dim*2, 1)\n",
    "        \n",
    "        rff_features_list.append(rff_features)\n",
    "    \n",
    "    return rff_features_list\n",
    "\n",
    "def compute_rff_kernel_features_dual(signatures, N, N1, rff_dim=128, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features for dual pricer (3D tensor format)\n",
    "    \n",
    "    Args:\n",
    "        signatures: Signature data with shape [M, T_steps, feature_dim]\n",
    "        N: Number of time steps in the discretization\n",
    "        N1: Number of exercise dates\n",
    "        rff_dim: Dimension of random features (default: 128)\n",
    "        gamma: RBF kernel bandwidth parameter (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor with shape [M, features, time] for all time points\n",
    "    \"\"\"\n",
    "    M, T_steps, feature_dim = signatures.shape\n",
    "    \n",
    "    # Calculate indices proportional to exercise dates\n",
    "    # We need to map our exercise indices to the full discretization grid\n",
    "    actual_steps = min(T_steps - 1, N)\n",
    "    all_indices = np.minimum(np.array([int(t * T_steps / (N+1)) for t in range(N+1)]), T_steps-1)\n",
    "    \n",
    "    print(f\"Using exercise indices for dual pricer: {all_indices[:5]}...{all_indices[-5:]}\")\n",
    "    \n",
    "    # Generate random projection matrix once\n",
    "    np.random.seed(42)\n",
    "    W = np.random.normal(0, np.sqrt(2*gamma), (feature_dim, rff_dim))\n",
    "    \n",
    "    # Extract all required signature data at once\n",
    "    X_all = signatures[:, all_indices, :]  # Shape: [M, N+1, feature_dim]\n",
    "    \n",
    "    # Reshape for batch matrix multiplication\n",
    "    X_reshaped = X_all.reshape(-1, feature_dim)  # Shape: [M*(N+1), feature_dim]\n",
    "    \n",
    "    # Compute all projections at once\n",
    "    projections = X_reshaped @ W  # Shape: [M*(N+1), rff_dim]\n",
    "    \n",
    "    # Compute RFF features\n",
    "    cos_features = np.cos(projections)\n",
    "    sin_features = np.sin(projections)\n",
    "    rff_features = np.column_stack([cos_features, sin_features]) * np.sqrt(1/rff_dim)\n",
    "    \n",
    "    # Reshape back to original dimensions\n",
    "    full_rff = rff_features.reshape(M, N+1, rff_dim*2)\n",
    "    \n",
    "    # Transpose to match expected format: [M, features, time]\n",
    "    return np.transpose(full_rff, (0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565afbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Calculate and use RFF features for pricing with corrected implementation\n",
    "\n",
    "# Calculate both sets of features\n",
    "rff_dim = 128\n",
    "print(\"Computing kernel features for lower bound...\")\n",
    "kernel_training = compute_rff_kernel_features(log_signatures_training, N1, rff_dim=rff_dim)\n",
    "kernel_testing = compute_rff_kernel_features(log_signatures_testing, N1, rff_dim=rff_dim)\n",
    "\n",
    "# IMPORTANT: For the dual approach, we need to generate features for N1 steps\n",
    "print(\"Computing kernel features for upper bound...\")\n",
    "kernel_training_dual = compute_rff_kernel_features_dual(log_signatures_training, N1, N1, rff_dim=rff_dim)\n",
    "kernel_testing_dual = compute_rff_kernel_features_dual(log_signatures_testing, N1, N1, rff_dim=rff_dim)\n",
    "print(\"Initializing kernel-based lower bound...\")\n",
    "# 1. LOWER BOUND calculation - this works correctly\n",
    "kernel_pricer = DeepKernelLongstaffSchwartzPricer(\n",
    "    N1=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    L=rff_dim*2,\n",
    "    mode=\"American Option\",\n",
    "    layers=3,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.001,\n",
    "    dropout=False,\n",
    "    layer_normalization=True\n",
    ")\n",
    "\n",
    "print(\"Computing kernel-based lower bound...\")\n",
    "lower_bound_kernel, lower_bound_kernel_std, kernel_models = kernel_pricer.price(\n",
    "    kernel_training,\n",
    "    kernel_testing,\n",
    "    Payoff_training,\n",
    "    Payoff_testing,\n",
    "    batch=2**15,\n",
    "    epochs=10,\n",
    "    learning_rate=0.0005\n",
    ")\n",
    "\n",
    "# 2. UPPER BOUND calculation - use direct payoff, no need to expand\n",
    "print(\"Initializing kernel-based upper bound...\")\n",
    "kernel_dual_pricer = DeepKernelDualPricer(\n",
    "    N1=N1,\n",
    "    N=N1,\n",
    "    T=T_years,\n",
    "    r=r,\n",
    "    layers=3,\n",
    "    nodes=32,\n",
    "    activation_function='relu',\n",
    "    batch_normalization=True,\n",
    "    regularizer=0.0005,\n",
    "    dropout=True,\n",
    "    attention_layer=False,\n",
    "    layer_normalization=True,\n",
    "    mode_dim=\"1-dim\"\n",
    ")\n",
    "print(\"Computing kernel-based upper bound...\")\n",
    "try:\n",
    "    y0_kernel, upper_bound_kernel, upper_bound_kernel_std, kernel_model, kernel_rule_model = kernel_dual_pricer.price(\n",
    "        kernel_training_dual,\n",
    "        Payoff_training,\n",
    "        dW_training[:,:,0],\n",
    "        kernel_testing_dual,\n",
    "        Payoff_testing,      \n",
    "        dW_testing[:,:,0],\n",
    "        M_val=int(0.9*M),\n",
    "        batch=2**15,\n",
    "        epochs=10,\n",
    "        learning_rate=0.0005\n",
    "    )\n",
    "    \n",
    "    # Report results\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} Â± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Deep Kernel Dual upper bound: {upper_bound_kernel} Â± {upper_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(f\"Kernel-based pricing interval: [{lower_bound_kernel}, {upper_bound_kernel}] Â± {np.maximum(upper_bound_kernel_std, lower_bound_kernel_std)/np.sqrt(M2)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in dual pricer: {e}\")\n",
    "    print(f\"Deep Kernel Longstaff-Schwartz lower bound: {lower_bound_kernel} Â± {lower_bound_kernel_std/np.sqrt(M2)}\")\n",
    "    print(\"Upper bound calculation failed - using only lower bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2K5Yrl-ejcN",
   "metadata": {
    "id": "a2K5Yrl-ejcN"
   },
   "source": [
    "We once again stress that the parameters for the the discretization (here $J=252$), the sample size (here $M=10^{15}$), and the signature trunaction level (here $K=3$) are not choosen big enough to get narrow gaps, but we can still already observe an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b1b07",
   "metadata": {},
   "source": [
    "<a id=\"5.0\"></a>\n",
    "# Section 5: Results Comparison and Trading Applications\n",
    "\n",
    "## Comparative Analysis of Methods\n",
    "\n",
    "We compare four different methods for American put option pricing in the rough Bergomi model:\n",
    "1. Linear Signature (baseline)\n",
    "2. Extended Linear Signature\n",
    "3. Deep Log-Signature\n",
    "4. Deep Kernel Method\n",
    "\n",
    "Our analysis reveals progressively tighter duality gaps as we move from simpler linear methods to more sophisticated approaches. This improvement is particularly significant in rough volatility regimes where traditional methods struggle.\n",
    "\n",
    "## Option Trading Interpretation\n",
    "\n",
    "The pricing bounds can be translated directly into actionable trading insights:\n",
    "\n",
    "- **Per-Contract Pricing**: By multiplying the per-share premium by the standard contract size (100 shares), we obtain practical pricing bounds for trading purposes.\n",
    "\n",
    "- **Trading Signals**: \n",
    "  - When market prices fall below our lower bound, a **BUY** signal is generated\n",
    "  - When market prices exceed our upper bound, a **SELL** signal is generated\n",
    "  - When prices fall within our bounds, a **NEUTRAL** stance is appropriate\n",
    "\n",
    "- **Risk Management Considerations**:\n",
    "  - Position sizing recommendations limit exposure to <5% of portfolio\n",
    "  - Early exercise considerations vary based on moneyness\n",
    "  - Hedging strategies include delta and vega hedging\n",
    "  - Model risk awareness emphasizes the importance of Hurst parameter calibration\n",
    "\n",
    "## Practical Implementation\n",
    "\n",
    "For options with different moneyness levels, we provide specific recommendations:\n",
    "- Deep ITM options: Consider early exercise if dividend yield exceeds interest rate\n",
    "- ATM options: Maximum sensitivity to volatility parameters, requires active monitoring\n",
    "- OTM options: Trade similar to European options, primarily valuable as insurance\n",
    "\n",
    "> By incorporating rough volatility effects that traditional models like Black-Scholes miss, our approach provides more accurate pricing and risk management, particularly in volatile market conditions.\n",
    "\n",
    "The comparative analysis demonstrates that our advanced methods (especially the deep kernel approach) provide significantly tighter bounds and thus more precise pricing in rough volatility regimes compared to traditional approaches.\n",
    "\n",
    "*Last updated: 2025-05-20 15:54:11 UTC by roshanshah11*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866c160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfbfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the normalized price bounds to actual USD values\n",
    "actual_stock_price = X0  # USD per share\n",
    "actual_strike = strike  # USD per share\n",
    "\n",
    "# Include all four methods in the list\n",
    "methods = [\n",
    "    \"Linear Signature\", \n",
    "    \"Extended Linear Signature\", \n",
    "    \"Deep Log-Signature\",\n",
    "    \"Deep Kernel Method\"\n",
    "]\n",
    "print(lower_bound_kernel, upper_bound_kernel, lower_bound_kernel_std, upper_bound_kernel_std)\n",
    "\n",
    "# Actual Bond Premium stored as variable premium\n",
    "actual_premium = model_inputs[\"market_premium\"]\n",
    "# Collect all price bounds\n",
    "lower_bounds = [lower_bound, lower_bound_extended, lower_bound_deep, lower_bound_kernel]\n",
    "upper_bounds = [upper_bound, upper_bound_extended, upper_bound_deep, upper_bound_kernel]\n",
    "stds = [lower_bound_std, lower_bound_extended_std, lower_bound_deep_std, lower_bound_kernel_std]\n",
    "\n",
    "# Create a table of results with duality gap\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "results = []\n",
    "for i, method in enumerate(methods):\n",
    "    usd_lower = float(lower_bounds[i])\n",
    "    \n",
    "    # Handle the case where upper bound might not be available for kernel method\n",
    "    if i == 3 and 'upper_bound_kernel' not in locals():\n",
    "        usd_upper = float('nan')  # Use NaN if upper bound isn't available\n",
    "    else:\n",
    "        usd_upper = float(upper_bounds[i]) \n",
    "    \n",
    "    usd_std = float(stds[i]) / np.sqrt(M2)\n",
    "    \n",
    "    # Calculate duality gap only if upper bound exists\n",
    "    if not np.isnan(usd_upper):\n",
    "        duality_gap = usd_upper - usd_lower\n",
    "        gap_percent = duality_gap / usd_lower * 100\n",
    "    else:\n",
    "        duality_gap = float('nan')\n",
    "        gap_percent = float('nan')\n",
    "    \n",
    "    # Determine if premium is within bounds\n",
    "    if not np.isnan(usd_upper):\n",
    "        premium_status = \"Within bounds\" if usd_lower <= actual_premium <= usd_upper else \"Outside bounds\"\n",
    "    else:\n",
    "        premium_status = \"Compared to lower bound only\"\n",
    "    \n",
    "    results.append({\n",
    "        \"Method\": method,\n",
    "        \"Lower Bound (USD)\": f\"${usd_lower:.2f}\",\n",
    "        \"Upper Bound (USD)\": f\"${usd_upper:.2f}\" if not np.isnan(usd_upper) else \"N/A\",\n",
    "        \"Std Error (USD)\": f\"${usd_std:.2f}\",\n",
    "        \"Duality Gap (USD)\": f\"${duality_gap:.2f}\" if not np.isnan(duality_gap) else \"N/A\",\n",
    "        \"Gap (%)\": f\"{gap_percent:.2f}%\" if not np.isnan(gap_percent) else \"N/A\",\n",
    "        \"Market Premium\": f\"${actual_premium:.2f}\",\n",
    "        \"Premium Status\": premium_status\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(HTML(results_df.to_html(index=False)))\n",
    "\n",
    "# Print text comparison as well\n",
    "print(f\"\\nMarket Premium: ${actual_premium:.2f}\")\n",
    "print(\"\\nComparison with calculated bounds:\")\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    lower = float(lower_bounds[i])\n",
    "    upper = float(upper_bounds[i]) if i < 3 or 'upper_bound_kernel' in locals() else float('nan')\n",
    "    \n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Lower Bound: ${lower:.2f}  {'(underprices)' if lower < actual_premium else '(overprices)'}\")\n",
    "    \n",
    "    if not np.isnan(upper):\n",
    "        print(f\"  Upper Bound: ${upper:.2f}  {'(underprices)' if upper < actual_premium else '(overprices)'}\")\n",
    "        print(f\"  Duality Gap: ${upper - lower:.2f} ({(upper - lower)/lower*100:.2f}%)\")\n",
    "        print(f\"  Premium within bounds: {'Yes' if lower <= actual_premium <= upper else 'No'}\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403373a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can calculate the fair price range for a standard options contract\n",
    "# Cell under \"# For a standard options contract (100 shares)\"\n",
    "shares_per_contract = 100\n",
    "per_share_lower = float(lower_bound_kernel)\n",
    "per_share_upper = float(upper_bound_kernel)\n",
    "per_share_midpoint = (per_share_lower + per_share_upper) / 2\n",
    "\n",
    "# Calculate total contract premium (100 shares)\n",
    "contract_lower = per_share_lower * shares_per_contract\n",
    "contract_upper = per_share_upper * shares_per_contract\n",
    "contract_midpoint = per_share_midpoint * shares_per_contract\n",
    "\n",
    "print(f\"American Put Option Contract Analysis (for {shares_per_contract} shares)\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Stock Price: ${actual_stock_price:.2f}\")\n",
    "print(f\"Strike Price: ${actual_strike:.2f}\")\n",
    "print(f\"Time to Maturity: {T} days\")\n",
    "print(f\"Interest Rate: {r*100:.2f}%\")\n",
    "print(f\"Rough Volatility Parameters: H={H}, Î·={eta}, Ï={rho}, Î¾â‚€={xi}\")\n",
    "print(f\"=====================================================================\")\n",
    "print(f\"Per-Share Premium Range: ${per_share_lower:.2f} to ${per_share_upper:.2f}\")\n",
    "print(f\"Total Contract Premium Range: ${contract_lower:.2f} to ${contract_upper:.2f}\")\n",
    "print(f\"Midpoint Price: ${contract_midpoint:.2f} per contract\")\n",
    "print(f\"=====================================================================\")\n",
    "\n",
    "# Trading recommendations based on market prices\n",
    "hypothetical_market_prices = [contract_lower * 0.8, contract_midpoint, contract_upper * 1.2]\n",
    "labels = [\"Below Fair Value\", \"At Fair Value\", \"Above Fair Value\"]\n",
    "\n",
    "print(\"Trading Recommendations:\")\n",
    "for price, label in zip(hypothetical_market_prices, labels):\n",
    "    print(f\"\\nIf market price is ${price:.2f} ({label}):\")\n",
    "    \n",
    "    if price < contract_lower:\n",
    "        print(\"â†’ BUY: Market price is below fair value range\")\n",
    "        print(f\"â†’ Expected edge: ${(contract_lower - price):.2f} to ${(contract_upper - price):.2f} per contract\")\n",
    "        print(\"â†’ Consider buying puts for protection or speculative profit\")\n",
    "    elif price > contract_upper:\n",
    "        print(\"â†’ SELL: Market price is above fair value range\")\n",
    "        print(f\"â†’ Expected edge: ${(price - contract_upper):.2f} to ${(price - contract_lower):.2f} per contract\")\n",
    "        print(\"â†’ Consider writing puts, potentially as part of a spread strategy to limit risk\")\n",
    "    else:\n",
    "        print(\"â†’ NEUTRAL: Market price is within fair value range\")\n",
    "        position = (price - contract_lower) / (contract_upper - contract_lower)\n",
    "        print(f\"â†’ Price is positioned {position:.0%} through the fair value range\")\n",
    "        if position < 0.4:\n",
    "            print(\"â†’ Slight bias toward buying\")\n",
    "        elif position > 0.6:\n",
    "            print(\"â†’ Slight bias toward selling\")\n",
    "        else:\n",
    "            print(\"â†’ No clear edge for buying or selling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5bf077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual market premium from the data\n",
    "market_premium_per_share = actual_premium \n",
    "percent_itm = max(0, (actual_strike - actual_stock_price) / actual_strike)\n",
    "\n",
    "moneyness = actual_stock_price / actual_strike\n",
    "# Corrected time value calculation for a put option\n",
    "intrinsic_value_per_share = max(0, actual_strike - actual_stock_price)\n",
    "time_value_per_share = market_premium_per_share - intrinsic_value_per_share\n",
    "\n",
    "# Calculate contract values\n",
    "market_premium_contract = market_premium_per_share * shares_per_contract\n",
    "intrinsic_value_contract = intrinsic_value_per_share * shares_per_contract\n",
    "time_value_contract = time_value_per_share * shares_per_contract\n",
    "\n",
    "rounded_hurst = [round(item, 2) for item in Hurst_list]\n",
    "hurst_display = \", \".join(map(str, rounded_hurst))\n",
    "\n",
    "print(\"Risk Management Considerations:\")\n",
    "print(\"=====================================================================\")\n",
    "print(f\"Moneyness: {moneyness:.2f} ({percent_itm:.0%} in-the-money)\")\n",
    "print(f\"Intrinsic Value: ${intrinsic_value_per_share:.2f} per share (${intrinsic_value_contract:.2f} per contract)\")\n",
    "print(f\"Time Value: ${time_value_per_share:.2f} per share (${time_value_contract:.2f} per contract)\")\n",
    "print(f\"Total Premium: ${market_premium_per_share:.2f} per share (${market_premium_contract:.2f} per contract)\")\n",
    "print(f\"Uncertainty Range: ${(per_share_upper - per_share_lower):.2f} per share (${(contract_upper - contract_lower):.2f} per contract)\")\n",
    "print(\"\\nRecommended Risk Management Strategies:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "print(\"1. Position Sizing: Limit exposure to <5% of portfolio per trade\")\n",
    "print(\"2. Early Exercise Consideration: Monitor optimal stopping boundaries\")\n",
    "print(\"3. Hedging: Consider delta and vega hedging for larger positions\")\n",
    "print(f\"4. Model Risk: Be aware model assumes H={hurst_display}, may differ from market\")\n",
    "\n",
    "# Additional practical advice\n",
    "print(\"\\nPractical Implementation:\")\n",
    "print(\"---------------------------------------------------------------------\")\n",
    "if moneyness < 0.95:\n",
    "    print(\"â†’ Deep ITM option: Consider early exercise if dividend yield > interest rate\")\n",
    "    print(\"â†’ Watch for significant changes in volatility that could shift optimal exercise boundary\")\n",
    "elif moneyness > 1.05:\n",
    "    print(\"â†’ OTM option: Early exercise unlikely, trade like European option\")\n",
    "    print(\"â†’ Primary value is in insurance against downside moves\")\n",
    "else:\n",
    "    print(\"â†’ ATM option: Maximum gamma/vega exposure\")\n",
    "    print(\"â†’ Most sensitive to changes in volatility and rough volatility parameters\")\n",
    "    print(\"â†’ Actively monitor for optimal early exercise conditions near expiration\")\n",
    "\n",
    "print(f\"\\nNote: This model incorporates rough volatility effects H=({hurst_display}) which\")\n",
    "print(\"traditional models like Black-Scholes miss. This can be particularly\")\n",
    "print(\"important for managing risk in volatile market conditions.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
